# 数据处理与可视化

## 1. 数据处理基础

### 1.1 Pandas核心概念

```python
# pandas_basic.py
import pandas as pd
import numpy as np
from typing import Dict, List, Optional
import warnings
warnings.filterwarnings('ignore')

class DataProcessor:
    """数据处理器"""
    
    def __init__(self):
        self.data: Optional[pd.DataFrame] = None
    
    def load_data(self, file_path: str, **kwargs) -> pd.DataFrame:
        """加载数据"""
        if file_path.endswith('.csv'):
            self.data = pd.read_csv(file_path, **kwargs)
        elif file_path.endswith('.xlsx'):
            self.data = pd.read_excel(file_path, **kwargs)
        elif file_path.endswith('.json'):
            self.data = pd.read_json(file_path, **kwargs)
        else:
            raise ValueError("不支持的文件格式")
        
        return self.data
    
    def create_sample_data(self) -> pd.DataFrame:
        """创建示例数据"""
        np.random.seed(42)
        
        data = {
            'id': range(1, 1001),
            'name': [f'User_{i}' for i in range(1, 1001)],
            'age': np.random.randint(18, 80, 1000),
            'salary': np.random.normal(50000, 15000, 1000),
            'department': np.random.choice(['IT', 'HR', 'Finance', 'Marketing'], 1000),
            'experience': np.random.randint(0, 30, 1000),
            'performance_score': np.random.uniform(1, 10, 1000),
            'join_date': pd.date_range('2020-01-01', periods=1000, freq='D')
        }
        
        self.data = pd.DataFrame(data)
        return self.data
    
    def basic_info(self) -> Dict:
        """获取数据基本信息"""
        if self.data is None:
            raise ValueError("数据未加载")
        
        return {
            'shape': self.data.shape,
            'columns': self.data.columns.tolist(),
            'dtypes': self.data.dtypes.to_dict(),
            'memory_usage': self.data.memory_usage(deep=True).sum(),
            'null_counts': self.data.isnull().sum().to_dict(),
            'duplicate_count': self.data.duplicated().sum()
        }
    
    def data_quality_report(self) -> pd.DataFrame:
        """数据质量报告"""
        if self.data is None:
            raise ValueError("数据未加载")
        
        report = pd.DataFrame({
            'column': self.data.columns,
            'dtype': self.data.dtypes,
            'null_count': self.data.isnull().sum(),
            'null_percentage': (self.data.isnull().sum() / len(self.data)) * 100,
            'unique_count': self.data.nunique(),
            'duplicate_count': self.data.duplicated().sum()
        })
        
        return report

# 使用示例
def demonstrate_basic_operations():
    """演示基本操作"""
    processor = DataProcessor()
    
    # 创建示例数据
    df = processor.create_sample_data()
    print("=== 数据基本信息 ===")
    print(processor.basic_info())
    
    print("\n=== 数据质量报告 ===")
    print(processor.data_quality_report())
    
    print("\n=== 数据预览 ===")
    print(df.head())
    
    print("\n=== 数据统计 ===")
    print(df.describe())

if __name__ == "__main__":
    demonstrate_basic_operations()
```

### 1.2 数据清洗技术

```python
# data_cleaning.py
import pandas as pd
import numpy as np
from typing import List, Dict, Any
import re

class DataCleaner:
    """数据清洗器"""
    
    def __init__(self, data: pd.DataFrame):
        self.data = data.copy()
        self.cleaning_log: List[Dict[str, Any]] = []
    
    def remove_duplicates(self, subset: List[str] = None) -> pd.DataFrame:
        """移除重复数据"""
        initial_count = len(self.data)
        self.data = self.data.drop_duplicates(subset=subset)
        removed_count = initial_count - len(self.data)
        
        self.cleaning_log.append({
            'operation': 'remove_duplicates',
            'removed_count': removed_count,
            'remaining_count': len(self.data)
        })
        
        return self.data
    
    def handle_missing_values(self, strategy: str = 'drop', fill_value: Any = None) -> pd.DataFrame:
        """处理缺失值"""
        initial_count = self.data.isnull().sum().sum()
        
        if strategy == 'drop':
            self.data = self.data.dropna()
        elif strategy == 'fill':
            if fill_value is not None:
                self.data = self.data.fillna(fill_value)
            else:
                # 数值列用均值填充，分类列用众数填充
                for col in self.data.columns:
                    if self.data[col].dtype in ['int64', 'float64']:
                        self.data[col].fillna(self.data[col].mean(), inplace=True)
                    else:
                        self.data[col].fillna(self.data[col].mode()[0], inplace=True)
        elif strategy == 'interpolate':
            self.data = self.data.interpolate()
        
        final_count = self.data.isnull().sum().sum()
        
        self.cleaning_log.append({
            'operation': 'handle_missing_values',
            'strategy': strategy,
            'initial_missing': initial_count,
            'final_missing': final_count
        })
        
        return self.data
    
    def remove_outliers(self, columns: List[str], method: str = 'iqr', threshold: float = 1.5) -> pd.DataFrame:
        """移除异常值"""
        initial_count = len(self.data)
        
        for col in columns:
            if method == 'iqr':
                Q1 = self.data[col].quantile(0.25)
                Q3 = self.data[col].quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - threshold * IQR
                upper_bound = Q3 + threshold * IQR
                self.data = self.data[(self.data[col] >= lower_bound) & (self.data[col] <= upper_bound)]
            elif method == 'zscore':
                z_scores = np.abs((self.data[col] - self.data[col].mean()) / self.data[col].std())
                self.data = self.data[z_scores < threshold]
        
        final_count = len(self.data)
        
        self.cleaning_log.append({
            'operation': 'remove_outliers',
            'columns': columns,
            'method': method,
            'threshold': threshold,
            'initial_count': initial_count,
            'final_count': final_count
        })
        
        return self.data
    
    def standardize_text(self, columns: List[str]) -> pd.DataFrame:
        """标准化文本数据"""
        for col in columns:
            if self.data[col].dtype == 'object':
                # 转换为小写
                self.data[col] = self.data[col].str.lower()
                # 移除多余空格
                self.data[col] = self.data[col].str.strip()
                # 移除特殊字符
                self.data[col] = self.data[col].str.replace(r'[^\w\s]', '', regex=True)
        
        self.cleaning_log.append({
            'operation': 'standardize_text',
            'columns': columns
        })
        
        return self.data
    
    def convert_data_types(self, type_mapping: Dict[str, str]) -> pd.DataFrame:
        """转换数据类型"""
        for col, dtype in type_mapping.items():
            if col in self.data.columns:
                try:
                    if dtype == 'datetime':
                        self.data[col] = pd.to_datetime(self.data[col])
                    elif dtype == 'category':
                        self.data[col] = self.data[col].astype('category')
                    else:
                        self.data[col] = self.data[col].astype(dtype)
                except Exception as e:
                    print(f"转换列 {col} 到 {dtype} 时出错: {e}")
        
        self.cleaning_log.append({
            'operation': 'convert_data_types',
            'type_mapping': type_mapping
        })
        
        return self.data
    
    def get_cleaning_summary(self) -> pd.DataFrame:
        """获取清洗摘要"""
        return pd.DataFrame(self.cleaning_log)

# 使用示例
def demonstrate_data_cleaning():
    """演示数据清洗"""
    # 创建包含问题的数据
    np.random.seed(42)
    data = {
        'id': range(1, 101),
        'name': [f'User_{i}' for i in range(1, 101)],
        'age': np.random.randint(18, 80, 100),
        'salary': np.random.normal(50000, 15000, 100),
        'department': np.random.choice(['IT', 'HR', 'Finance', 'Marketing', None], 100),
        'email': [f'user{i}@example.com' if i % 10 != 0 else None for i in range(1, 101)]
    }
    
    # 添加一些异常值
    data['salary'][0] = 500000  # 异常高薪
    data['age'][1] = 150  # 异常年龄
    
    # 添加重复数据
    data['id'].extend([1, 2, 3])
    data['name'].extend(['User_1', 'User_2', 'User_3'])
    data['age'].extend([25, 30, 35])
    data['salary'].extend([50000, 60000, 70000])
    data['department'].extend(['IT', 'HR', 'Finance'])
    data['email'].extend(['user1@example.com', 'user2@example.com', 'user3@example.com'])
    
    df = pd.DataFrame(data)
    print("=== 原始数据 ===")
    print(f"数据形状: {df.shape}")
    print(f"缺失值: {df.isnull().sum().sum()}")
    print(f"重复值: {df.duplicated().sum()}")
    
    # 数据清洗
    cleaner = DataCleaner(df)
    
    # 移除重复数据
    cleaner.remove_duplicates(subset=['id'])
    
    # 处理缺失值
    cleaner.handle_missing_values(strategy='fill')
    
    # 移除异常值
    cleaner.remove_outliers(['salary', 'age'], method='iqr')
    
    # 标准化文本
    cleaner.standardize_text(['name', 'department'])
    
    # 转换数据类型
    cleaner.convert_data_types({
        'id': 'int64',
        'age': 'int64',
        'salary': 'float64',
        'department': 'category'
    })
    
    print("\n=== 清洗后数据 ===")
    print(f"数据形状: {cleaner.data.shape}")
    print(f"缺失值: {cleaner.data.isnull().sum().sum()}")
    print(f"重复值: {cleaner.data.duplicated().sum()}")
    
    print("\n=== 清洗摘要 ===")
    print(cleaner.get_cleaning_summary())

if __name__ == "__main__":
    demonstrate_data_cleaning()
```

## 2. 数据探索性分析

### 2.1 统计分析

```python
# exploratory_analysis.py
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple
from scipy import stats

class ExploratoryAnalyzer:
    """探索性分析器"""
    
    def __init__(self, data: pd.DataFrame):
        self.data = data
        self.numeric_columns = data.select_dtypes(include=[np.number]).columns.tolist()
        self.categorical_columns = data.select_dtypes(include=['object', 'category']).columns.tolist()
    
    def descriptive_statistics(self) -> pd.DataFrame:
        """描述性统计"""
        return self.data.describe(include='all')
    
    def correlation_analysis(self) -> pd.DataFrame:
        """相关性分析"""
        if len(self.numeric_columns) < 2:
            return pd.DataFrame()
        
        return self.data[self.numeric_columns].corr()
    
    def distribution_analysis(self, column: str) -> Dict:
        """分布分析"""
        if column not in self.data.columns:
            raise ValueError(f"列 {column} 不存在")
        
        series = self.data[column]
        
        if series.dtype in ['int64', 'float64']:
            # 数值型数据
            return {
                'mean': series.mean(),
                'median': series.median(),
                'mode': series.mode().iloc[0] if not series.mode().empty else None,
                'std': series.std(),
                'skewness': stats.skew(series),
                'kurtosis': stats.kurtosis(series),
                'min': series.min(),
                'max': series.max(),
                'range': series.max() - series.min(),
                'iqr': series.quantile(0.75) - series.quantile(0.25)
            }
        else:
            # 分类型数据
            value_counts = series.value_counts()
            return {
                'unique_count': series.nunique(),
                'most_frequent': value_counts.index[0],
                'most_frequent_count': value_counts.iloc[0],
                'most_frequent_percentage': (value_counts.iloc[0] / len(series)) * 100,
                'value_counts': value_counts.head(10).to_dict()
            }
    
    def outlier_detection(self, column: str, method: str = 'iqr') -> List[int]:
        """异常值检测"""
        if column not in self.numeric_columns:
            raise ValueError(f"列 {column} 不是数值型")
        
        series = self.data[column]
        
        if method == 'iqr':
            Q1 = series.quantile(0.25)
            Q3 = series.quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            outliers = self.data[(series < lower_bound) | (series > upper_bound)].index.tolist()
        elif method == 'zscore':
            z_scores = np.abs((series - series.mean()) / series.std())
            outliers = self.data[z_scores > 3].index.tolist()
        
        return outliers
    
    def missing_pattern_analysis(self) -> pd.DataFrame:
        """缺失值模式分析"""
        missing_data = self.data.isnull()
        missing_summary = pd.DataFrame({
            'missing_count': missing_data.sum(),
            'missing_percentage': (missing_data.sum() / len(self.data)) * 100,
            'data_type': self.data.dtypes
        })
        
        return missing_summary.sort_values('missing_percentage', ascending=False)
    
    def feature_importance_analysis(self, target_column: str) -> pd.DataFrame:
        """特征重要性分析"""
        if target_column not in self.data.columns:
            raise ValueError(f"目标列 {target_column} 不存在")
        
        target = self.data[target_column]
        importance_scores = []
        
        for col in self.numeric_columns:
            if col != target_column:
                # 计算相关系数
                corr = self.data[col].corr(target)
                importance_scores.append({
                    'feature': col,
                    'correlation': abs(corr),
                    'correlation_direction': 'positive' if corr > 0 else 'negative'
                })
        
        for col in self.categorical_columns:
            if col != target_column:
                # 计算卡方检验
                contingency_table = pd.crosstab(self.data[col], target)
                chi2, p_value, dof, expected = stats.chi2_contingency(contingency_table)
                importance_scores.append({
                    'feature': col,
                    'correlation': chi2,
                    'correlation_direction': 'categorical'
                })
        
        return pd.DataFrame(importance_scores).sort_values('correlation', ascending=False)

# 使用示例
def demonstrate_exploratory_analysis():
    """演示探索性分析"""
    # 创建示例数据
    np.random.seed(42)
    data = {
        'id': range(1, 1001),
        'age': np.random.randint(18, 80, 1000),
        'salary': np.random.normal(50000, 15000, 1000),
        'experience': np.random.randint(0, 30, 1000),
        'performance_score': np.random.uniform(1, 10, 1000),
        'department': np.random.choice(['IT', 'HR', 'Finance', 'Marketing'], 1000),
        'education': np.random.choice(['Bachelor', 'Master', 'PhD'], 1000),
        'satisfaction': np.random.uniform(1, 5, 1000)
    }
    
    df = pd.DataFrame(data)
    analyzer = ExploratoryAnalyzer(df)
    
    print("=== 描述性统计 ===")
    print(analyzer.descriptive_statistics())
    
    print("\n=== 相关性分析 ===")
    print(analyzer.correlation_analysis())
    
    print("\n=== 分布分析 - 年龄 ===")
    print(analyzer.distribution_analysis('age'))
    
    print("\n=== 分布分析 - 部门 ===")
    print(analyzer.distribution_analysis('department'))
    
    print("\n=== 异常值检测 - 薪资 ===")
    outliers = analyzer.outlier_detection('salary')
    print(f"发现 {len(outliers)} 个异常值")
    
    print("\n=== 缺失值模式分析 ===")
    print(analyzer.missing_pattern_analysis())
    
    print("\n=== 特征重要性分析 ===")
    print(analyzer.feature_importance_analysis('satisfaction'))

if __name__ == "__main__":
    demonstrate_exploratory_analysis()
```

### 2.2 数据可视化

```python
# data_visualization.py
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
from typing import List, Dict, Optional
import warnings
warnings.filterwarnings('ignore')

# 设置中文字体
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class DataVisualizer:
    """数据可视化器"""
    
    def __init__(self, data: pd.DataFrame):
        self.data = data
        self.numeric_columns = data.select_dtypes(include=[np.number]).columns.tolist()
        self.categorical_columns = data.select_dtypes(include=['object', 'category']).columns.tolist()
    
    def plot_distribution(self, column: str, bins: int = 30, figsize: Tuple[int, int] = (10, 6)):
        """绘制分布图"""
        if column not in self.data.columns:
            raise ValueError(f"列 {column} 不存在")
        
        fig, axes = plt.subplots(1, 2, figsize=figsize)
        
        # 直方图
        self.data[column].hist(bins=bins, ax=axes[0], alpha=0.7)
        axes[0].set_title(f'{column} 分布直方图')
        axes[0].set_xlabel(column)
        axes[0].set_ylabel('频次')
        
        # 箱线图
        self.data.boxplot(column=column, ax=axes[1])
        axes[1].set_title(f'{column} 箱线图')
        axes[1].set_ylabel(column)
        
        plt.tight_layout()
        plt.show()
    
    def plot_correlation_heatmap(self, figsize: Tuple[int, int] = (10, 8)):
        """绘制相关性热力图"""
        if len(self.numeric_columns) < 2:
            print("数值型列数量不足，无法绘制相关性热力图")
            return
        
        correlation_matrix = self.data[self.numeric_columns].corr()
        
        plt.figure(figsize=figsize)
        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,
                   square=True, fmt='.2f', cbar_kws={'shrink': 0.8})
        plt.title('特征相关性热力图')
        plt.tight_layout()
        plt.show()
    
    def plot_categorical_distribution(self, column: str, figsize: Tuple[int, int] = (12, 6)):
        """绘制分类变量分布"""
        if column not in self.categorical_columns:
            raise ValueError(f"列 {column} 不是分类变量")
        
        fig, axes = plt.subplots(1, 2, figsize=figsize)
        
        # 柱状图
        value_counts = self.data[column].value_counts()
        value_counts.plot(kind='bar', ax=axes[0])
        axes[0].set_title(f'{column} 分布柱状图')
        axes[0].set_xlabel(column)
        axes[0].set_ylabel('频次')
        axes[0].tick_params(axis='x', rotation=45)
        
        # 饼图
        value_counts.plot(kind='pie', ax=axes[1], autopct='%1.1f%%')
        axes[1].set_title(f'{column} 分布饼图')
        axes[1].set_ylabel('')
        
        plt.tight_layout()
        plt.show()
    
    def plot_scatter_matrix(self, columns: List[str], figsize: Tuple[int, int] = (15, 15)):
        """绘制散点图矩阵"""
        if len(columns) < 2:
            raise ValueError("至少需要2个数值型列")
        
        numeric_cols = [col for col in columns if col in self.numeric_columns]
        if len(numeric_cols) < 2:
            raise ValueError("指定的列中数值型列数量不足")
        
        pd.plotting.scatter_matrix(self.data[numeric_cols], figsize=figsize, alpha=0.6)
        plt.suptitle('散点图矩阵', y=0.95)
        plt.tight_layout()
        plt.show()
    
    def plot_time_series(self, date_column: str, value_column: str, 
                        figsize: Tuple[int, int] = (12, 6)):
        """绘制时间序列图"""
        if date_column not in self.data.columns or value_column not in self.data.columns:
            raise ValueError("指定的列不存在")
        
        # 确保日期列是datetime类型
        if not pd.api.types.is_datetime64_any_dtype(self.data[date_column]):
            self.data[date_column] = pd.to_datetime(self.data[date_column])
        
        plt.figure(figsize=figsize)
        plt.plot(self.data[date_column], self.data[value_column])
        plt.title(f'{value_column} 时间序列图')
        plt.xlabel(date_column)
        plt.ylabel(value_column)
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.show()
    
    def plot_grouped_analysis(self, group_column: str, value_column: str,
                             plot_type: str = 'box', figsize: Tuple[int, int] = (12, 6)):
        """绘制分组分析图"""
        if group_column not in self.categorical_columns:
            raise ValueError(f"分组列 {group_column} 不是分类变量")
        
        if value_column not in self.numeric_columns:
            raise ValueError(f"数值列 {value_column} 不是数值型")
        
        plt.figure(figsize=figsize)
        
        if plot_type == 'box':
            self.data.boxplot(column=value_column, by=group_column, ax=plt.gca())
            plt.title(f'{value_column} 按 {group_column} 分组的箱线图')
        elif plot_type == 'violin':
            sns.violinplot(data=self.data, x=group_column, y=value_column)
            plt.title(f'{value_column} 按 {group_column} 分组的小提琴图')
        elif plot_type == 'bar':
            grouped_data = self.data.groupby(group_column)[value_column].mean()
            grouped_data.plot(kind='bar')
            plt.title(f'{value_column} 按 {group_column} 分组的平均柱状图')
            plt.ylabel(f'平均 {value_column}')
        
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.show()
    
    def plot_missing_data_pattern(self, figsize: Tuple[int, int] = (12, 8)):
        """绘制缺失数据模式"""
        missing_data = self.data.isnull()
        
        fig, axes = plt.subplots(2, 1, figsize=figsize)
        
        # 缺失值数量
        missing_counts = missing_data.sum()
        missing_counts = missing_counts[missing_counts > 0]
        
        if len(missing_counts) > 0:
            missing_counts.plot(kind='bar', ax=axes[0])
            axes[0].set_title('各列缺失值数量')
            axes[0].set_ylabel('缺失值数量')
            axes[0].tick_params(axis='x', rotation=45)
        else:
            axes[0].text(0.5, 0.5, '无缺失值', ha='center', va='center', transform=axes[0].transAxes)
            axes[0].set_title('缺失值分析')
        
        # 缺失值模式热力图
        if missing_data.sum().sum() > 0:
            sns.heatmap(missing_data, cbar=True, yticklabels=False, ax=axes[1])
            axes[1].set_title('缺失值模式热力图')
        else:
            axes[1].text(0.5, 0.5, '无缺失值', ha='center', va='center', transform=axes[1].transAxes)
            axes[1].set_title('缺失值模式')
        
        plt.tight_layout()
        plt.show()
    
    def create_dashboard(self, figsize: Tuple[int, int] = (20, 15)):
        """创建数据仪表板"""
        fig, axes = plt.subplots(3, 3, figsize=figsize)
        axes = axes.flatten()
        
        # 1. 数值型变量分布
        if len(self.numeric_columns) > 0:
            self.data[self.numeric_columns[0]].hist(bins=30, ax=axes[0], alpha=0.7)
            axes[0].set_title(f'{self.numeric_columns[0]} 分布')
        
        # 2. 分类变量分布
        if len(self.categorical_columns) > 0:
            self.data[self.categorical_columns[0]].value_counts().plot(kind='bar', ax=axes[1])
            axes[1].set_title(f'{self.categorical_columns[0]} 分布')
            axes[1].tick_params(axis='x', rotation=45)
        
        # 3. 相关性热力图
        if len(self.numeric_columns) > 1:
            correlation_matrix = self.data[self.numeric_columns].corr()
            sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,
                       square=True, fmt='.2f', ax=axes[2])
            axes[2].set_title('相关性热力图')
        
        # 4-9. 其他图表
        for i in range(3, 9):
            axes[i].text(0.5, 0.5, f'图表 {i+1}', ha='center', va='center', transform=axes[i].transAxes)
            axes[i].set_title(f'图表 {i+1}')
        
        plt.tight_layout()
        plt.show()

# 使用示例
def demonstrate_visualization():
    """演示数据可视化"""
    # 创建示例数据
    np.random.seed(42)
    data = {
        'age': np.random.randint(18, 80, 1000),
        'salary': np.random.normal(50000, 15000, 1000),
        'experience': np.random.randint(0, 30, 1000),
        'performance_score': np.random.uniform(1, 10, 1000),
        'department': np.random.choice(['IT', 'HR', 'Finance', 'Marketing'], 1000),
        'education': np.random.choice(['Bachelor', 'Master', 'PhD'], 1000),
        'satisfaction': np.random.uniform(1, 5, 1000)
    }
    
    df = pd.DataFrame(data)
    visualizer = DataVisualizer(df)
    
    print("=== 数据可视化演示 ===")
    
    # 分布图
    print("绘制年龄分布图...")
    visualizer.plot_distribution('age')
    
    # 相关性热力图
    print("绘制相关性热力图...")
    visualizer.plot_correlation_heatmap()
    
    # 分类变量分布
    print("绘制部门分布图...")
    visualizer.plot_categorical_distribution('department')
    
    # 分组分析
    print("绘制薪资按部门分组的箱线图...")
    visualizer.plot_grouped_analysis('department', 'salary', 'box')
    
    # 散点图矩阵
    print("绘制散点图矩阵...")
    visualizer.plot_scatter_matrix(['age', 'salary', 'experience', 'performance_score'])

if __name__ == "__main__":
    demonstrate_visualization()
```

## 3. 高级数据处理

### 3.1 特征工程

```python
# feature_engineering.py
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder
from sklearn.feature_selection import SelectKBest, f_regression, f_classif
from sklearn.decomposition import PCA
from typing import List, Dict, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')

class FeatureEngineer:
    """特征工程师"""
    
    def __init__(self, data: pd.DataFrame):
        self.data = data.copy()
        self.numeric_columns = data.select_dtypes(include=[np.number]).columns.tolist()
        self.categorical_columns = data.select_dtypes(include=['object', 'category']).columns.tolist()
        self.feature_names = []
        self.scalers = {}
        self.encoders = {}
    
    def create_polynomial_features(self, columns: List[str], degree: int = 2) -> pd.DataFrame:
        """创建多项式特征"""
        from sklearn.preprocessing import PolynomialFeatures
        
        for col in columns:
            if col in self.numeric_columns:
                poly = PolynomialFeatures(degree=degree, include_bias=False)
                poly_features = poly.fit_transform(self.data[[col]])
                
                # 创建特征名称
                feature_names = [f"{col}_poly_{i}" for i in range(1, degree + 1)]
                
                # 添加到数据框
                for i, name in enumerate(feature_names):
                    self.data[name] = poly_features[:, i]
                    self.feature_names.append(name)
        
        return self.data
    
    def create_interaction_features(self, columns: List[str]) -> pd.DataFrame:
        """创建交互特征"""
        for i, col1 in enumerate(columns):
            for col2 in columns[i+1:]:
                if col1 in self.numeric_columns and col2 in self.numeric_columns:
                    interaction_name = f"{col1}_x_{col2}"
                    self.data[interaction_name] = self.data[col1] * self.data[col2]
                    self.feature_names.append(interaction_name)
        
        return self.data
    
    def create_binning_features(self, columns: List[str], bins: int = 5) -> pd.DataFrame:
        """创建分箱特征"""
        for col in columns:
            if col in self.numeric_columns:
                bin_name = f"{col}_bin"
                self.data[bin_name] = pd.cut(self.data[col], bins=bins, labels=False)
                self.feature_names.append(bin_name)
        
        return self.data
    
    def create_statistical_features(self, group_columns: List[str], 
                                  value_columns: List[str]) -> pd.DataFrame:
        """创建统计特征"""
        for group_col in group_columns:
            if group_col in self.categorical_columns:
                for value_col in value_columns:
                    if value_col in self.numeric_columns:
                        # 组内统计
                        group_stats = self.data.groupby(group_col)[value_col].agg([
                            'mean', 'std', 'min', 'max', 'median'
                        ]).add_prefix(f"{value_col}_group_")
                        
                        # 合并到原数据
                        self.data = self.data.merge(
                            group_stats, left_on=group_col, right_index=True, how='left'
                        )
                        
                        # 记录特征名称
                        for stat in ['mean', 'std', 'min', 'max', 'median']:
                            self.feature_names.append(f"{value_col}_group_{stat}")
        
        return self.data
    
    def create_time_features(self, date_column: str) -> pd.DataFrame:
        """创建时间特征"""
        if date_column not in self.data.columns:
            raise ValueError(f"日期列 {date_column} 不存在")
        
        # 确保是datetime类型
        if not pd.api.types.is_datetime64_any_dtype(self.data[date_column]):
            self.data[date_column] = pd.to_datetime(self.data[date_column])
        
        # 提取时间特征
        self.data[f"{date_column}_year"] = self.data[date_column].dt.year
        self.data[f"{date_column}_month"] = self.data[date_column].dt.month
        self.data[f"{date_column}_day"] = self.data[date_column].dt.day
        self.data[f"{date_column}_weekday"] = self.data[date_column].dt.weekday
        self.data[f"{date_column}_quarter"] = self.data[date_column].dt.quarter
        self.data[f"{date_column}_is_weekend"] = self.data[date_column].dt.weekday >= 5
        
        # 记录特征名称
        time_features = ['year', 'month', 'day', 'weekday', 'quarter', 'is_weekend']
        for feature in time_features:
            self.feature_names.append(f"{date_column}_{feature}")
        
        return self.data
    
    def encode_categorical_features(self, columns: List[str], 
                                  method: str = 'onehot') -> pd.DataFrame:
        """编码分类特征"""
        for col in columns:
            if col in self.categorical_columns:
                if method == 'onehot':
                    # 独热编码
                    dummies = pd.get_dummies(self.data[col], prefix=col)
                    self.data = pd.concat([self.data, dummies], axis=1)
                    self.feature_names.extend(dummies.columns.tolist())
                
                elif method == 'label':
                    # 标签编码
                    encoder = LabelEncoder()
                    self.data[f"{col}_encoded"] = encoder.fit_transform(self.data[col])
                    self.encoders[col] = encoder
                    self.feature_names.append(f"{col}_encoded")
                
                elif method == 'target':
                    # 目标编码（需要目标变量）
                    print(f"目标编码需要目标变量，跳过 {col}")
        
        return self.data
    
    def scale_features(self, columns: List[str], method: str = 'standard') -> pd.DataFrame:
        """特征缩放"""
        for col in columns:
            if col in self.numeric_columns:
                if method == 'standard':
                    scaler = StandardScaler()
                elif method == 'minmax':
                    scaler = MinMaxScaler()
                else:
                    raise ValueError("缩放方法必须是 'standard' 或 'minmax'")
                
                self.data[f"{col}_scaled"] = scaler.fit_transform(self.data[[col]])
                self.scalers[col] = scaler
                self.feature_names.append(f"{col}_scaled")
        
        return self.data
    
    def select_features(self, target_column: str, k: int = 10, 
                       method: str = 'f_regression') -> List[str]:
        """特征选择"""
        if target_column not in self.data.columns:
            raise ValueError(f"目标列 {target_column} 不存在")
        
        # 准备特征和目标
        feature_columns = [col for col in self.data.columns 
                          if col != target_column and col in self.numeric_columns]
        
        if len(feature_columns) == 0:
            return []
        
        X = self.data[feature_columns]
        y = self.data[target_column]
        
        # 选择特征
        if method == 'f_regression':
            selector = SelectKBest(score_func=f_regression, k=k)
        elif method == 'f_classif':
            selector = SelectKBest(score_func=f_classif, k=k)
        else:
            raise ValueError("特征选择方法必须是 'f_regression' 或 'f_classif'")
        
        selector.fit(X, y)
        selected_features = [feature_columns[i] for i in selector.get_support(indices=True)]
        
        return selected_features
    
    def apply_pca(self, columns: List[str], n_components: int = 2) -> pd.DataFrame:
        """应用主成分分析"""
        numeric_cols = [col for col in columns if col in self.numeric_columns]
        
        if len(numeric_cols) == 0:
            return self.data
        
        # 标准化数据
        scaler = StandardScaler()
        scaled_data = scaler.fit_transform(self.data[numeric_cols])
        
        # 应用PCA
        pca = PCA(n_components=n_components)
        pca_features = pca.fit_transform(scaled_data)
        
        # 添加到数据框
        for i in range(n_components):
            col_name = f"pca_{i+1}"
            self.data[col_name] = pca_features[:, i]
            self.feature_names.append(col_name)
        
        # 记录解释方差比
        print(f"PCA解释方差比: {pca.explained_variance_ratio_}")
        print(f"累计解释方差比: {np.cumsum(pca.explained_variance_ratio_)}")
        
        return self.data
    
    def get_feature_importance(self, target_column: str) -> pd.DataFrame:
        """获取特征重要性"""
        if target_column not in self.data.columns:
            raise ValueError(f"目标列 {target_column} 不存在")
        
        feature_columns = [col for col in self.data.columns 
                          if col != target_column and col in self.numeric_columns]
        
        if len(feature_columns) == 0:
            return pd.DataFrame()
        
        X = self.data[feature_columns]
        y = self.data[target_column]
        
        # 计算特征重要性
        selector = SelectKBest(score_func=f_regression, k='all')
        selector.fit(X, y)
        
        importance_df = pd.DataFrame({
            'feature': feature_columns,
            'score': selector.scores_,
            'p_value': selector.pvalues_
        }).sort_values('score', ascending=False)
        
        return importance_df

# 使用示例
def demonstrate_feature_engineering():
    """演示特征工程"""
    # 创建示例数据
    np.random.seed(42)
    data = {
        'age': np.random.randint(18, 80, 1000),
        'salary': np.random.normal(50000, 15000, 1000),
        'experience': np.random.randint(0, 30, 1000),
        'performance_score': np.random.uniform(1, 10, 1000),
        'department': np.random.choice(['IT', 'HR', 'Finance', 'Marketing'], 1000),
        'education': np.random.choice(['Bachelor', 'Master', 'PhD'], 1000),
        'join_date': pd.date_range('2020-01-01', periods=1000, freq='D'),
        'satisfaction': np.random.uniform(1, 5, 1000)
    }
    
    df = pd.DataFrame(data)
    engineer = FeatureEngineer(df)
    
    print("=== 特征工程演示 ===")
    print(f"原始数据形状: {engineer.data.shape}")
    
    # 创建多项式特征
    engineer.create_polynomial_features(['age', 'experience'], degree=2)
    print(f"添加多项式特征后: {engineer.data.shape}")
    
    # 创建交互特征
    engineer.create_interaction_features(['age', 'salary', 'experience'])
    print(f"添加交互特征后: {engineer.data.shape}")
    
    # 创建分箱特征
    engineer.create_binning_features(['age', 'salary'], bins=5)
    print(f"添加分箱特征后: {engineer.data.shape}")
    
    # 创建统计特征
    engineer.create_statistical_features(['department'], ['salary', 'performance_score'])
    print(f"添加统计特征后: {engineer.data.shape}")
    
    # 创建时间特征
    engineer.create_time_features('join_date')
    print(f"添加时间特征后: {engineer.data.shape}")
    
    # 编码分类特征
    engineer.encode_categorical_features(['department', 'education'], method='onehot')
    print(f"编码分类特征后: {engineer.data.shape}")
    
    # 特征缩放
    engineer.scale_features(['age', 'salary', 'experience'], method='standard')
    print(f"特征缩放后: {engineer.data.shape}")
    
    # 特征选择
    selected_features = engineer.select_features('satisfaction', k=10)
    print(f"选择的特征: {selected_features}")
    
    # 特征重要性
    importance_df = engineer.get_feature_importance('satisfaction')
    print("\n=== 特征重要性 ===")
    print(importance_df.head(10))
    
    print(f"\n最终数据形状: {engineer.data.shape}")
    print(f"创建的特征数量: {len(engineer.feature_names)}")

if __name__ == "__main__":
    demonstrate_feature_engineering()
```

## 4. 总结

### 4.1 数据处理最佳实践

1. **数据质量**：始终检查数据质量，处理缺失值和异常值
2. **特征工程**：创建有意义的特征，提高模型性能
3. **数据可视化**：使用可视化探索数据模式和关系
4. **性能优化**：使用适当的数据类型和数据结构
5. **可重现性**：设置随机种子，记录数据处理步骤

### 4.2 工具推荐

- **数据处理**：Pandas, NumPy, Polars
- **可视化**：Matplotlib, Seaborn, Plotly
- **特征工程**：Scikit-learn, Feature-engine
- **统计分析**：SciPy, Statsmodels
- **大数据处理**：Dask, Vaex

---

## 返回与相关

- 返回目录：[06-数据科学/README](../README.md)
- 相关文档：[数据分析与机器学习](./数据分析与机器学习.md)
- 示例项目：[pandas_sklearn_min](../examples/pandas_sklearn_min/)
