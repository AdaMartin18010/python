# Python数据科学与机器学习

## 目录

- [Python数据科学与机器学习](#python数据科学与机器学习)
  - [目录](#目录)
  - [概述](#概述)
    - [数据科学生态系统](#数据科学生态系统)
  - [数据处理与分析](#数据处理与分析)
    - [现代数据处理工具](#现代数据处理工具)
      - [Polars - 高性能数据处理](#polars---高性能数据处理)
      - [数据质量检查](#数据质量检查)
  - [机器学习框架](#机器学习框架)
    - [Scikit-learn 现代应用](#scikit-learn-现代应用)
  - [相关主题](#相关主题)

---

## 概述

Python数据科学与机器学习生态系统提供了从数据预处理到模型部署的完整解决方案。
通过NumPy、Pandas、Scikit-learn、PyTorch等强大的库，Python已成为数据科学和机器学习的首选语言。

### 数据科学生态系统

```python
# 数据科学生态系统
data_science_ecosystem = {
    "data_processing": {
        "numpy": {
            "description": "数值计算基础库",
            "features": ["多维数组", "线性代数", "随机数生成", "傅里叶变换"],
            "use_cases": ["数值计算", "科学计算", "数组操作"]
        },
        "pandas": {
            "description": "数据分析与操作库",
            "features": ["DataFrame", "数据清洗", "数据聚合", "时间序列"],
            "use_cases": ["数据分析", "数据预处理", "ETL操作"]
        },
        "polars": {
            "description": "高性能数据处理库",
            "features": ["Rust实现", "多线程", "内存高效", "Lazy API"],
            "use_cases": ["大数据处理", "高性能计算", "内存优化"]
        }
    },
    "machine_learning": {
        "scikit_learn": {
            "description": "传统机器学习库",
            "features": ["分类", "回归", "聚类", "降维", "模型选择"],
            "use_cases": ["传统ML", "特征工程", "模型评估"]
        },
        "xgboost": {
            "description": "梯度提升框架",
            "features": ["高性能", "可扩展", "特征重要性", "早停"],
            "use_cases": ["表格数据", "竞赛", "生产环境"]
        },
        "lightgbm": {
            "description": "轻量级梯度提升",
            "features": ["快速训练", "低内存", "GPU支持", "分类特征"],
            "use_cases": ["大规模数据", "实时预测", "特征工程"]
        }
    },
    "deep_learning": {
        "pytorch": {
            "description": "动态深度学习框架",
            "features": ["动态图", "Python原生", "研究友好", "部署灵活"],
            "use_cases": ["研究", "NLP", "计算机视觉", "强化学习"]
        },
        "tensorflow": {
            "description": "端到端机器学习平台",
            "features": ["静态图", "生产就绪", "TensorBoard", "移动端"],
            "use_cases": ["生产部署", "大规模训练", "移动应用"]
        }
    },
    "visualization": {
        "matplotlib": {
            "description": "基础绘图库",
            "features": ["2D绘图", "自定义样式", "动画", "交互式"],
            "use_cases": ["科学绘图", "报告生成", "数据分析"]
        },
        "seaborn": {
            "description": "统计可视化库",
            "features": ["统计图表", "美观样式", "多变量分析", "分布图"],
            "use_cases": ["探索性分析", "统计可视化", "数据报告"]
        },
        "plotly": {
            "description": "交互式可视化库",
            "features": ["交互式图表", "Web集成", "3D可视化", "仪表板"],
            "use_cases": ["Web应用", "仪表板", "交互式分析"]
        }
    }
}
```

---

## 数据处理与分析

### 现代数据处理工具

#### Polars - 高性能数据处理

```python
# Polars 高性能数据处理示例
import polars as pl
import numpy as np
from datetime import datetime, timedelta
import time

class ModernDataProcessor:
    """现代数据处理器"""
    
    def __init__(self):
        self.df = None
    
    def create_sample_data(self, n_rows: int = 1000000) -> pl.DataFrame:
        """创建示例数据"""
        # 生成随机数据
        np.random.seed(42)
        
        data = {
            "id": range(1, n_rows + 1),
            "name": [f"user_{i}" for i in range(1, n_rows + 1)],
            "age": np.random.randint(18, 80, n_rows),
            "salary": np.random.normal(50000, 15000, n_rows),
            "department": np.random.choice(["IT", "HR", "Finance", "Marketing"], n_rows),
            "join_date": [
                datetime.now() - timedelta(days=np.random.randint(0, 3650))
                for _ in range(n_rows)
            ],
            "score": np.random.uniform(0, 100, n_rows)
        }
        
        self.df = pl.DataFrame(data)
        return self.df
    
    def demonstrate_polars_features(self):
        """演示Polars特性"""
        if self.df is None:
            self.create_sample_data()
        
        print("=== Polars 特性演示 ===")
        
        # 1. 基本操作
        print("\n1. 基本数据操作:")
        print(f"数据形状: {self.df.shape}")
        print(f"列名: {self.df.columns}")
        print(f"数据类型: {self.df.dtypes}")
        
        # 2. 过滤和选择
        print("\n2. 数据过滤:")
        high_salary = self.df.filter(pl.col("salary") > 60000)
        print(f"高薪员工数量: {len(high_salary)}")
        
        # 3. 聚合操作
        print("\n3. 聚合操作:")
        dept_stats = self.df.group_by("department").agg([
            pl.col("salary").mean().alias("avg_salary"),
            pl.col("salary").std().alias("std_salary"),
            pl.col("age").mean().alias("avg_age"),
            pl.count().alias("count")
        ])
        print(dept_stats)
        
        # 4. 窗口函数
        print("\n4. 窗口函数:")
        ranked_df = self.df.with_columns([
            pl.col("salary").rank().over("department").alias("salary_rank"),
            pl.col("salary").quantile(0.8).over("department").alias("dept_80th_percentile")
        ])
        print(ranked_df.select(["name", "department", "salary", "salary_rank"]).head())
        
        # 5. 性能对比
        print("\n5. 性能测试:")
        self._performance_comparison()
    
    def _performance_comparison(self):
        """性能对比测试"""
        import pandas as pd
        
        # 创建测试数据
        n_rows = 100000
        test_data = {
            "id": range(n_rows),
            "value1": np.random.randn(n_rows),
            "value2": np.random.randn(n_rows),
            "category": np.random.choice(["A", "B", "C"], n_rows)
        }
        
        # Polars测试
        start_time = time.time()
        pl_df = pl.DataFrame(test_data)
        pl_result = pl_df.group_by("category").agg([
            pl.col("value1").sum(),
            pl.col("value2").mean()
        ])
        pl_time = time.time() - start_time
        
        # Pandas测试
        start_time = time.time()
        pd_df = pd.DataFrame(test_data)
        pd_result = pd_df.groupby("category").agg({
            "value1": "sum",
            "value2": "mean"
        })
        pd_time = time.time() - start_time
        
        print(f"Polars 时间: {pl_time:.4f}秒")
        print(f"Pandas 时间: {pd_time:.4f}秒")
        print(f"Polars 比 Pandas 快: {pd_time/pl_time:.2f}倍")
    
    def advanced_data_processing(self):
        """高级数据处理"""
        if self.df is None:
            self.create_sample_data()
        
        print("\n=== 高级数据处理 ===")
        
        # 1. 复杂查询
        print("\n1. 复杂查询:")
        complex_query = self.df.filter(
            (pl.col("age") >= 25) & 
            (pl.col("age") <= 55) &
            (pl.col("salary") > pl.col("salary").quantile(0.7))
        ).select([
            "name", "age", "salary", "department"
        ]).sort("salary", descending=True)
        
        print(f"符合条件的员工数量: {len(complex_query)}")
        print(complex_query.head())
        
        # 2. 数据转换
        print("\n2. 数据转换:")
        transformed_df = self.df.with_columns([
            pl.col("salary").log().alias("log_salary"),
            pl.col("age").cast(pl.Float32).alias("age_float"),
            pl.when(pl.col("salary") > 60000)
            .then(pl.lit("High"))
            .otherwise(pl.lit("Low"))
            .alias("salary_level")
        ])
        
        print(transformed_df.select(["name", "salary", "log_salary", "salary_level"]).head())
        
        # 3. 时间序列处理
        print("\n3. 时间序列处理:")
        time_series = self.df.with_columns([
            pl.col("join_date").dt.year().alias("join_year"),
            pl.col("join_date").dt.month().alias("join_month"),
            (pl.col("join_date").dt.year() - 2020).alias("years_since_2020")
        ])
        
        yearly_stats = time_series.group_by("join_year").agg([
            pl.count().alias("employee_count"),
            pl.col("salary").mean().alias("avg_salary")
        ]).sort("join_year")
        
        print(yearly_stats)
```

#### 数据质量检查

```python
# 数据质量检查工具
class DataQualityChecker:
    """数据质量检查器"""
    
    def __init__(self, df: pl.DataFrame):
        self.df = df
        self.quality_report = {}
    
    def check_missing_values(self) -> Dict[str, Any]:
        """检查缺失值"""
        missing_stats = {}
        
        for col in self.df.columns:
            null_count = self.df.select(pl.col(col).is_null().sum()).item()
            null_percentage = (null_count / len(self.df)) * 100
            
            missing_stats[col] = {
                "null_count": null_count,
                "null_percentage": null_percentage,
                "status": "good" if null_percentage < 5 else "warning" if null_percentage < 20 else "critical"
            }
        
        self.quality_report["missing_values"] = missing_stats
        return missing_stats
    
    def check_data_types(self) -> Dict[str, Any]:
        """检查数据类型"""
        type_stats = {}
        
        for col in self.df.columns:
            dtype = self.df.select(pl.col(col)).dtypes[0]
            unique_count = self.df.select(pl.col(col).n_unique()).item()
            
            type_stats[col] = {
                "dtype": str(dtype),
                "unique_count": unique_count,
                "cardinality": unique_count / len(self.df)
            }
        
        self.quality_report["data_types"] = type_stats
        return type_stats
    
    def check_outliers(self, numeric_columns: List[str]) -> Dict[str, Any]:
        """检查异常值"""
        outlier_stats = {}
        
        for col in numeric_columns:
            if col in self.df.columns:
                # 计算四分位数
                q1 = self.df.select(pl.col(col).quantile(0.25)).item()
                q3 = self.df.select(pl.col(col).quantile(0.75)).item()
                iqr = q3 - q1
                
                # 定义异常值边界
                lower_bound = q1 - 1.5 * iqr
                upper_bound = q3 + 1.5 * iqr
                
                # 计算异常值数量
                outlier_count = self.df.filter(
                    (pl.col(col) < lower_bound) | (pl.col(col) > upper_bound)
                ).height
                
                outlier_stats[col] = {
                    "q1": q1,
                    "q3": q3,
                    "iqr": iqr,
                    "lower_bound": lower_bound,
                    "upper_bound": upper_bound,
                    "outlier_count": outlier_count,
                    "outlier_percentage": (outlier_count / len(self.df)) * 100
                }
        
        self.quality_report["outliers"] = outlier_stats
        return outlier_stats
    
    def check_duplicates(self) -> Dict[str, Any]:
        """检查重复值"""
        total_duplicates = self.df.height - self.df.unique().height
        duplicate_percentage = (total_duplicates / self.df.height) * 100
        
        duplicate_stats = {
            "total_duplicates": total_duplicates,
            "duplicate_percentage": duplicate_percentage,
            "status": "good" if duplicate_percentage < 1 else "warning" if duplicate_percentage < 5 else "critical"
        }
        
        self.quality_report["duplicates"] = duplicate_stats
        return duplicate_stats
    
    def generate_quality_report(self) -> str:
        """生成质量报告"""
        report = "数据质量检查报告\n"
        report += "=" * 50 + "\n\n"
        
        # 基本信息
        report += f"数据形状: {self.df.shape}\n"
        report += f"列数: {len(self.df.columns)}\n"
        report += f"行数: {len(self.df)}\n\n"
        
        # 缺失值检查
        if "missing_values" in self.quality_report:
            report += "缺失值检查:\n"
            for col, stats in self.quality_report["missing_values"].items():
                report += f"  {col}: {stats['null_count']} ({stats['null_percentage']:.2f}%) - {stats['status']}\n"
            report += "\n"
        
        # 重复值检查
        if "duplicates" in self.quality_report:
            report += "重复值检查:\n"
            stats = self.quality_report["duplicates"]
            report += f"  重复行数: {stats['total_duplicates']} ({stats['duplicate_percentage']:.2f}%) - {stats['status']}\n\n"
        
        # 异常值检查
        if "outliers" in self.quality_report:
            report += "异常值检查:\n"
            for col, stats in self.quality_report["outliers"].items():
                report += f"  {col}: {stats['outlier_count']} ({stats['outlier_percentage']:.2f}%)\n"
            report += "\n"
        
        return report
```

---

## 机器学习框架

### Scikit-learn 现代应用

```python
# Scikit-learn 现代机器学习应用
from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, mean_squared_error
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
import joblib
import numpy as np

class ModernMLPipeline:
    """现代机器学习管道"""
    
    def __init__(self):
        self.preprocessor = None
        self.model = None
        self.feature_names = None
        self.target_name = None
    
    def create_sample_dataset(self, n_samples: int = 10000) -> Tuple[np.ndarray, np.ndarray]:
        """创建示例数据集"""
        np.random.seed(42)
        
        # 生成特征
        n_features = 20
        X = np.random.randn(n_samples, n_features)
        
        # 添加一些相关性
        X[:, 0] = X[:, 0] * 2 + X[:, 1]  # 特征0与特征1相关
        X[:, 5] = X[:, 5] * 0.5 + X[:, 6] * 0.5  # 特征5与特征6相关
        
        # 生成目标变量（分类）
        y = (X[:, 0] + X[:, 1] + X[:, 2] + np.random.randn(n_samples) * 0.1 > 0).astype(int)
        
        # 添加一些噪声
        noise_indices = np.random.choice(n_samples, size=int(n_samples * 0.05), replace=False)
        y[noise_indices] = 1 - y[noise_indices]
        
        self.feature_names = [f"feature_{i}" for i in range(n_features)]
        self.target_name = "target"
        
        return X, y
    
    def build_preprocessing_pipeline(self, X: np.ndarray) -> ColumnTransformer:
        """构建预处理管道"""
        # 数值特征预处理
        numeric_transformer = Pipeline(steps=[
            ('scaler', StandardScaler())
        ])
        
        # 分类特征预处理（如果有的话）
        categorical_transformer = Pipeline(steps=[
            ('encoder', LabelEncoder())
        ])
        
        # 组合预处理器
        preprocessor = ColumnTransformer(
            transformers=[
                ('num', numeric_transformer, list(range(X.shape[1])))
            ]
        )
        
        self.preprocessor = preprocessor
        return preprocessor
    
    def train_classification_model(self, X: np.ndarray, y: np.ndarray) -> RandomForestClassifier:
        """训练分类模型"""
        # 数据分割
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        # 构建完整管道
        pipeline = Pipeline(steps=[
            ('preprocessor', self.preprocessor),
            ('classifier', RandomForestClassifier(
                n_estimators=100,
                random_state=42,
                n_jobs=-1
            ))
        ])
        
        # 训练模型
        pipeline.fit(X_train, y_train)
        
        # 评估模型
        y_pred = pipeline.predict(X_test)
        y_pred_proba = pipeline.predict_proba(X_test)
        
        print("分类模型评估:")
        print(classification_report(y_test, y_pred))
        print("\n混淆矩阵:")
        print(confusion_matrix(y_test, y_pred))
        
        # 交叉验证
        cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='accuracy')
        print(f"\n交叉验证准确率: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")
        
        self.model = pipeline
        return pipeline
    
    def hyperparameter_tuning(self, X: np.ndarray, y: np.ndarray) -> Dict[str, Any]:
        """超参数调优"""
        # 定义参数网格
        param_grid = {
            'classifier__n_estimators': [50, 100, 200],
            'classifier__max_depth': [None, 10, 20, 30],
            'classifier__min_samples_split': [2, 5, 10],
            'classifier__min_samples_leaf': [1, 2, 4]
        }
        
        # 构建管道
        pipeline = Pipeline(steps=[
            ('preprocessor', self.preprocessor),
            ('classifier', RandomForestClassifier(random_state=42, n_jobs=-1))
        ])
        
        # 网格搜索
        grid_search = GridSearchCV(
            pipeline, 
            param_grid, 
            cv=3, 
            scoring='accuracy',
            n_jobs=-1,
            verbose=1
        )
        
        grid_search.fit(X, y)
        
        print("最佳参数:")
        print(grid_search.best_params_)
        print(f"最佳交叉验证分数: {grid_search.best_score_:.4f}")
        
        return {
            'best_params': grid_search.best_params_,
            'best_score': grid_search.best_score_,
            'best_estimator': grid_search.best_estimator_
        }
    
    def feature_importance_analysis(self) -> Dict[str, float]:
        """特征重要性分析"""
        if self.model is None:
            raise ValueError("模型尚未训练")
        
        # 获取特征重要性
        feature_importance = self.model.named_steps['classifier'].feature_importances_
        
        # 创建特征重要性字典
        importance_dict = dict(zip(self.feature_names, feature_importance))
        
        # 按重要性排序
        sorted_importance = dict(sorted(importance_dict.items(), key=lambda x: x[1], reverse=True))
        
        print("特征重要性 (前10个):")
        for i, (feature, importance) in enumerate(list(sorted_importance.items())[:10]):
            print(f"{i+1:2d}. {feature}: {importance:.4f}")
        
        return sorted_importance
    
    def model_persistence(self, model_path: str = "model.pkl"):
        """模型持久化"""
        if self.model is None:
            raise ValueError("模型尚未训练")
        
        # 保存模型
        joblib.dump(self.model, model_path)
        print(f"模型已保存到: {model_path}")
        
        # 加载模型测试
        loaded_model = joblib.load(model_path)
        print("模型加载成功")
        
        return loaded_model
```

---

## 相关主题

- [Python语言新特性](./../../01-语言与生态/迁移/01-语言新特性.md)
- [Python技术栈2025](./../../01-语言与生态/迁移/02-技术栈2025.md)
- [质量检查](./../../02-测试与质量/迁移/质量检查.md)
- [项目管理](./../../03-工程与交付/迁移/项目管理.md)
- [现代Web框架](./../../05-Web开发/迁移/现代Web框架.md)

---

**下一主题**: [并发与异步](./../../04-并发与异步/README.md)
