"""
å®Œæ•´çš„RAGèŠå¤©æœºå™¨äººç¤ºä¾‹
ä½¿ç”¨ LangChain + OpenAI + Qdrant

åŠŸèƒ½ï¼š
- æ–‡æ¡£æ‘„å–å’Œå‘é‡åŒ–
- è¯­ä¹‰æœç´¢
- ä¸Šä¸‹æ–‡å¢å¼ºå›ç­”
- å¯¹è¯å†å²
- æµå¼è¾“å‡º

è¿è¡Œæ–¹å¼:
    uv add langchain langchain-openai langchain-community qdrant-client tiktoken
    
    export OPENAI_API_KEY="your-api-key"
    python rag_chatbot.py
"""

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.schema import HumanMessage, AIMessage, SystemMessage
from langchain.schema.runnable import RunnablePassthrough
from langchain_community.vectorstores import Qdrant
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams
from typing import List, Dict, Tuple
import asyncio
import os
from datetime import datetime

# ============ é…ç½® ============

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "your-api-key-here")
QDRANT_URL = "http://localhost:6333"  # æœ¬åœ°Qdrant
COLLECTION_NAME = "knowledge_base"

# ============ RAGèŠå¤©æœºå™¨äºº ============

class RAGChatbot:
    """RAGèŠå¤©æœºå™¨äºº"""
    
    def __init__(
        self,
        model_name: str = "gpt-4-turbo",
        temperature: float = 0.7,
        top_k: int = 3
    ):
        """åˆå§‹åŒ–èŠå¤©æœºå™¨äºº"""
        
        # LLM
        self.llm = ChatOpenAI(
            model=model_name,
            temperature=temperature,
            api_key=OPENAI_API_KEY,
            streaming=True
        )
        
        # åµŒå…¥æ¨¡å‹
        self.embeddings = OpenAIEmbeddings(
            model="text-embedding-3-large",
            api_key=OPENAI_API_KEY
        )
        
        # å‘é‡æ•°æ®åº“å®¢æˆ·ç«¯
        try:
            self.qdrant_client = QdrantClient(url=QDRANT_URL)
            self._ensure_collection()
        except Exception as e:
            print(f"âš ï¸  Warning: Could not connect to Qdrant: {e}")
            print("   Running in-memory mode (documents will not persist)")
            self.qdrant_client = QdrantClient(":memory:")
            self._ensure_collection()
        
        # å‘é‡å­˜å‚¨
        self.vector_store = Qdrant(
            client=self.qdrant_client,
            collection_name=COLLECTION_NAME,
            embeddings=self.embeddings
        )
        
        # æ–‡æœ¬åˆ†å‰²å™¨
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", ".", "!", "?", " ", ""]
        )
        
        # å¯¹è¯å†å²
        self.conversation_history: List[Tuple[str, str]] = []
        
        # é…ç½®
        self.top_k = top_k
        
        print("âœ“ RAG Chatbot initialized")
    
    def _ensure_collection(self):
        """ç¡®ä¿é›†åˆå­˜åœ¨"""
        try:
            self.qdrant_client.get_collection(COLLECTION_NAME)
            print(f"âœ“ Using existing collection: {COLLECTION_NAME}")
        except:
            self.qdrant_client.create_collection(
                collection_name=COLLECTION_NAME,
                vectors_config=VectorParams(
                    size=3072,  # text-embedding-3-large dimension
                    distance=Distance.COSINE
                )
            )
            print(f"âœ“ Created new collection: {COLLECTION_NAME}")
    
    def ingest_documents(self, documents: List[str], metadatas: List[Dict] = None):
        """
        æ‘„å–æ–‡æ¡£
        
        Args:
            documents: æ–‡æ¡£æ–‡æœ¬åˆ—è¡¨
            metadatas: æ–‡æ¡£å…ƒæ•°æ®åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
        """
        print(f"\nğŸ“¥ Ingesting {len(documents)} documents...")
        
        # åˆ†å‰²æ–‡æ¡£
        all_splits = []
        all_metadatas = []
        
        for idx, doc in enumerate(documents):
            splits = self.text_splitter.split_text(doc)
            all_splits.extend(splits)
            
            # æ·»åŠ å…ƒæ•°æ®
            if metadatas and idx < len(metadatas):
                metadata = metadatas[idx]
            else:
                metadata = {"source": f"document_{idx}"}
            
            metadata["ingested_at"] = datetime.now().isoformat()
            all_metadatas.extend([metadata] * len(splits))
        
        # æ·»åŠ åˆ°å‘é‡æ•°æ®åº“
        self.vector_store.add_texts(
            texts=all_splits,
            metadatas=all_metadatas
        )
        
        print(f"âœ“ Ingested {len(all_splits)} chunks")
    
    def search(self, query: str, top_k: int = None) -> List[Dict]:
        """
        æœç´¢ç›¸å…³æ–‡æ¡£
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡
        
        Returns:
            ç›¸å…³æ–‡æ¡£åˆ—è¡¨
        """
        if top_k is None:
            top_k = self.top_k
        
        results = self.vector_store.similarity_search_with_score(
            query, k=top_k
        )
        
        formatted_results = []
        for doc, score in results:
            formatted_results.append({
                "content": doc.page_content,
                "metadata": doc.metadata,
                "score": float(score)
            })
        
        return formatted_results
    
    def _build_prompt(self, query: str, context_docs: List[Dict]) -> str:
        """æ„å»ºæç¤ºè¯"""
        
        # æ„å»ºä¸Šä¸‹æ–‡
        context_parts = []
        for idx, doc in enumerate(context_docs, 1):
            context_parts.append(f"[æ–‡æ¡£ {idx}]\n{doc['content']}\n")
        
        context = "\n".join(context_parts)
        
        # æ„å»ºå¯¹è¯å†å²
        history_text = ""
        if self.conversation_history:
            history_parts = []
            for human_msg, ai_msg in self.conversation_history[-3:]:  # åªä¿ç•™æœ€è¿‘3è½®
                history_parts.append(f"ç”¨æˆ·: {human_msg}")
                history_parts.append(f"åŠ©æ‰‹: {ai_msg}")
            history_text = "\n".join(history_parts)
        
        # æ„å»ºå®Œæ•´æç¤º
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
{context}

"""
        
        if history_text:
            prompt += f"""å¯¹è¯å†å²ï¼š
{history_text}

"""
        
        prompt += f"""å½“å‰é—®é¢˜ï¼š
{query}

è¯·æä¾›è¯¦ç»†ä¸”å‡†ç¡®çš„å›ç­”ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯šå®åœ°è¯´"æˆ‘ä¸çŸ¥é“"æˆ–"ä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰æä¾›è¶³å¤Ÿçš„ä¿¡æ¯"ã€‚

å›ç­”ï¼š"""
        
        return prompt
    
    async def chat(self, query: str, stream: bool = False) -> str:
        """
        ä¸èŠå¤©æœºå™¨äººå¯¹è¯
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            stream: æ˜¯å¦æµå¼è¾“å‡º
        
        Returns:
            AIå›ç­”
        """
        print(f"\nğŸ’¬ User: {query}")
        
        # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
        print(f"ğŸ” Searching knowledge base...")
        relevant_docs = self.search(query, top_k=self.top_k)
        
        if not relevant_docs:
            response = "æŠ±æ­‰ï¼Œæˆ‘çš„çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"
            self.conversation_history.append((query, response))
            print(f"ğŸ¤– Assistant: {response}")
            return response
        
        print(f"âœ“ Found {len(relevant_docs)} relevant documents")
        
        # 2. æ„å»ºæç¤º
        prompt = self._build_prompt(query, relevant_docs)
        
        # 3. ç”Ÿæˆå›ç­”
        if stream:
            print("ğŸ¤– Assistant: ", end="", flush=True)
            response_parts = []
            
            async for chunk in self.llm.astream([HumanMessage(content=prompt)]):
                if hasattr(chunk, 'content'):
                    content = chunk.content
                    response_parts.append(content)
                    print(content, end="", flush=True)
            
            print()  # æ¢è¡Œ
            response = "".join(response_parts)
        else:
            print("ğŸ¤– Assistant: ", end="", flush=True)
            result = await self.llm.ainvoke([HumanMessage(content=prompt)])
            response = result.content
            print(response)
        
        # 4. ä¿å­˜å¯¹è¯å†å²
        self.conversation_history.append((query, response))
        
        return response
    
    def clear_history(self):
        """æ¸…é™¤å¯¹è¯å†å²"""
        self.conversation_history = []
        print("âœ“ Conversation history cleared")
    
    def show_stats(self):
        """æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯"""
        try:
            collection_info = self.qdrant_client.get_collection(COLLECTION_NAME)
            print(f"\nğŸ“Š Knowledge Base Stats:")
            print(f"  Total vectors: {collection_info.points_count}")
            print(f"  Conversation turns: {len(self.conversation_history)}")
        except Exception as e:
            print(f"âš ï¸  Could not get stats: {e}")


# ============ ç¤ºä¾‹ä½¿ç”¨ ============

async def main():
    """ä¸»å‡½æ•°"""
    
    print("="*60)
    print("ğŸ¤– RAG Chatbot Demo")
    print("="*60)
    
    # åˆå§‹åŒ–èŠå¤©æœºå™¨äºº
    chatbot = RAGChatbot(
        model_name="gpt-4-turbo",
        temperature=0.7,
        top_k=3
    )
    
    # ç¤ºä¾‹æ–‡æ¡£
    sample_documents = [
        """
        Python 3.13çš„é©å‘½æ€§ç‰¹æ€§
        
        Python 3.13å¼•å…¥äº†ä¸¤ä¸ªé©å‘½æ€§ç‰¹æ€§ï¼šFree-Threadedæ¨¡å¼å’ŒJITç¼–è¯‘å™¨ã€‚
        
        Free-Threadedæ¨¡å¼ï¼ˆæ— GILï¼‰ï¼š
        - ç§»é™¤äº†å…¨å±€è§£é‡Šå™¨é”ï¼ˆGILï¼‰
        - å…è®¸çœŸæ­£çš„å¤šçº¿ç¨‹å¹¶è¡Œæ‰§è¡Œ
        - åœ¨å¤šæ ¸CPUä¸Šæ€§èƒ½æå‡2-8å€
        - ç‰¹åˆ«é€‚åˆCPUå¯†é›†å‹ä»»åŠ¡
        
        JITç¼–è¯‘å™¨ï¼š
        - å³æ—¶ç¼–è¯‘æŠ€æœ¯
        - å¯ä»¥å°†çƒ­ç‚¹ä»£ç ç¼–è¯‘ä¸ºæœºå™¨ç 
        - çº¯Pythonä»£ç æ€§èƒ½æå‡20-60%
        - æ— éœ€ä¿®æ”¹ä»£ç å³å¯è·å¾—åŠ é€Ÿ
        
        è¿™ä¸¤ä¸ªç‰¹æ€§æ ‡å¿—ç€Pythonåœ¨é«˜æ€§èƒ½è®¡ç®—é¢†åŸŸè¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚
        """,
        
        """
        ç°ä»£Pythonå¼€å‘å·¥å…·é“¾ï¼ˆ2025ï¼‰
        
        uv - è¶…å¿«é€ŸåŒ…ç®¡ç†å™¨ï¼š
        - ä½¿ç”¨Rustç¼–å†™
        - æ¯”pipå¿«10-100å€
        - ç»Ÿä¸€ç®¡ç†ä¾èµ–ã€è™šæ‹Ÿç¯å¢ƒ
        - æ”¯æŒpyproject.toml
        
        ruff - æé€Ÿä»£ç æ£€æŸ¥å™¨ï¼š
        - æ¯”Blackå¿«90å€
        - é›†æˆäº†10+ä¸ªå·¥å…·çš„åŠŸèƒ½
        - æ”¯æŒè‡ªåŠ¨ä¿®å¤
        - é…ç½®ç®€å•
        
        mypy - é™æ€ç±»å‹æ£€æŸ¥ï¼š
        - æ•è·ç±»å‹é”™è¯¯
        - æé«˜ä»£ç è´¨é‡
        - æ”¯æŒæ¸è¿›å¼ç±»å‹æ³¨è§£
        
        pytest - ç°ä»£æµ‹è¯•æ¡†æ¶ï¼š
        - ç®€æ´çš„æµ‹è¯•è¯­æ³•
        - ä¸°å¯Œçš„æ’ä»¶ç”Ÿæ€
        - æ”¯æŒå¼‚æ­¥æµ‹è¯•
        """,
        
        """
        FastAPIæœ€ä½³å®è·µï¼ˆ2025ï¼‰
        
        FastAPIæ˜¯2025å¹´æœ€å—æ¬¢è¿çš„Python Webæ¡†æ¶ã€‚
        
        æ ¸å¿ƒä¼˜åŠ¿ï¼š
        1. é«˜æ€§èƒ½ï¼šåŸºäºASGIï¼Œæ€§èƒ½åª²ç¾Node.js
        2. è‡ªåŠ¨æ–‡æ¡£ï¼šç”ŸæˆOpenAPIå’ŒSwaggeræ–‡æ¡£
        3. ç±»å‹å®‰å…¨ï¼šåŸºäºPydanticï¼Œ100%ç±»å‹æ³¨è§£
        4. å¼‚æ­¥æ”¯æŒï¼šåŸç”Ÿæ”¯æŒasync/await
        
        æœ€ä½³å®è·µï¼š
        - ä½¿ç”¨ä¾èµ–æ³¨å…¥ç®¡ç†æ•°æ®åº“è¿æ¥
        - ä½¿ç”¨Pydanticæ¨¡å‹éªŒè¯æ•°æ®
        - ä½¿ç”¨åå°ä»»åŠ¡å¤„ç†è€—æ—¶æ“ä½œ
        - ä½¿ç”¨uvicornä½œä¸ºç”Ÿäº§æœåŠ¡å™¨
        - é…ç½®CORSå’Œå®‰å…¨å¤´
        
        æ€§èƒ½ä¼˜åŒ–ï¼š
        - ä½¿ç”¨Redisç¼“å­˜
        - ä½¿ç”¨è¿æ¥æ± 
        - å¯ç”¨å“åº”å‹ç¼©
        - ä½¿ç”¨CDNåˆ†å‘é™æ€èµ„æº
        """
    ]
    
    # æ‘„å–æ–‡æ¡£
    chatbot.ingest_documents(
        documents=sample_documents,
        metadatas=[
            {"source": "python_313_features.md", "category": "language"},
            {"source": "modern_toolchain.md", "category": "tools"},
            {"source": "fastapi_best_practices.md", "category": "web"}
        ]
    )
    
    # æ˜¾ç¤ºç»Ÿè®¡
    chatbot.show_stats()
    
    # ç¤ºä¾‹å¯¹è¯
    print("\n" + "="*60)
    print("ğŸ’¬ Sample Conversations")
    print("="*60)
    
    # é—®é¢˜1
    await chatbot.chat(
        "Python 3.13æœ‰å“ªäº›æ–°ç‰¹æ€§ï¼Ÿæ€§èƒ½æå‡å¦‚ä½•ï¼Ÿ",
        stream=True
    )
    
    # é—®é¢˜2
    await chatbot.chat(
        "æ¨èä¸€äº›ç°ä»£Pythonå¼€å‘å·¥å…·",
        stream=True
    )
    
    # é—®é¢˜3ï¼ˆåŸºäºä¸Šä¸‹æ–‡çš„è¿½é—®ï¼‰
    await chatbot.chat(
        "uvç›¸æ¯”pipæœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ",
        stream=True
    )
    
    # é—®é¢˜4
    await chatbot.chat(
        "ä½¿ç”¨FastAPIéœ€è¦æ³¨æ„ä»€ä¹ˆï¼Ÿ",
        stream=True
    )
    
    # é—®é¢˜5ï¼ˆçŸ¥è¯†åº“å¤–çš„é—®é¢˜ï¼‰
    await chatbot.chat(
        "å¦‚ä½•åšçº¢çƒ§è‚‰ï¼Ÿ",
        stream=True
    )
    
    # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
    print("\n" + "="*60)
    chatbot.show_stats()
    print("="*60)
    
    # äº¤äº’æ¨¡å¼ï¼ˆå¯é€‰ï¼‰
    print("\nğŸ’¡ Tip: You can now ask questions interactively")
    print("   Type 'quit' to exit, 'clear' to clear history\n")
    
    while True:
        try:
            user_input = input("You: ").strip()
            
            if not user_input:
                continue
            
            if user_input.lower() in ['quit', 'exit', 'q']:
                print("ğŸ‘‹ Goodbye!")
                break
            
            if user_input.lower() == 'clear':
                chatbot.clear_history()
                continue
            
            if user_input.lower() == 'stats':
                chatbot.show_stats()
                continue
            
            await chatbot.chat(user_input, stream=True)
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ Goodbye!")
            break
        except Exception as e:
            print(f"âŒ Error: {e}")


if __name__ == "__main__":
    # æ£€æŸ¥APIå¯†é’¥
    if OPENAI_API_KEY == "your-api-key-here":
        print("âš ï¸  Warning: Please set OPENAI_API_KEY environment variable")
        print("   export OPENAI_API_KEY='your-api-key'")
        exit(1)
    
    # è¿è¡Œ
    asyncio.run(main())

