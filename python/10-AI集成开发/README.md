# Python AIé›†æˆå¼€å‘å®Œæ•´æŒ‡å— (2025)

**æœ€åæ›´æ–°ï¼š** 2025å¹´10æœˆ24æ—¥  
**çŠ¶æ€ï¼š** âœ… ç”Ÿäº§å°±ç»ª

---

## ğŸ“‹ ç›®å½•

- [Python AIé›†æˆå¼€å‘å®Œæ•´æŒ‡å— (2025)](#python-aié›†æˆå¼€å‘å®Œæ•´æŒ‡å—-2025)
  - [ğŸ“‹ ç›®å½•](#-ç›®å½•)
  - [ğŸš€ æŠ€æœ¯æ ˆæ¦‚è§ˆ](#-æŠ€æœ¯æ ˆæ¦‚è§ˆ)
    - [2025å¹´æ¨èAIæŠ€æœ¯æ ˆ](#2025å¹´æ¨èaiæŠ€æœ¯æ ˆ)
    - [AIåº”ç”¨æ¶æ„æ¼”è¿›](#aiåº”ç”¨æ¶æ„æ¼”è¿›)
    - [æˆæœ¬å¯¹æ¯”ï¼ˆ2025ï¼‰](#æˆæœ¬å¯¹æ¯”2025)
  - [ğŸ¦œ LangChain 3.0](#-langchain-30)
    - [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
      - [1. å®‰è£…ä¾èµ–](#1-å®‰è£…ä¾èµ–)
      - [2. åŸºç¡€LLMè°ƒç”¨](#2-åŸºç¡€llmè°ƒç”¨)
    - [3. LangChain Chains](#3-langchain-chains)
    - [4. LangGraph - å¤æ‚å·¥ä½œæµ](#4-langgraph---å¤æ‚å·¥ä½œæµ)
  - [ğŸ¤– AI Agentå¼€å‘](#-ai-agentå¼€å‘)
    - [1. Function Calling Agent](#1-function-calling-agent)
    - [2. AutoGPTé£æ ¼Agent](#2-autogpté£æ ¼agent)
  - [ğŸ—‚ï¸ å‘é‡æ•°æ®åº“](#ï¸-å‘é‡æ•°æ®åº“)
    - [Qdranté›†æˆ](#qdranté›†æˆ)
  - [ğŸ“ RAGç³»ç»Ÿ](#-ragç³»ç»Ÿ)
    - [å®Œæ•´RAGå®ç°](#å®Œæ•´ragå®ç°)
  - [ğŸ“Š AIç›‘æ§ä¸è¯„ä¼°](#-aiç›‘æ§ä¸è¯„ä¼°)
    - [LangSmithé›†æˆ](#langsmithé›†æˆ)
    - [æ€§èƒ½è¯„ä¼°](#æ€§èƒ½è¯„ä¼°)
  - [ğŸ’¡ ç”Ÿäº§æœ€ä½³å®è·µ](#-ç”Ÿäº§æœ€ä½³å®è·µ)
    - [1. é”™è¯¯å¤„ç†ä¸é‡è¯•](#1-é”™è¯¯å¤„ç†ä¸é‡è¯•)
    - [2. æˆæœ¬æ§åˆ¶](#2-æˆæœ¬æ§åˆ¶)
    - [3. ç¼“å­˜ç­–ç•¥](#3-ç¼“å­˜ç­–ç•¥)
  - [ğŸ“š å‚è€ƒèµ„æº](#-å‚è€ƒèµ„æº)
    - [å®˜æ–¹æ–‡æ¡£](#å®˜æ–¹æ–‡æ¡£)
    - [å­¦ä¹ èµ„æº](#å­¦ä¹ èµ„æº)

---

## ğŸš€ æŠ€æœ¯æ ˆæ¦‚è§ˆ

### 2025å¹´æ¨èAIæŠ€æœ¯æ ˆ

| ç±»åˆ« | å·¥å…· | ç‰ˆæœ¬ | ç”¨é€” |
|------|------|------|------|
| **æ¡†æ¶** | LangChain | 3.0+ | AIåº”ç”¨å¼€å‘æ¡†æ¶ |
| **AI Agent** | AutoGPT | 2025+ | è‡ªä¸»AIä»£ç† |
| **LLM** | OpenAI GPT-4/GPT-5 | - | å¤§è¯­è¨€æ¨¡å‹ |
| **æœ¬åœ°LLM** | Ollama | 0.5+ | æœ¬åœ°æ¨¡å‹è¿è¡Œ |
| **åµŒå…¥æ¨¡å‹** | OpenAI Embeddings | text-embedding-3 | æ–‡æœ¬å‘é‡åŒ– |
| **å‘é‡æ•°æ®åº“** | Qdrant | 1.12+ | å‘é‡å­˜å‚¨å’Œæœç´¢ |
| **å‘é‡æ•°æ®åº“** | Weaviate | 1.27+ | AIåŸç”Ÿå‘é‡æ•°æ®åº“ |
| **æ·±åº¦å­¦ä¹ ** | PyTorch | 2.5+ | æ¨¡å‹è®­ç»ƒ |
| **æ¨ç†æœåŠ¡** | vLLM | 0.6+ | é«˜æ€§èƒ½æ¨ç† |
| **æ¨¡å‹é‡åŒ–** | BitsAndBytes | 0.44+ | æ¨¡å‹å‹ç¼© |
| **ç›‘æ§** | LangSmith | - | LangChainç›‘æ§ |

### AIåº”ç”¨æ¶æ„æ¼”è¿›

```text
ä¼ ç»Ÿæ¶æ„              â†’  2024æ¶æ„           â†’  2025æ¶æ„
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
è§„åˆ™å¼•æ“              â†’  LLMè°ƒç”¨          â†’  Multi-Agentç³»ç»Ÿ
å›ºå®šæµç¨‹              â†’  Promptå·¥ç¨‹       â†’  è‡ªä¸»è§„åˆ’
æœ¬åœ°æ•°æ®              â†’  RAGæ£€ç´¢          â†’  çŸ¥è¯†å›¾è°±èåˆ
å•æ¨¡å‹                â†’  æ¨¡å‹ç»„åˆ         â†’  æ¨¡å‹å¾®è°ƒ+å·¥å…·è°ƒç”¨
åç«¯æ‰¹å¤„ç†            â†’  å®æ—¶æ¨ç†         â†’  æµå¼è¾“å‡º+å¹¶è¡Œæ¨ç†
```

### æˆæœ¬å¯¹æ¯”ï¼ˆ2025ï¼‰

| æ¨¡å‹ | æ¯ç™¾ä¸‡Tokenæˆæœ¬ | é€Ÿåº¦ | è´¨é‡ | é€‚ç”¨åœºæ™¯ |
|------|----------------|------|------|---------|
| **GPT-4 Turbo** | $10 | ä¸­ç­‰ | æœ€é«˜ | å¤æ‚æ¨ç†ã€åˆ›ä½œ |
| **GPT-3.5 Turbo** | $0.5 | å¿« | é«˜ | é€šç”¨å¯¹è¯ã€åˆ†ç±» |
| **Claude 3.5** | $8 | å¿« | æœ€é«˜ | é•¿æ–‡æœ¬ã€ä»£ç  |
| **Llama 3.3 (8B)** | å…è´¹ï¼ˆè‡ªæ‰˜ç®¡ï¼‰ | å¿« | ä¸­é«˜ | æœ¬åœ°éƒ¨ç½² |
| **Mistral 7B** | å…è´¹ï¼ˆè‡ªæ‰˜ç®¡ï¼‰ | æå¿« | ä¸­ | è½»é‡ä»»åŠ¡ |

---

## ğŸ¦œ LangChain 3.0

### å¿«é€Ÿå¼€å§‹

#### 1. å®‰è£…ä¾èµ–

```bash
# ä½¿ç”¨uvå®‰è£…ï¼ˆæ¨èï¼‰
uv add langchain==3.0.0
uv add langchain-openai==3.0.0
uv add langchain-community==3.0.0
uv add langgraph==2.0.0
uv add langsmith==0.5.0

# æˆ–ä½¿ç”¨pip
pip install langchain==3.0.0 langchain-openai langchain-community
```

#### 2. åŸºç¡€LLMè°ƒç”¨

```python
# app/ai/llm_basic.py
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
from typing import List
import os

# é…ç½®APIå¯†é’¥
os.environ["OPENAI_API_KEY"] = "your-api-key-here"

class LLMService:
    """LLMæœåŠ¡å°è£…"""
    
    def __init__(self, model: str = "gpt-4-turbo", temperature: float = 0.7):
        self.llm = ChatOpenAI(
            model=model,
            temperature=temperature,
            max_tokens=2000,
            timeout=30,
            max_retries=3
        )
    
    async def chat(self, messages: List[dict]) -> str:
        """å¼‚æ­¥èŠå¤©"""
        # è½¬æ¢æ¶ˆæ¯æ ¼å¼
        langchain_messages = [
            SystemMessage(content=msg["content"]) if msg["role"] == "system"
            else HumanMessage(content=msg["content"])
            for msg in messages
        ]
        
        # è°ƒç”¨LLM
        response = await self.llm.ainvoke(langchain_messages)
        return response.content
    
    async def stream_chat(self, messages: List[dict]):
        """æµå¼è¾“å‡º"""
        langchain_messages = [
            SystemMessage(content=msg["content"]) if msg["role"] == "system"
            else HumanMessage(content=msg["content"])
            for msg in messages
        ]
        
        # æµå¼è°ƒç”¨
        async for chunk in self.llm.astream(langchain_messages):
            if hasattr(chunk, 'content'):
                yield chunk.content


# FastAPIé›†æˆ
from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel

app = FastAPI()
llm_service = LLMService()

class ChatRequest(BaseModel):
    messages: List[dict]

@app.post("/api/chat")
async def chat(request: ChatRequest):
    """èŠå¤©æ¥å£"""
    try:
        response = await llm_service.chat(request.messages)
        return {"response": response}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/chat/stream")
async def chat_stream(request: ChatRequest):
    """æµå¼èŠå¤©æ¥å£"""
    async def generate():
        try:
            async for chunk in llm_service.stream_chat(request.messages):
                yield f"data: {chunk}\n\n"
        except Exception as e:
            yield f"data: [ERROR] {str(e)}\n\n"
    
    return StreamingResponse(generate(), media_type="text/event-stream")
```

### 3. LangChain Chains

```python
# app/ai/chains.py
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI

class ContentGenerator:
    """å†…å®¹ç”Ÿæˆå™¨ï¼ˆä½¿ç”¨Chainï¼‰"""
    
    def __init__(self):
        self.llm = ChatOpenAI(model="gpt-4-turbo", temperature=0.9)
        
        # å®šä¹‰Promptæ¨¡æ¿
        self.blog_template = PromptTemplate(
            input_variables=["topic", "tone", "length"],
            template="""
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å†…å®¹åˆ›ä½œè€…ã€‚è¯·æ ¹æ®ä»¥ä¸‹è¦æ±‚æ’°å†™ä¸€ç¯‡åšå®¢æ–‡ç« ï¼š

ä¸»é¢˜ï¼š{topic}
è¯­æ°”ï¼š{tone}
é•¿åº¦ï¼š{length}å­—

è¦æ±‚ï¼š
1. ç»“æ„æ¸…æ™°ï¼Œåˆ†æ®µåˆç†
2. è¯­è¨€ç”ŸåŠ¨ï¼Œå¼•äººå…¥èƒœ
3. åŒ…å«å…·ä½“æ¡ˆä¾‹æˆ–æ•°æ®æ”¯æ’‘
4. ä»¥è¡ŒåŠ¨å·å¬ç»“å°¾

å¼€å§‹åˆ›ä½œï¼š
"""
        )
        
        # åˆ›å»ºChain
        self.blog_chain = LLMChain(
            llm=self.llm,
            prompt=self.blog_template
        )
    
    async def generate_blog(self, topic: str, tone: str = "ä¸“ä¸š", length: int = 1000) -> str:
        """ç”Ÿæˆåšå®¢æ–‡ç« """
        result = await self.blog_chain.ainvoke({
            "topic": topic,
            "tone": tone,
            "length": length
        })
        return result["text"]


# ä½¿ç”¨ç¤ºä¾‹
generator = ContentGenerator()
blog = await generator.generate_blog(
    topic="Python 3.13çš„é©å‘½æ€§ç‰¹æ€§",
    tone="ä¸“ä¸šä¸”æ˜“æ‡‚",
    length=1500
)
```

### 4. LangGraph - å¤æ‚å·¥ä½œæµ

```python
# app/ai/workflows.py
from langgraph.graph import StateGraph, END
from typing import TypedDict, Annotated, Sequence
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, AIMessage
import operator

# å®šä¹‰çŠ¶æ€
class AgentState(TypedDict):
    """AgentçŠ¶æ€"""
    messages: Annotated[Sequence[HumanMessage | AIMessage], operator.add]
    next: str

# åˆ›å»ºå·¥ä½œæµ
workflow = StateGraph(AgentState)

# å®šä¹‰èŠ‚ç‚¹
async def research_node(state: AgentState) -> AgentState:
    """ç ”ç©¶èŠ‚ç‚¹"""
    llm = ChatOpenAI(model="gpt-4-turbo")
    
    # æ‰§è¡Œç ”ç©¶
    research_prompt = HumanMessage(
        content=f"è¯·å¯¹ä»¥ä¸‹ä¸»é¢˜è¿›è¡Œæ·±å…¥ç ”ç©¶ï¼š{state['messages'][-1].content}"
    )
    response = await llm.ainvoke([research_prompt])
    
    return {
        "messages": [response],
        "next": "analyze"
    }

async def analyze_node(state: AgentState) -> AgentState:
    """åˆ†æèŠ‚ç‚¹"""
    llm = ChatOpenAI(model="gpt-4-turbo")
    
    # åˆ†æç ”ç©¶ç»“æœ
    analyze_prompt = HumanMessage(
        content=f"è¯·åˆ†æä»¥ä¸‹ç ”ç©¶ç»“æœå¹¶æå–å…³é”®è§è§£ï¼š{state['messages'][-1].content}"
    )
    response = await llm.ainvoke([analyze_prompt])
    
    return {
        "messages": [response],
        "next": "report"
    }

async def report_node(state: AgentState) -> AgentState:
    """æŠ¥å‘ŠèŠ‚ç‚¹"""
    llm = ChatOpenAI(model="gpt-4-turbo")
    
    # ç”ŸæˆæŠ¥å‘Š
    report_prompt = HumanMessage(
        content=f"åŸºäºä»¥ä¸‹åˆ†æï¼Œç”Ÿæˆä¸€ä»½ç»“æ„åŒ–æŠ¥å‘Šï¼š{state['messages'][-1].content}"
    )
    response = await llm.ainvoke([report_prompt])
    
    return {
        "messages": [response],
        "next": END
    }

# æ·»åŠ èŠ‚ç‚¹
workflow.add_node("research", research_node)
workflow.add_node("analyze", analyze_node)
workflow.add_node("report", report_node)

# å®šä¹‰è¾¹
workflow.set_entry_point("research")
workflow.add_edge("research", "analyze")
workflow.add_edge("analyze", "report")
workflow.add_edge("report", END)

# ç¼–è¯‘å·¥ä½œæµ
app = workflow.compile()

# ä½¿ç”¨ç¤ºä¾‹
async def run_research_workflow(topic: str) -> str:
    """è¿è¡Œç ”ç©¶å·¥ä½œæµ"""
    initial_state = {
        "messages": [HumanMessage(content=topic)],
        "next": "research"
    }
    
    result = await app.ainvoke(initial_state)
    return result["messages"][-1].content
```

---

## ğŸ¤– AI Agentå¼€å‘

### 1. Function Calling Agent

```python
# app/ai/agents/function_calling.py
from langchain_openai import ChatOpenAI
from langchain.tools import tool
from langchain.agents import create_openai_functions_agent, AgentExecutor
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from typing import List, Dict

# å®šä¹‰å·¥å…·
@tool
def search_database(query: str) -> str:
    """æœç´¢æ•°æ®åº“è·å–ä¿¡æ¯
    
    Args:
        query: æœç´¢æŸ¥è¯¢
    """
    # å®é™…çš„æ•°æ®åº“æŸ¥è¯¢é€»è¾‘
    return f"æ•°æ®åº“æŸ¥è¯¢ç»“æœï¼š{query}"

@tool
def calculate(expression: str) -> float:
    """è®¡ç®—æ•°å­¦è¡¨è¾¾å¼
    
    Args:
        expression: æ•°å­¦è¡¨è¾¾å¼ï¼Œå¦‚ "2 + 2"
    """
    try:
        return eval(expression)
    except Exception as e:
        return f"è®¡ç®—é”™è¯¯ï¼š{str(e)}"

@tool
def send_email(to: str, subject: str, body: str) -> str:
    """å‘é€ç”µå­é‚®ä»¶
    
    Args:
        to: æ”¶ä»¶äººé‚®ç®±
        subject: é‚®ä»¶ä¸»é¢˜
        body: é‚®ä»¶æ­£æ–‡
    """
    # å®é™…çš„é‚®ä»¶å‘é€é€»è¾‘
    return f"é‚®ä»¶å·²å‘é€è‡³ {to}"


class FunctionCallingAgent:
    """å‡½æ•°è°ƒç”¨Agent"""
    
    def __init__(self):
        # åˆå§‹åŒ–LLM
        self.llm = ChatOpenAI(
            model="gpt-4-turbo",
            temperature=0
        )
        
        # å®šä¹‰å·¥å…·åˆ—è¡¨
        self.tools = [search_database, calculate, send_email]
        
        # å®šä¹‰Prompt
        self.prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹å·¥å…·æ¥å¸®åŠ©ç”¨æˆ·ï¼š
            
1. search_database - æœç´¢æ•°æ®åº“
2. calculate - è®¡ç®—æ•°å­¦è¡¨è¾¾å¼
3. send_email - å‘é€é‚®ä»¶

è¯·æ ¹æ®ç”¨æˆ·çš„è¯·æ±‚ï¼Œé€‰æ‹©åˆé€‚çš„å·¥å…·å¹¶æ‰§è¡Œã€‚"""),
            MessagesPlaceholder(variable_name="chat_history", optional=True),
            ("human", "{input}"),
            MessagesPlaceholder(variable_name="agent_scratchpad"),
        ])
        
        # åˆ›å»ºAgent
        agent = create_openai_functions_agent(
            llm=self.llm,
            tools=self.tools,
            prompt=self.prompt
        )
        
        # åˆ›å»ºExecutor
        self.agent_executor = AgentExecutor(
            agent=agent,
            tools=self.tools,
            verbose=True,
            max_iterations=5
        )
    
    async def run(self, user_input: str, chat_history: List[Dict] = None) -> str:
        """è¿è¡ŒAgent"""
        result = await self.agent_executor.ainvoke({
            "input": user_input,
            "chat_history": chat_history or []
        })
        return result["output"]


# ä½¿ç”¨ç¤ºä¾‹
agent = FunctionCallingAgent()

# ç®€å•æŸ¥è¯¢
response = await agent.run("å¸®æˆ‘æŸ¥è¯¢user_123çš„è®¢å•ä¿¡æ¯")
# Agentä¼šè‡ªåŠ¨è°ƒç”¨ search_database("user_123çš„è®¢å•")

# å¤æ‚ä»»åŠ¡
response = await agent.run("""
è¯·æ‰§è¡Œä»¥ä¸‹ä»»åŠ¡ï¼š
1. æŸ¥è¯¢user_456çš„æ€»è®¢å•é‡‘é¢
2. è®¡ç®—æŠ˜æ‰£åçš„é‡‘é¢ï¼ˆæ‰“8æŠ˜ï¼‰
3. å‘é€é‚®ä»¶ç»™user_456@example.comï¼Œä¸»é¢˜æ˜¯"æ‚¨çš„è®¢å•æ€»é¢"
""")
# Agentä¼šè‡ªåŠ¨æ‹†è§£ä»»åŠ¡å¹¶ä¾æ¬¡è°ƒç”¨å·¥å…·
```

### 2. AutoGPTé£æ ¼Agent

```python
# app/ai/agents/autogpt.py
from typing import List, Dict, Optional
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, AIMessage, SystemMessage
import json

class AutoGPTAgent:
    """AutoGPTé£æ ¼çš„è‡ªä¸»Agent"""
    
    def __init__(self, goal: str):
        self.goal = goal
        self.llm = ChatOpenAI(model="gpt-4-turbo", temperature=0.7)
        self.thoughts: List[str] = []
        self.actions: List[Dict] = []
        self.max_iterations = 10
    
    async def run(self) -> str:
        """è¿è¡ŒAgentç›´åˆ°å®Œæˆç›®æ ‡"""
        system_prompt = f"""
ä½ æ˜¯ä¸€ä¸ªè‡ªä¸»AIä»£ç†ã€‚ä½ çš„ç›®æ ‡æ˜¯ï¼š{self.goal}

ä½ éœ€è¦ï¼š
1. åˆ†æå½“å‰çŠ¶æ€
2. è§„åˆ’ä¸‹ä¸€æ­¥è¡ŒåŠ¨
3. æ‰§è¡Œè¡ŒåŠ¨
4. è¯„ä¼°ç»“æœ
5. å†³å®šæ˜¯å¦ç»§ç»­æˆ–å®Œæˆ

è¯·ä»¥JSONæ ¼å¼è¾“å‡ºä½ çš„æ€è€ƒå’Œè¡ŒåŠ¨ï¼š
{{
    "thought": "ä½ çš„æ€è€ƒè¿‡ç¨‹",
    "action": "ä¸‹ä¸€æ­¥è¡ŒåŠ¨",
    "action_type": "search|calculate|complete",
    "action_input": "è¡ŒåŠ¨çš„è¾“å…¥",
    "reasoning": "é€‰æ‹©è¯¥è¡ŒåŠ¨çš„ç†ç”±"
}}
"""
        
        messages = [SystemMessage(content=system_prompt)]
        
        for iteration in range(self.max_iterations):
            # Agentæ€è€ƒ
            response = await self.llm.ainvoke(messages)
            
            try:
                decision = json.loads(response.content)
            except json.JSONDecodeError:
                continue
            
            # è®°å½•æ€è€ƒ
            self.thoughts.append(decision["thought"])
            
            # æ‰§è¡Œè¡ŒåŠ¨
            if decision["action_type"] == "complete":
                return decision["action_input"]
            
            action_result = await self._execute_action(
                decision["action_type"],
                decision["action_input"]
            )
            
            # è®°å½•è¡ŒåŠ¨
            self.actions.append({
                "iteration": iteration,
                "action": decision["action"],
                "result": action_result
            })
            
            # æ·»åŠ ç»“æœåˆ°å¯¹è¯å†å²
            messages.append(AIMessage(content=response.content))
            messages.append(HumanMessage(
                content=f"è¡ŒåŠ¨ç»“æœï¼š{action_result}\n\nç»§ç»­ä¸‹ä¸€æ­¥ã€‚"
            ))
        
        return "è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œä»»åŠ¡æœªå®Œæˆã€‚"
    
    async def _execute_action(self, action_type: str, action_input: str) -> str:
        """æ‰§è¡Œå…·ä½“è¡ŒåŠ¨"""
        if action_type == "search":
            # å®é™…çš„æœç´¢é€»è¾‘
            return f"æœç´¢ç»“æœï¼š{action_input}"
        elif action_type == "calculate":
            try:
                return str(eval(action_input))
            except Exception as e:
                return f"è®¡ç®—é”™è¯¯ï¼š{str(e)}"
        return "æœªçŸ¥è¡ŒåŠ¨ç±»å‹"


# ä½¿ç”¨ç¤ºä¾‹
agent = AutoGPTAgent(goal="ç ”ç©¶Python 3.13çš„æ–°ç‰¹æ€§å¹¶ç”Ÿæˆæ€»ç»“æŠ¥å‘Š")
result = await agent.run()
print(result)
```

---

## ğŸ—‚ï¸ å‘é‡æ•°æ®åº“

### Qdranté›†æˆ

```python
# app/ai/vector_store.py
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct
from langchain_openai import OpenAIEmbeddings
from typing import List, Dict
import uuid

class VectorStore:
    """å‘é‡æ•°æ®åº“ç®¡ç†"""
    
    def __init__(self, collection_name: str = "documents"):
        # è¿æ¥Qdrant
        self.client = QdrantClient(host="localhost", port=6333)
        self.collection_name = collection_name
        
        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        self.embeddings = OpenAIEmbeddings(
            model="text-embedding-3-large",
            dimensions=1536
        )
        
        # åˆ›å»ºé›†åˆ
        self._create_collection()
    
    def _create_collection(self):
        """åˆ›å»ºå‘é‡é›†åˆ"""
        try:
            self.client.get_collection(self.collection_name)
        except:
            self.client.create_collection(
                collection_name=self.collection_name,
                vectors_config=VectorParams(
                    size=1536,
                    distance=Distance.COSINE
                )
            )
    
    async def add_documents(self, documents: List[Dict]) -> None:
        """æ·»åŠ æ–‡æ¡£"""
        points = []
        
        for doc in documents:
            # ç”ŸæˆåµŒå…¥å‘é‡
            vector = await self.embeddings.aembed_query(doc["text"])
            
            # åˆ›å»ºç‚¹
            point = PointStruct(
                id=str(uuid.uuid4()),
                vector=vector,
                payload={
                    "text": doc["text"],
                    "metadata": doc.get("metadata", {})
                }
            )
            points.append(point)
        
        # æ‰¹é‡æ’å…¥
        self.client.upsert(
            collection_name=self.collection_name,
            points=points
        )
    
    async def search(self, query: str, limit: int = 5) -> List[Dict]:
        """æœç´¢ç›¸ä¼¼æ–‡æ¡£"""
        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        query_vector = await self.embeddings.aembed_query(query)
        
        # æœç´¢
        results = self.client.search(
            collection_name=self.collection_name,
            query_vector=query_vector,
            limit=limit
        )
        
        return [
            {
                "text": hit.payload["text"],
                "metadata": hit.payload["metadata"],
                "score": hit.score
            }
            for hit in results
        ]
    
    def delete_collection(self) -> None:
        """åˆ é™¤é›†åˆ"""
        self.client.delete_collection(self.collection_name)


# ä½¿ç”¨ç¤ºä¾‹
vector_store = VectorStore(collection_name="knowledge_base")

# æ·»åŠ æ–‡æ¡£
await vector_store.add_documents([
    {
        "text": "Python 3.13å¼•å…¥äº†Free-Threadedæ¨¡å¼...",
        "metadata": {"source": "docs", "category": "python"}
    },
    {
        "text": "JITç¼–è¯‘å™¨å¯ä»¥æ˜¾è‘—æå‡æ€§èƒ½...",
        "metadata": {"source": "docs", "category": "performance"}
    }
])

# æœç´¢
results = await vector_store.search("Pythonæ€§èƒ½ä¼˜åŒ–", limit=3)
for result in results:
    print(f"ç›¸å…³åº¦ï¼š{result['score']:.2f}")
    print(f"å†…å®¹ï¼š{result['text']}")
```

---

## ğŸ“ RAGç³»ç»Ÿ

### å®Œæ•´RAGå®ç°

```python
# app/ai/rag_system.py
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from typing import List
import asyncio

class RAGSystem:
    """æ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿ"""
    
    def __init__(self, vector_store: VectorStore):
        self.vector_store = vector_store
        self.llm = ChatOpenAI(model="gpt-4-turbo", temperature=0)
        
        # å®šä¹‰RAG Prompt
        self.rag_prompt = PromptTemplate(
            input_variables=["context", "question"],
            template="""
åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯šå®åœ°è¯´"æˆ‘ä¸çŸ¥é“"ã€‚

ä¸Šä¸‹æ–‡ï¼š
{context}

é—®é¢˜ï¼š{question}

è¯¦ç»†å›ç­”ï¼š
"""
        )
    
    async def ingest_documents(self, documents: List[str]) -> None:
        """æ‘„å–æ–‡æ¡£"""
        # æ–‡æœ¬åˆ†å‰²
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", ".", "!", "?", " ", ""]
        )
        
        chunks = []
        for doc in documents:
            splits = text_splitter.split_text(doc)
            chunks.extend([
                {"text": split, "metadata": {"source": "upload"}}
                for split in splits
            ])
        
        # å­˜å…¥å‘é‡æ•°æ®åº“
        await self.vector_store.add_documents(chunks)
    
    async def query(self, question: str, top_k: int = 3) -> Dict:
        """æŸ¥è¯¢RAGç³»ç»Ÿ"""
        # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
        relevant_docs = await self.vector_store.search(question, limit=top_k)
        
        # 2. æ„å»ºä¸Šä¸‹æ–‡
        context = "\n\n".join([
            f"[æ–‡æ¡£{i+1}] {doc['text']}"
            for i, doc in enumerate(relevant_docs)
        ])
        
        # 3. ç”Ÿæˆå›ç­”
        prompt = self.rag_prompt.format(context=context, question=question)
        response = await self.llm.ainvoke([HumanMessage(content=prompt)])
        
        return {
            "answer": response.content,
            "sources": relevant_docs,
            "context_length": len(context)
        }


# FastAPIé›†æˆ
from fastapi import FastAPI, UploadFile, File
from pydantic import BaseModel

app = FastAPI()
vector_store = VectorStore(collection_name="rag_kb")
rag_system = RAGSystem(vector_store)

class QueryRequest(BaseModel):
    question: str
    top_k: int = 3

@app.post("/api/rag/ingest")
async def ingest_document(file: UploadFile = File(...)):
    """ä¸Šä¼ å¹¶æ‘„å–æ–‡æ¡£"""
    content = await file.read()
    text = content.decode("utf-8")
    
    await rag_system.ingest_documents([text])
    
    return {"message": "Document ingested successfully"}

@app.post("/api/rag/query")
async def query_rag(request: QueryRequest):
    """æŸ¥è¯¢RAGç³»ç»Ÿ"""
    result = await rag_system.query(request.question, request.top_k)
    return result
```

---

## ğŸ“Š AIç›‘æ§ä¸è¯„ä¼°

### LangSmithé›†æˆ

```python
# app/ai/monitoring.py
from langsmith import Client
from langchain.callbacks.tracers import LangChainTracer
import os

# é…ç½®LangSmith
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_ENDPOINT"] = "https://api.smith.langchain.com"
os.environ["LANGCHAIN_API_KEY"] = "your-api-key"
os.environ["LANGCHAIN_PROJECT"] = "production"

# åˆ›å»ºTracer
tracer = LangChainTracer(project_name="production")

# åœ¨LLMè°ƒç”¨ä¸­ä½¿ç”¨
async def monitored_chat(messages: List[dict]) -> str:
    """å¸¦ç›‘æ§çš„èŠå¤©"""
    llm = ChatOpenAI(
        model="gpt-4-turbo",
        callbacks=[tracer]  # æ·»åŠ ç›‘æ§
    )
    
    response = await llm.ainvoke(messages)
    return response.content
```

### æ€§èƒ½è¯„ä¼°

```python
# app/ai/evaluation.py
from typing import List, Dict
import asyncio

class AIEvaluator:
    """AIç³»ç»Ÿè¯„ä¼°"""
    
    @staticmethod
    async def evaluate_rag_quality(
        test_cases: List[Dict],
        rag_system: RAGSystem
    ) -> Dict:
        """è¯„ä¼°RAGè´¨é‡"""
        results = {
            "total": len(test_cases),
            "passed": 0,
            "failed": 0,
            "average_score": 0.0
        }
        
        scores = []
        
        for case in test_cases:
            question = case["question"]
            expected_answer = case["expected_answer"]
            
            # è·å–RAGå›ç­”
            result = await rag_system.query(question)
            actual_answer = result["answer"]
            
            # è¯„ä¼°ç›¸ä¼¼åº¦ï¼ˆä½¿ç”¨LLMä½œä¸ºè¯„åˆ¤ï¼‰
            score = await AIEvaluator._evaluate_answer_similarity(
                expected_answer,
                actual_answer
            )
            
            scores.append(score)
            
            if score >= 0.7:
                results["passed"] += 1
            else:
                results["failed"] += 1
        
        results["average_score"] = sum(scores) / len(scores)
        
        return results
    
    @staticmethod
    async def _evaluate_answer_similarity(
        expected: str,
        actual: str
    ) -> float:
        """è¯„ä¼°å›ç­”ç›¸ä¼¼åº¦"""
        llm = ChatOpenAI(model="gpt-4-turbo", temperature=0)
        
        prompt = f"""
è¯·è¯„ä¼°ä»¥ä¸‹ä¸¤ä¸ªå›ç­”çš„ç›¸ä¼¼åº¦ï¼ˆ0-1ä¹‹é—´çš„åˆ†æ•°ï¼‰ï¼š

æœŸæœ›å›ç­”ï¼š{expected}

å®é™…å›ç­”ï¼š{actual}

åªè¿”å›æ•°å­—åˆ†æ•°ï¼Œä¸éœ€è¦è§£é‡Šã€‚
"""
        
        response = await llm.ainvoke([HumanMessage(content=prompt)])
        
        try:
            return float(response.content.strip())
        except ValueError:
            return 0.0
```

---

## ğŸ’¡ ç”Ÿäº§æœ€ä½³å®è·µ

### 1. é”™è¯¯å¤„ç†ä¸é‡è¯•

```python
# app/ai/resilience.py
from tenacity import retry, stop_after_attempt, wait_exponential
from functools import wraps
import logging

logger = logging.getLogger(__name__)

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=4, max=10),
    reraise=True
)
async def resilient_llm_call(llm, messages):
    """å¸¦é‡è¯•çš„LLMè°ƒç”¨"""
    try:
        return await llm.ainvoke(messages)
    except Exception as e:
        logger.error(f"LLM call failed: {str(e)}")
        raise

# é™çº§ç­–ç•¥
async def llm_with_fallback(primary_llm, fallback_llm, messages):
    """LLMé™çº§ç­–ç•¥"""
    try:
        return await resilient_llm_call(primary_llm, messages)
    except Exception as e:
        logger.warning(f"Primary LLM failed, using fallback: {str(e)}")
        return await fallback_llm.ainvoke(messages)
```

### 2. æˆæœ¬æ§åˆ¶

```python
# app/ai/cost_control.py
from typing import Dict
import tiktoken

class CostController:
    """æˆæœ¬æ§åˆ¶å™¨"""
    
    # 2025å¹´å®šä»·ï¼ˆæ¯ç™¾ä¸‡Tokenï¼‰
    PRICING = {
        "gpt-4-turbo": {"input": 10.0, "output": 30.0},
        "gpt-3.5-turbo": {"input": 0.5, "output": 1.5},
        "claude-3.5": {"input": 8.0, "output": 24.0}
    }
    
    def __init__(self, budget_limit: float):
        self.budget_limit = budget_limit
        self.total_cost = 0.0
        self.encoding = tiktoken.encoding_for_model("gpt-4")
    
    def calculate_cost(
        self,
        model: str,
        input_text: str,
        output_text: str
    ) -> float:
        """è®¡ç®—è°ƒç”¨æˆæœ¬"""
        input_tokens = len(self.encoding.encode(input_text))
        output_tokens = len(self.encoding.encode(output_text))
        
        pricing = self.PRICING.get(model, {"input": 10.0, "output": 30.0})
        
        cost = (
            (input_tokens / 1_000_000) * pricing["input"] +
            (output_tokens / 1_000_000) * pricing["output"]
        )
        
        return cost
    
    def check_budget(self, estimated_cost: float) -> bool:
        """æ£€æŸ¥é¢„ç®—"""
        return (self.total_cost + estimated_cost) <= self.budget_limit
    
    def record_cost(self, cost: float) -> None:
        """è®°å½•æˆæœ¬"""
        self.total_cost += cost
```

### 3. ç¼“å­˜ç­–ç•¥

```python
# app/ai/caching.py
from functools import wraps
import hashlib
import json

def cache_llm_response(ttl: int = 3600):
    """ç¼“å­˜LLMå“åº”"""
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            # ç”Ÿæˆç¼“å­˜é”®
            cache_key = hashlib.md5(
                json.dumps([args, kwargs], sort_keys=True).encode()
            ).hexdigest()
            
            # å°è¯•ä»ç¼“å­˜è·å–
            cached = await redis_cache.get(f"llm:{cache_key}")
            if cached:
                return cached
            
            # è°ƒç”¨LLM
            result = await func(*args, **kwargs)
            
            # å­˜å…¥ç¼“å­˜
            await redis_cache.set(f"llm:{cache_key}", result, ttl)
            
            return result
        return wrapper
    return decorator
```

---

## ğŸ“š å‚è€ƒèµ„æº

### å®˜æ–¹æ–‡æ¡£

- **LangChain**: <https://python.langchain.com/>
- **LangSmith**: <https://docs.smith.langchain.com/>
- **OpenAI**: <https://platform.openai.com/docs/>
- **Qdrant**: <https://qdrant.tech/documentation/>

### å­¦ä¹ èµ„æº

- [LangChain Cookbook](https://github.com/langchain-ai/langchain/tree/master/cookbook)
- [RAGä»å…¥é—¨åˆ°ç²¾é€š](https://www.deeplearning.ai/short-courses/building-applications-vector-databases/)
- [Prompt Engineering Guide](https://www.promptingguide.ai/)

---

**æ›´æ–°æ—¥æœŸï¼š** 2025å¹´10æœˆ24æ—¥  
**ç»´æŠ¤è€…ï¼š** Python Knowledge Base Team  
**è¿”å›ç›®å½•ï¼š** [../README.md](../README.md)
