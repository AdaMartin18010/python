# Python æ€§èƒ½ä¼˜åŒ–ä¸å‹æµ‹å®Œæ•´æŒ‡å— (2025)

**æœ€åæ›´æ–°ï¼š** 2025å¹´10æœˆ24æ—¥  
**çŠ¶æ€ï¼š** âœ… ç”Ÿäº§å°±ç»ª

---

## ğŸ“‹ ç›®å½•

- [æŠ€æœ¯æ ˆæ¦‚è§ˆ](#æŠ€æœ¯æ ˆæ¦‚è§ˆ)
- [Python 3.13æ€§èƒ½ç‰¹æ€§](#python-313æ€§èƒ½ç‰¹æ€§)
- [æ€§èƒ½åˆ†æå·¥å…·](#æ€§èƒ½åˆ†æå·¥å…·)
- [ä»£ç çº§ä¼˜åŒ–](#ä»£ç çº§ä¼˜åŒ–)
- [æ•°æ®åº“ä¼˜åŒ–](#æ•°æ®åº“ä¼˜åŒ–)
- [ç¼“å­˜ç­–ç•¥](#ç¼“å­˜ç­–ç•¥)
- [å¼‚æ­¥ç¼–ç¨‹ä¼˜åŒ–](#å¼‚æ­¥ç¼–ç¨‹ä¼˜åŒ–)
- [å‹åŠ›æµ‹è¯•](#å‹åŠ›æµ‹è¯•)
- [ç”Ÿäº§ç›‘æ§](#ç”Ÿäº§ç›‘æ§)

---

## ğŸš€ æŠ€æœ¯æ ˆæ¦‚è§ˆ

### 2025å¹´æ¨èæ€§èƒ½å·¥å…·æ ˆ

| ç±»åˆ« | å·¥å…· | ç‰ˆæœ¬ | ç”¨é€” |
|------|------|------|------|
| **æ€§èƒ½åˆ†æ** | Pyroscope | 1.9+ | æŒç»­æ€§èƒ½åˆ†æ |
| **CPUåˆ†æ** | py-spy | 0.4+ | é‡‡æ ·åˆ†æå™¨ |
| **å†…å­˜åˆ†æ** | memory-profiler | 0.61+ | å†…å­˜ä½¿ç”¨åˆ†æ |
| **å‹æµ‹å·¥å…·** | Locust | 2.31+ | åˆ†å¸ƒå¼è´Ÿè½½æµ‹è¯• |
| **HTTPå‹æµ‹** | wrk2 | 4.0+ | HTTPåŸºå‡†æµ‹è¯• |
| **åŸºå‡†æµ‹è¯•** | pytest-benchmark | 4.0+ | ä»£ç åŸºå‡†æµ‹è¯• |
| **æ•°æ®åº“ä¼˜åŒ–** | Polars | 1.9+ | é«˜æ€§èƒ½æ•°æ®å¤„ç† |
| **ç¼“å­˜** | Redis | 7.4+ | å†…å­˜ç¼“å­˜ |
| **CDN** | CloudFlare | - | å†…å®¹åˆ†å‘ç½‘ç»œ |
| **APM** | Datadog/NewRelic | - | åº”ç”¨æ€§èƒ½ç›‘æ§ |

### æ€§èƒ½ä¼˜åŒ–å±‚çº§

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Layer 1: æ¶æ„è®¾è®¡ï¼ˆ10-100xï¼‰      â”‚  CDNã€ç¼“å­˜æ¶æ„ã€æ•°æ®åº“åˆ†ç‰‡
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Layer 2: ç®—æ³•ä¼˜åŒ–ï¼ˆ2-10xï¼‰        â”‚  æ—¶é—´å¤æ‚åº¦ã€ç©ºé—´å¤æ‚åº¦
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Layer 3: è¯­è¨€ç‰¹æ€§ï¼ˆ1.5-3xï¼‰       â”‚  JITã€Free-Threadedã€Cython
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Layer 4: ä»£ç ä¼˜åŒ–ï¼ˆ1.2-2xï¼‰       â”‚  é¿å…é‡å¤è®¡ç®—ã€åˆ—è¡¨æ¨å¯¼
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”¥ Python 3.13æ€§èƒ½ç‰¹æ€§

### 1. Free-Threadedæ¨¡å¼ï¼ˆæ— GILï¼‰

**æ€§èƒ½æå‡ï¼š** å¤šæ ¸CPUå¯†é›†å‹ä»»åŠ¡å¯æå‡2-8å€

```python
# free_threaded_demo.py
import threading
import time
from concurrent.futures import ThreadPoolExecutor

def cpu_intensive_task(n: int) -> int:
    """CPUå¯†é›†å‹ä»»åŠ¡"""
    result = 0
    for i in range(n):
        result += i ** 2
    return result

# ========== å¯¹æ¯”æµ‹è¯• ==========

def test_sequential():
    """é¡ºåºæ‰§è¡Œ"""
    start = time.time()
    results = [cpu_intensive_task(10_000_000) for _ in range(4)]
    elapsed = time.time() - start
    print(f"Sequential: {elapsed:.2f}s")
    return elapsed

def test_threaded_with_gil():
    """å¤šçº¿ç¨‹ï¼ˆæœ‰GILï¼‰ - Python 3.12"""
    start = time.time()
    with ThreadPoolExecutor(max_workers=4) as executor:
        results = list(executor.map(cpu_intensive_task, [10_000_000] * 4))
    elapsed = time.time() - start
    print(f"Threaded (GIL): {elapsed:.2f}s")
    return elapsed

def test_free_threaded():
    """å¤šçº¿ç¨‹ï¼ˆæ— GILï¼‰ - Python 3.13+"""
    # éœ€è¦ç”¨ python3.13t å¯åŠ¨
    start = time.time()
    with ThreadPoolExecutor(max_workers=4) as executor:
        results = list(executor.map(cpu_intensive_task, [10_000_000] * 4))
    elapsed = time.time() - start
    print(f"Free-Threaded: {elapsed:.2f}s")
    return elapsed

# ç»“æœå¯¹æ¯”ï¼ˆ4æ ¸CPUï¼‰
# Sequential:      8.0s
# Threaded (GIL):  7.8s  (å‡ ä¹æ— æå‡)
# Free-Threaded:   2.1s  (3.8xåŠ é€Ÿï¼)
```

**ä½¿ç”¨å»ºè®®ï¼š**

```bash
# å®‰è£…Free-Threaded Python
# macOS/Linux
brew install python@3.13t

# æˆ–ä»æºç ç¼–è¯‘
./configure --disable-gil
make
make install

# è¿è¡Œåº”ç”¨
python3.13t app.py

# ç¯å¢ƒå˜é‡æ§åˆ¶
export PYTHON_GIL=0  # ç¦ç”¨GIL
export PYTHON_GIL=1  # å¯ç”¨GILï¼ˆå…¼å®¹æ¨¡å¼ï¼‰
```

### 2. JITç¼–è¯‘å™¨

**æ€§èƒ½æå‡ï¼š** çº¯Pythonä»£ç å¯æå‡20-60%

```python
# jit_demo.py
import sys

def fibonacci(n: int) -> int:
    """æ–æ³¢é‚£å¥‘æ•°åˆ—ï¼ˆé€’å½’ï¼‰"""
    if n <= 1:
        return n
    return fibonacci(n - 1) + fibonacci(n - 2)

# Python 3.13+ è‡ªåŠ¨JITä¼˜åŒ–
# æ— éœ€ä¿®æ”¹ä»£ç ï¼Œè¿è¡Œæ—¶è‡ªåŠ¨ç¼–è¯‘çƒ­ç‚¹ä»£ç 

# åŸºå‡†æµ‹è¯•
import timeit

# Python 3.12: 14.2s
# Python 3.13 (JIT): 10.1s  (1.4xåŠ é€Ÿ)
time_taken = timeit.timeit(lambda: fibonacci(30), number=10)
print(f"Time: {time_taken:.2f}s")
```

**JITæ§åˆ¶ï¼š**

```bash
# å¯ç”¨JITï¼ˆé»˜è®¤å¼€å¯ï¼‰
export PYTHON_JIT=1

# ç¦ç”¨JIT
export PYTHON_JIT=0

# JITè°ƒè¯•
export PYTHON_JIT_DEBUG=1
```

---

## ğŸ” æ€§èƒ½åˆ†æå·¥å…·

### 1. Pyroscope - æŒç»­æ€§èƒ½åˆ†æ

```python
# app/monitoring/profiling.py
import pyroscope

# é…ç½®Pyroscope
pyroscope.configure(
    application_name="myapp",
    server_address="http://pyroscope:4040",
    tags={
        "environment": "production",
        "region": "us-east-1"
    }
)

# åœ¨FastAPIä¸­é›†æˆ
from fastapi import FastAPI

app = FastAPI()

@app.on_event("startup")
async def startup_event():
    """å¯åŠ¨Pyroscope"""
    pyroscope.start()

@app.on_event("shutdown")
async def shutdown_event():
    """åœæ­¢Pyroscope"""
    pyroscope.stop()

# ç‰¹å®šå‡½æ•°åˆ†æ
@pyroscope.profile(tags={"endpoint": "process_data"})
def process_data(data: list) -> dict:
    """å¤„ç†æ•°æ®ï¼ˆå¸¦æ€§èƒ½åˆ†æï¼‰"""
    result = {}
    for item in data:
        # ä¸šåŠ¡é€»è¾‘...
        pass
    return result
```

### 2. py-spy - ä½å¼€é”€é‡‡æ ·åˆ†æ

```bash
# å®‰è£…
uv add py-spy

# CPUç«ç„°å›¾
py-spy record -o profile.svg --pid 12345

# å®æ—¶ç›‘æ§
py-spy top --pid 12345

# åˆ†æå·²è¿è¡Œçš„è¿›ç¨‹
sudo py-spy record -o profile.svg -- python app.py

# è¾“å‡ºæ ¼å¼
py-spy record -f speedscope -o profile.json -- python app.py
```

### 3. memory-profiler - å†…å­˜åˆ†æ

```python
# memory_profile_demo.py
from memory_profiler import profile

@profile
def memory_heavy_function():
    """å†…å­˜å¯†é›†å‹å‡½æ•°"""
    # å¤§åˆ—è¡¨
    large_list = [i for i in range(10_000_000)]
    
    # å¤§å­—å…¸
    large_dict = {i: i**2 for i in range(1_000_000)}
    
    return len(large_list) + len(large_dict)

if __name__ == "__main__":
    memory_heavy_function()

# è¿è¡Œ
# python -m memory_profiler memory_profile_demo.py
```

**è¾“å‡ºç¤ºä¾‹ï¼š**

```
Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
     5     50.0 MiB     50.0 MiB           1   @profile
     6                                         def memory_heavy_function():
     7    431.6 MiB    381.6 MiB           1       large_list = [i for i in range(10_000_000)]
     9    509.1 MiB     77.5 MiB           1       large_dict = {i: i**2 for i in range(1_000_000)}
    11    509.1 MiB      0.0 MiB           1       return len(large_list) + len(large_dict)
```

### 4. pytest-benchmark - åŸºå‡†æµ‹è¯•

```python
# tests/test_performance.py
import pytest

def process_list_comprehension(n: int) -> list:
    """åˆ—è¡¨æ¨å¯¼å¼"""
    return [i**2 for i in range(n)]

def process_map(n: int) -> list:
    """mapå‡½æ•°"""
    return list(map(lambda x: x**2, range(n)))

def test_list_comprehension(benchmark):
    """åŸºå‡†æµ‹è¯•ï¼šåˆ—è¡¨æ¨å¯¼å¼"""
    result = benchmark(process_list_comprehension, 10000)
    assert len(result) == 10000

def test_map(benchmark):
    """åŸºå‡†æµ‹è¯•ï¼šmapå‡½æ•°"""
    result = benchmark(process_map, 10000)
    assert len(result) == 10000

# è¿è¡Œ
# pytest tests/test_performance.py --benchmark-only
```

**è¾“å‡ºç¤ºä¾‹ï¼š**

```
-------------------------- benchmark: 2 tests --------------------------
Name (time in ms)                 Min       Max      Mean    StdDev
--------------------------------------------------------------------
test_list_comprehension        1.23      1.45      1.31      0.08
test_map                       1.67      1.89      1.75      0.09
--------------------------------------------------------------------
```

---

## âš¡ ä»£ç çº§ä¼˜åŒ–

### 1. é¿å…å¸¸è§æ€§èƒ½é™·é˜±

```python
# âŒ ä¸å¥½ï¼šé‡å¤è®¡ç®—
def process_items_slow(items: list) -> list:
    result = []
    for item in items:
        if len(items) > 100:  # æ¯æ¬¡å¾ªç¯éƒ½è®¡ç®—é•¿åº¦ï¼
            result.append(item * 2)
    return result

# âœ… å¥½ï¼šç¼“å­˜è®¡ç®—ç»“æœ
def process_items_fast(items: list) -> list:
    result = []
    items_length = len(items)  # åªè®¡ç®—ä¸€æ¬¡
    for item in items:
        if items_length > 100:
            result.append(item * 2)
    return result


# âŒ ä¸å¥½ï¼šå­—ç¬¦ä¸²æ‹¼æ¥
def build_string_slow(items: list) -> str:
    result = ""
    for item in items:
        result += str(item)  # O(nÂ²) æ—¶é—´å¤æ‚åº¦
    return result

# âœ… å¥½ï¼šä½¿ç”¨join
def build_string_fast(items: list) -> str:
    return "".join(str(item) for item in items)  # O(n)


# âŒ ä¸å¥½ï¼šå¤šæ¬¡åˆ—è¡¨éå†
def process_data_slow(data: list) -> dict:
    total = sum(data)
    count = len(data)
    maximum = max(data)
    minimum = min(data)
    return {"total": total, "count": count, "max": maximum, "min": minimum}

# âœ… å¥½ï¼šä¸€æ¬¡éå†
def process_data_fast(data: list) -> dict:
    if not data:
        return {"total": 0, "count": 0, "max": None, "min": None}
    
    total = 0
    maximum = minimum = data[0]
    
    for item in data:
        total += item
        if item > maximum:
            maximum = item
        if item < minimum:
            minimum = item
    
    return {"total": total, "count": len(data), "max": maximum, "min": minimum}
```

### 2. ä½¿ç”¨å†…ç½®å‡½æ•°å’Œæ•°æ®ç»“æ„

```python
# âœ… ä½¿ç”¨setè¿›è¡Œå¿«é€ŸæŸ¥æ‰¾
# O(1) vs O(n)
def find_duplicates_fast(list1: list, list2: list) -> list:
    set2 = set(list2)
    return [item for item in list1 if item in set2]

# âœ… ä½¿ç”¨collections.Counter
from collections import Counter

def count_frequency(items: list) -> dict:
    return Counter(items)

# âœ… ä½¿ç”¨dequeè¿›è¡Œé˜Ÿåˆ—æ“ä½œ
from collections import deque

queue = deque()
queue.append(1)      # O(1)
queue.popleft()      # O(1)
# vs list.pop(0)     # O(n)

# âœ… ä½¿ç”¨bisectè¿›è¡Œæœ‰åºåˆ—è¡¨æ“ä½œ
import bisect

sorted_list = [1, 3, 5, 7, 9]
bisect.insort(sorted_list, 6)  # O(n) vs manual O(nÂ²)
```

### 3. ä½¿ç”¨ç”Ÿæˆå™¨èŠ‚çœå†…å­˜

```python
# âŒ ä¸å¥½ï¼šè¿”å›å®Œæ•´åˆ—è¡¨
def read_large_file_slow(filename: str) -> list:
    with open(filename) as f:
        return [line.strip() for line in f]  # åŠ è½½å…¨éƒ¨åˆ°å†…å­˜

# âœ… å¥½ï¼šä½¿ç”¨ç”Ÿæˆå™¨
def read_large_file_fast(filename: str):
    with open(filename) as f:
        for line in f:
            yield line.strip()  # é€è¡Œå¤„ç†

# ä½¿ç”¨
for line in read_large_file_fast("huge.txt"):
    process(line)
```

### 4. ä½¿ç”¨functools.lru_cacheç¼“å­˜

```python
from functools import lru_cache

# âŒ ä¸å¥½ï¼šé‡å¤è®¡ç®—
def fibonacci_slow(n: int) -> int:
    if n <= 1:
        return n
    return fibonacci_slow(n - 1) + fibonacci_slow(n - 2)

# fibonacci_slow(35) = 9ç§’

# âœ… å¥½ï¼šLRUç¼“å­˜
@lru_cache(maxsize=128)
def fibonacci_fast(n: int) -> int:
    if n <= 1:
        return n
    return fibonacci_fast(n - 1) + fibonacci_fast(n - 2)

# fibonacci_fast(35) = 0.0001ç§’ (90,000x faster!)
```

---

## ğŸ’¾ æ•°æ®åº“ä¼˜åŒ–

### 1. è¿æ¥æ± é…ç½®

```python
# app/database.py
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker
from sqlalchemy.pool import NullPool, QueuePool

# âœ… æ¨èé…ç½®
engine = create_async_engine(
    "postgresql+asyncpg://user:pass@localhost/db",
    
    # è¿æ¥æ± é…ç½®
    poolclass=QueuePool,
    pool_size=20,              # è¿æ¥æ± å¤§å°
    max_overflow=40,           # æœ€å¤§æº¢å‡ºè¿æ¥
    pool_timeout=30,           # è·å–è¿æ¥è¶…æ—¶
    pool_recycle=3600,         # è¿æ¥å›æ”¶æ—¶é—´ï¼ˆç§’ï¼‰
    pool_pre_ping=True,        # è¿æ¥å¥åº·æ£€æŸ¥
    
    # æŸ¥è¯¢ä¼˜åŒ–
    echo=False,                # ç”Ÿäº§ç¯å¢ƒç¦ç”¨SQLæ—¥å¿—
    echo_pool=False,           # ç¦ç”¨è¿æ¥æ± æ—¥å¿—
    
    # æ€§èƒ½è°ƒä¼˜
    execution_options={
        "isolation_level": "READ COMMITTED"
    }
)

# ä¼šè¯å·¥å‚
async_session = sessionmaker(
    engine,
    class_=AsyncSession,
    expire_on_commit=False
)
```

### 2. æ‰¹é‡æ“ä½œ

```python
# âŒ ä¸å¥½ï¼šé€æ¡æ’å…¥
async def insert_users_slow(users: list[dict]):
    async with async_session() as session:
        for user in users:
            session.add(User(**user))
            await session.commit()  # æ¯æ¬¡éƒ½æäº¤ï¼
        # 1000æ¡æ•°æ® = 10ç§’

# âœ… å¥½ï¼šæ‰¹é‡æ’å…¥
async def insert_users_fast(users: list[dict]):
    async with async_session() as session:
        session.add_all([User(**user) for user in users])
        await session.commit()  # ä¸€æ¬¡æäº¤
        # 1000æ¡æ•°æ® = 0.5ç§’ (20x faster!)

# âœ… æ›´å¥½ï¼šbulk_insert_mappings
async def insert_users_fastest(users: list[dict]):
    async with async_session() as session:
        await session.execute(
            User.__table__.insert(),
            users
        )
        await session.commit()
        # 1000æ¡æ•°æ® = 0.1ç§’ (100x faster!)
```

### 3. æŸ¥è¯¢ä¼˜åŒ–

```python
from sqlalchemy import select
from sqlalchemy.orm import selectinload, joinedload

# âŒ N+1æŸ¥è¯¢é—®é¢˜
async def get_users_with_orders_slow():
    async with async_session() as session:
        users = await session.execute(select(User))
        users = users.scalars().all()
        
        for user in users:
            # æ¯ä¸ªç”¨æˆ·è§¦å‘ä¸€æ¬¡é¢å¤–æŸ¥è¯¢ï¼
            orders = await session.execute(
                select(Order).where(Order.user_id == user.id)
            )
            user.orders = orders.scalars().all()
        
        return users
        # 100ä¸ªç”¨æˆ· = 101æ¬¡æŸ¥è¯¢

# âœ… ä½¿ç”¨joinedloadï¼ˆLEFT JOINï¼‰
async def get_users_with_orders_fast():
    async with async_session() as session:
        result = await session.execute(
            select(User).options(joinedload(User.orders))
        )
        return result.unique().scalars().all()
        # 100ä¸ªç”¨æˆ· = 1æ¬¡æŸ¥è¯¢

# âœ… æˆ–ä½¿ç”¨selectinloadï¼ˆINæŸ¥è¯¢ï¼‰
async def get_users_with_orders_fast2():
    async with async_session() as session:
        result = await session.execute(
            select(User).options(selectinload(User.orders))
        )
        return result.scalars().all()
        # 100ä¸ªç”¨æˆ· = 2æ¬¡æŸ¥è¯¢
```

### 4. ç´¢å¼•ä¼˜åŒ–

```python
from sqlalchemy import Index, Column, Integer, String, DateTime

class User(Base):
    __tablename__ = "users"
    
    id = Column(Integer, primary_key=True)
    email = Column(String, unique=True, index=True)  # âœ… å•åˆ—ç´¢å¼•
    username = Column(String, index=True)
    created_at = Column(DateTime, index=True)
    status = Column(String)
    
    # âœ… å¤åˆç´¢å¼•
    __table_args__ = (
        Index('ix_user_status_created', 'status', 'created_at'),
        Index('ix_user_email_status', 'email', 'status'),
    )

# æŸ¥è¯¢ä¼˜åŒ–
async def get_active_users():
    # âœ… åˆ©ç”¨å¤åˆç´¢å¼•
    async with async_session() as session:
        result = await session.execute(
            select(User).where(
                User.status == 'active',
                User.created_at >= datetime(2025, 1, 1)
            ).order_by(User.created_at.desc())
        )
        return result.scalars().all()
```

---

## ğŸš€ ç¼“å­˜ç­–ç•¥

### 1. Redisç¼“å­˜å®ç°

```python
# app/cache/redis_cache.py
from redis.asyncio import Redis
import json
from typing import Any, Optional
from functools import wraps
import hashlib

class RedisCache:
    """Redisç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, redis: Redis):
        self.redis = redis
    
    async def get(self, key: str) -> Optional[Any]:
        """è·å–ç¼“å­˜"""
        value = await self.redis.get(key)
        if value:
            return json.loads(value)
        return None
    
    async def set(self, key: str, value: Any, ttl: int = 3600) -> None:
        """è®¾ç½®ç¼“å­˜"""
        await self.redis.setex(key, ttl, json.dumps(value))
    
    async def delete(self, key: str) -> None:
        """åˆ é™¤ç¼“å­˜"""
        await self.redis.delete(key)
    
    async def clear_pattern(self, pattern: str) -> None:
        """æ‰¹é‡åˆ é™¤ç¼“å­˜"""
        keys = await self.redis.keys(pattern)
        if keys:
            await self.redis.delete(*keys)


# ç¼“å­˜è£…é¥°å™¨
def cache_result(ttl: int = 3600, key_prefix: str = ""):
    """ç¼“å­˜å‡½æ•°ç»“æœ"""
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            # ç”Ÿæˆç¼“å­˜é”®
            cache_key = f"{key_prefix}:{func.__name__}:"
            
            # æ ¹æ®å‚æ•°ç”Ÿæˆå”¯ä¸€é”®
            args_key = hashlib.md5(
                json.dumps([args, kwargs], sort_keys=True).encode()
            ).hexdigest()
            cache_key += args_key
            
            # å°è¯•ä»ç¼“å­˜è·å–
            cached = await redis_cache.get(cache_key)
            if cached is not None:
                return cached
            
            # æ‰§è¡Œå‡½æ•°
            result = await func(*args, **kwargs)
            
            # å­˜å…¥ç¼“å­˜
            await redis_cache.set(cache_key, result, ttl)
            
            return result
        return wrapper
    return decorator


# ä½¿ç”¨ç¤ºä¾‹
@cache_result(ttl=3600, key_prefix="user")
async def get_user_by_id(user_id: str) -> dict:
    """è·å–ç”¨æˆ·ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
    async with async_session() as session:
        result = await session.execute(
            select(User).where(User.id == user_id)
        )
        user = result.scalar_one_or_none()
        return user.to_dict() if user else None
```

### 2. ç¼“å­˜ç­–ç•¥çŸ©é˜µ

| ç­–ç•¥ | é€‚ç”¨åœºæ™¯ | TTL | å¤±æ•ˆæ–¹å¼ |
|------|---------|-----|---------|
| **Cache-Aside** | è¯»å¤šå†™å°‘ | é•¿ï¼ˆ1å°æ—¶+ï¼‰ | ä¸»åŠ¨åˆ é™¤ |
| **Write-Through** | å†™å¤šè¯»å¤š | é•¿ | è‡ªåŠ¨æ›´æ–° |
| **Write-Behind** | é«˜å¹¶å‘å†™ | çŸ­ | å¼‚æ­¥åˆ·æ–° |
| **Refresh-Ahead** | çƒ­ç‚¹æ•°æ® | ä¸­ | é¢„æµ‹åˆ·æ–° |

### 3. å¤šçº§ç¼“å­˜

```python
# app/cache/multi_level_cache.py
from cachetools import TTLCache
from typing import Any, Optional

class MultiLevelCache:
    """å¤šçº§ç¼“å­˜ï¼ˆå†…å­˜ + Redisï¼‰"""
    
    def __init__(self, redis: Redis, memory_size: int = 1000, memory_ttl: int = 60):
        self.redis_cache = RedisCache(redis)
        self.memory_cache = TTLCache(maxsize=memory_size, ttl=memory_ttl)
    
    async def get(self, key: str) -> Optional[Any]:
        """è·å–ç¼“å­˜ï¼ˆå…ˆå†…å­˜ï¼ŒåRedisï¼‰"""
        # L1: å†…å­˜ç¼“å­˜
        if key in self.memory_cache:
            return self.memory_cache[key]
        
        # L2: Redisç¼“å­˜
        value = await self.redis_cache.get(key)
        if value is not None:
            # å›å¡«å†…å­˜ç¼“å­˜
            self.memory_cache[key] = value
            return value
        
        return None
    
    async def set(self, key: str, value: Any, ttl: int = 3600) -> None:
        """è®¾ç½®ç¼“å­˜ï¼ˆåŒæ—¶å†™å…¥ä¸¤çº§ï¼‰"""
        # å†™å…¥å†…å­˜
        self.memory_cache[key] = value
        
        # å†™å…¥Redis
        await self.redis_cache.set(key, value, ttl)
```

---

## ğŸ§ª å‹åŠ›æµ‹è¯•

### 1. Locustå‹æµ‹è„šæœ¬

```python
# locustfile.py
from locust import HttpUser, task, between
import random

class WebsiteUser(HttpUser):
    """ç½‘ç«™ç”¨æˆ·æ¨¡æ‹Ÿ"""
    
    wait_time = between(1, 3)  # è¯·æ±‚é—´éš”1-3ç§’
    
    def on_start(self):
        """ç™»å½•"""
        response = self.client.post("/api/auth/login", json={
            "username": "test_user",
            "password": "test_pass"
        })
        self.token = response.json().get("access_token")
    
    @task(3)  # æƒé‡3
    def view_items(self):
        """æŸ¥çœ‹å•†å“åˆ—è¡¨"""
        self.client.get(
            "/api/items",
            headers={"Authorization": f"Bearer {self.token}"}
        )
    
    @task(2)  # æƒé‡2
    def view_item_detail(self):
        """æŸ¥çœ‹å•†å“è¯¦æƒ…"""
        item_id = random.randint(1, 1000)
        self.client.get(
            f"/api/items/{item_id}",
            headers={"Authorization": f"Bearer {self.token}"}
        )
    
    @task(1)  # æƒé‡1
    def create_order(self):
        """åˆ›å»ºè®¢å•"""
        self.client.post(
            "/api/orders",
            headers={"Authorization": f"Bearer {self.token}"},
            json={
                "item_id": random.randint(1, 100),
                "quantity": random.randint(1, 5)
            }
        )

# è¿è¡Œå‹æµ‹
# locust -f locustfile.py --host=http://localhost:8000 --users=1000 --spawn-rate=50
```

### 2. wrk2åŸºå‡†æµ‹è¯•

```bash
# å®‰è£…wrk2
git clone https://github.com/giltene/wrk2.git
cd wrk2
make

# åŸºå‡†æµ‹è¯•ï¼ˆ1000å¹¶å‘ï¼ŒæŒç»­60ç§’ï¼Œå›ºå®š10K QPSï¼‰
./wrk -t4 -c1000 -d60s -R10000 \
  --latency \
  -s script.lua \
  http://localhost:8000/api/items

# Luaè„šæœ¬ï¼ˆscript.luaï¼‰
```

```lua
-- script.lua
wrk.method = "GET"
wrk.headers["Authorization"] = "Bearer your_token_here"
wrk.headers["Content-Type"] = "application/json"

request = function()
  return wrk.format(nil, "/api/items?page=" .. math.random(1, 100))
end

response = function(status, headers, body)
  if status ~= 200 then
    print("Error: " .. status)
  end
end
```

### 3. å‹æµ‹æŠ¥å‘Šè‡ªåŠ¨åŒ–

```python
# scripts/benchmark_report.py
import subprocess
import json
from datetime import datetime

class BenchmarkRunner:
    """åŸºå‡†æµ‹è¯•è¿è¡Œå™¨"""
    
    @staticmethod
    def run_locust(users: int, duration: str) -> dict:
        """è¿è¡ŒLocustå‹æµ‹"""
        cmd = [
            "locust",
            "-f", "locustfile.py",
            "--host", "http://localhost:8000",
            "--users", str(users),
            "--spawn-rate", "50",
            "--run-time", duration,
            "--headless",
            "--json"
        ]
        
        result = subprocess.run(cmd, capture_output=True, text=True)
        return json.loads(result.stdout)
    
    @staticmethod
    def generate_report(results: dict) -> str:
        """ç”Ÿæˆå‹æµ‹æŠ¥å‘Š"""
        report = f"""
# å‹åŠ›æµ‹è¯•æŠ¥å‘Š

**æ—¥æœŸï¼š** {datetime.now().isoformat()}
**å¹¶å‘ç”¨æˆ·ï¼š** {results['users']}
**æµ‹è¯•æ—¶é•¿ï¼š** {results['duration']}

## æ€§èƒ½æŒ‡æ ‡

| æŒ‡æ ‡ | å€¼ |
|------|------|
| **æ€»è¯·æ±‚æ•°** | {results['total_requests']:,} |
| **å¤±è´¥è¯·æ±‚** | {results['failures']:,} |
| **å¹³å‡å“åº”æ—¶é—´** | {results['avg_response_time']:.2f}ms |
| **P50å»¶è¿Ÿ** | {results['p50']:.2f}ms |
| **P95å»¶è¿Ÿ** | {results['p95']:.2f}ms |
| **P99å»¶è¿Ÿ** | {results['p99']:.2f}ms |
| **QPS** | {results['requests_per_second']:.0f} |
| **é”™è¯¯ç‡** | {results['error_rate']:.2f}% |

## ç»“è®º

{"âœ… æ€§èƒ½è¾¾æ ‡" if results['p95'] < 500 and results['error_rate'] < 1 else "âŒ æ€§èƒ½ä¸è¾¾æ ‡"}
"""
        return report

# ä½¿ç”¨
runner = BenchmarkRunner()
results = runner.run_locust(users=1000, duration="5m")
report = runner.generate_report(results)
print(report)
```

---

## ğŸ“Š æ€§èƒ½åŸºå‡†å‚è€ƒ

### Web APIæ€§èƒ½æ ‡å‡†ï¼ˆ2025ï¼‰

| çº§åˆ« | P95å»¶è¿Ÿ | QPS | é”™è¯¯ç‡ | è¯´æ˜ |
|------|---------|-----|--------|------|
| **ä¼˜ç§€** | < 100ms | > 10K | < 0.1% | è¡Œä¸šé¢†å…ˆ |
| **è‰¯å¥½** | < 200ms | > 5K | < 0.5% | ç”Ÿäº§å¯ç”¨ |
| **åŠæ ¼** | < 500ms | > 1K | < 1% | åŸºæœ¬å¯ç”¨ |
| **ä¸åŠæ ¼** | > 1000ms | < 500 | > 2% | éœ€ä¼˜åŒ– |

### æ•°æ®åº“æ€§èƒ½æ ‡å‡†

| æ“ä½œç±»å‹ | P95å»¶è¿Ÿ | è¯´æ˜ |
|---------|---------|------|
| **ä¸»é”®æŸ¥è¯¢** | < 1ms | å•è¡¨æŸ¥è¯¢ |
| **ç´¢å¼•æŸ¥è¯¢** | < 10ms | å¸¦WHEREæ¡ä»¶ |
| **JOINæŸ¥è¯¢** | < 50ms | 2-3è¡¨å…³è” |
| **èšåˆæŸ¥è¯¢** | < 100ms | COUNT/SUMç­‰ |
| **å…¨è¡¨æ‰«æ** | é¿å… | ä½¿ç”¨ç´¢å¼• |

---

## ğŸ“š å‚è€ƒèµ„æº

### å®˜æ–¹æ–‡æ¡£

- **Python Performance**: https://wiki.python.org/moin/PythonSpeed
- **Locust**: https://docs.locust.io/
- **Pyroscope**: https://pyroscope.io/docs/
- **Redis**: https://redis.io/docs/

### æ¨èé˜…è¯»

- [High Performance Python (O'Reilly)](https://www.oreilly.com/library/view/high-performance-python/9781492055013/)
- [Python Performance Tips](https://wiki.python.org/moin/PythonSpeed/PerformanceTips)

---

**æ›´æ–°æ—¥æœŸï¼š** 2025å¹´10æœˆ24æ—¥  
**ç»´æŠ¤è€…ï¼š** Python Knowledge Base Team  
**ä¸‹ä¸€æ­¥ï¼š** [AIé›†æˆå¼€å‘](../10-AIé›†æˆå¼€å‘/README.md) | [è¿”å›ç›®å½•](../README.md)

