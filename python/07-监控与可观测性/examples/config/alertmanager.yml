# Alertmanager 配置文件
# 用于告警路由和通知

global:
  # 全局配置
  resolve_timeout: 5m
  
  # SMTP配置（邮件通知）
  smtp_from: 'alertmanager@example.com'
  smtp_smarthost: 'smtp.example.com:587'
  smtp_auth_username: 'alertmanager@example.com'
  smtp_auth_password: 'password'
  smtp_require_tls: true

# 路由配置
route:
  # 默认接收者
  receiver: 'default'
  
  # 分组配置
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 12h
  
  # 子路由
  routes:
    # 严重告警：立即通知
    - match:
        severity: critical
      receiver: 'critical-alerts'
      group_wait: 0s
      group_interval: 5m
      repeat_interval: 1h
      continue: true
    
    # 警告级别：批量通知
    - match:
        severity: warning
      receiver: 'warning-alerts'
      group_wait: 30s
      group_interval: 10m
      repeat_interval: 4h
    
    # 数据库告警
    - match_re:
        service: '(postgres|mysql|redis)'
      receiver: 'database-team'
      group_by: ['alertname', 'instance']
    
    # 应用告警
    - match:
        category: availability
      receiver: 'oncall-team'
      group_interval: 5m
    
    # 性能告警
    - match:
        category: performance
      receiver: 'performance-team'
      group_interval: 15m

# 告警抑制规则
inhibit_rules:
  # 如果服务下线，抑制该服务的其他告警
  - source_match:
      alertname: 'ServiceDown'
    target_match_re:
      alertname: '(HighErrorRate|HighLatency|HighMemoryUsage)'
    equal: ['job', 'instance']
  
  # 如果节点下线，抑制节点上的所有告警
  - source_match:
      alertname: 'NodeDown'
    target_match_re:
      alertname: '.*'
    equal: ['instance']
  
  # 如果有严重告警，抑制同一服务的警告告警
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['alertname', 'service']

# 接收者配置
receivers:
  # 默认接收者
  - name: 'default'
    webhook_configs:
      - url: 'http://localhost:5001/webhook'
        send_resolved: true
  
  # 严重告警接收者
  - name: 'critical-alerts'
    # 邮件通知
    email_configs:
      - to: 'oncall@example.com'
        headers:
          Subject: '🚨 CRITICAL: {{ .GroupLabels.alertname }}'
        html: |
          <h2>Critical Alert</h2>
          <p><strong>Alert:</strong> {{ .GroupLabels.alertname }}</p>
          <p><strong>Severity:</strong> {{ .CommonLabels.severity }}</p>
          <p><strong>Summary:</strong> {{ .CommonAnnotations.summary }}</p>
          <p><strong>Description:</strong> {{ .CommonAnnotations.description }}</p>
          <p><strong>Time:</strong> {{ .StartsAt }}</p>
    
    # Slack通知
    slack_configs:
      - api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
        channel: '#alerts-critical'
        title: '🚨 {{ .GroupLabels.alertname }}'
        text: |
          *Severity:* {{ .CommonLabels.severity }}
          *Summary:* {{ .CommonAnnotations.summary }}
          *Description:* {{ .CommonAnnotations.description }}
        send_resolved: true
    
    # PagerDuty通知
    pagerduty_configs:
      - service_key: 'YOUR_PAGERDUTY_SERVICE_KEY'
        description: '{{ .CommonAnnotations.summary }}'
  
  # 警告告警接收者
  - name: 'warning-alerts'
    email_configs:
      - to: 'team@example.com'
        headers:
          Subject: '⚠️ WARNING: {{ .GroupLabels.alertname }}'
    
    slack_configs:
      - api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
        channel: '#alerts-warning'
        title: '⚠️ {{ .GroupLabels.alertname }}'
        text: '{{ .CommonAnnotations.summary }}'
  
  # 数据库团队
  - name: 'database-team'
    email_configs:
      - to: 'database-team@example.com'
        headers:
          Subject: 'Database Alert: {{ .GroupLabels.alertname }}'
  
  # 值班团队
  - name: 'oncall-team'
    pagerduty_configs:
      - service_key: 'ONCALL_PAGERDUTY_KEY'
        description: '{{ .CommonAnnotations.summary }}'
  
  # 性能团队
  - name: 'performance-team'
    email_configs:
      - to: 'performance@example.com'
        headers:
          Subject: 'Performance Alert: {{ .GroupLabels.alertname }}'

# 模板
templates:
  - '/etc/alertmanager/templates/*.tmpl'

