# æœºå™¨å­¦ä¹ åŸºç¡€

## ğŸ“‹ æ¦‚è¿°

æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒåˆ†æ”¯ï¼Œé€šè¿‡ç®—æ³•è®©è®¡ç®—æœºä»æ•°æ®ä¸­å­¦ä¹ æ¨¡å¼å’Œè§„å¾‹ã€‚æœ¬æ–‡æ¡£ä»å½¢å¼åŒ–è§’åº¦é˜è¿°æœºå™¨å­¦ä¹ çš„ç†è®ºåŸºç¡€ï¼Œå¹¶æä¾›å®Œæ•´çš„Pythonå®ç°ã€‚

## 1. åŸºæœ¬æ¦‚å¿µ

### 1.1 æœºå™¨å­¦ä¹ å®šä¹‰

**å®šä¹‰ 1.1** (æœºå™¨å­¦ä¹ )
æœºå™¨å­¦ä¹ æ˜¯è®©è®¡ç®—æœºç³»ç»Ÿé€šè¿‡ç»éªŒè‡ªåŠ¨æ”¹è¿›æ€§èƒ½çš„ç®—æ³•é›†åˆã€‚

**å½¢å¼åŒ–å®šä¹‰**:
$$\text{ML} = (D, H, L, A)$$
å…¶ä¸­ï¼š

- $D$ æ˜¯æ•°æ®åˆ†å¸ƒ
- $H$ æ˜¯å‡è®¾ç©ºé—´
- $L$ æ˜¯å­¦ä¹ ç®—æ³•
- $A$ æ˜¯è¯„ä¼°æŒ‡æ ‡

### 1.2 å­¦ä¹ ç±»å‹

**å®šä¹‰ 1.2** (ç›‘ç£å­¦ä¹ )
ç›‘ç£å­¦ä¹ æ˜¯ä»æ ‡è®°æ•°æ®ä¸­å­¦ä¹ æ˜ å°„å‡½æ•° $f: X \rightarrow Y$ã€‚

**å®šä¹‰ 1.3** (æ— ç›‘ç£å­¦ä¹ )
æ— ç›‘ç£å­¦ä¹ æ˜¯ä»æœªæ ‡è®°æ•°æ®ä¸­å‘ç°éšè—æ¨¡å¼ã€‚

**å®šä¹‰ 1.4** (å¼ºåŒ–å­¦ä¹ )
å¼ºåŒ–å­¦ä¹ æ˜¯é€šè¿‡ä¸ç¯å¢ƒäº¤äº’å­¦ä¹ æœ€ä¼˜ç­–ç•¥ã€‚

## 2. Pythonå®ç°

### 2.1 åŸºæœ¬æ•°æ®ç»“æ„

```python
from typing import Dict, List, Tuple, Optional, Any, Union, Callable
from dataclasses import dataclass, field
from enum import Enum
from abc import ABC, abstractmethod
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, mean_squared_error
import matplotlib.pyplot as plt
import seaborn as sns

class LearningType(Enum):
    """å­¦ä¹ ç±»å‹"""
    SUPERVISED = "supervised"
    UNSUPERVISED = "unsupervised"
    REINFORCEMENT = "reinforcement"

class ModelType(Enum):
    """æ¨¡å‹ç±»å‹"""
    LINEAR_REGRESSION = "linear_regression"
    LOGISTIC_REGRESSION = "logistic_regression"
    DECISION_TREE = "decision_tree"
    RANDOM_FOREST = "random_forest"
    SVM = "svm"
    NEURAL_NETWORK = "neural_network"

@dataclass
class Dataset:
    """æ•°æ®é›†"""
    X: np.ndarray
    y: Optional[np.ndarray] = None
    feature_names: Optional[List[str]] = None
    target_name: Optional[str] = None
    
    def __post_init__(self):
        if self.feature_names is None:
            self.feature_names = [f"feature_{i}" for i in range(self.X.shape[1])]
    
    def split(self, test_size: float = 0.2, random_state: int = 42) -> Tuple['Dataset', 'Dataset']:
        """åˆ†å‰²æ•°æ®é›†"""
        if self.y is None:
            raise ValueError("æ— ç›‘ç£å­¦ä¹ æ•°æ®é›†æ— æ³•åˆ†å‰²")
        
        X_train, X_test, y_train, y_test = train_test_split(
            self.X, self.y, test_size=test_size, random_state=random_state
        )
        
        train_dataset = Dataset(X_train, y_train, self.feature_names, self.target_name)
        test_dataset = Dataset(X_test, y_test, self.feature_names, self.target_name)
        
        return train_dataset, test_dataset
    
    def normalize(self) -> 'Dataset':
        """æ ‡å‡†åŒ–æ•°æ®"""
        mean = np.mean(self.X, axis=0)
        std = np.std(self.X, axis=0)
        X_normalized = (self.X - mean) / (std + 1e-8)
        
        return Dataset(X_normalized, self.y, self.feature_names, self.target_name)
    
    def __str__(self) -> str:
        return f"Dataset(shape={self.X.shape}, features={len(self.feature_names)})"

@dataclass
class Model:
    """æ¨¡å‹åŸºç±»"""
    model_type: ModelType
    parameters: Dict[str, Any] = field(default_factory=dict)
    is_trained: bool = False
    
    @abstractmethod
    def fit(self, dataset: Dataset) -> 'Model':
        """è®­ç»ƒæ¨¡å‹"""
        pass
    
    @abstractmethod
    def predict(self, X: np.ndarray) -> np.ndarray:
        """é¢„æµ‹"""
        pass
    
    def evaluate(self, dataset: Dataset) -> Dict[str, float]:
        """è¯„ä¼°æ¨¡å‹"""
        if not self.is_trained:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒ")
        
        predictions = self.predict(dataset.X)
        
        if dataset.y is None:
            return {"predictions": predictions}
        
        # æ ¹æ®ä»»åŠ¡ç±»å‹é€‰æ‹©è¯„ä¼°æŒ‡æ ‡
        if len(np.unique(dataset.y)) <= 10:  # åˆ†ç±»ä»»åŠ¡
            accuracy = accuracy_score(dataset.y, predictions)
            return {"accuracy": accuracy}
        else:  # å›å½’ä»»åŠ¡
            mse = mean_squared_error(dataset.y, predictions)
            rmse = np.sqrt(mse)
            return {"mse": mse, "rmse": rmse}

class LinearRegression(Model):
    """çº¿æ€§å›å½’æ¨¡å‹"""
    
    def __init__(self, learning_rate: float = 0.01, max_iterations: int = 1000):
        super().__init__(ModelType.LINEAR_REGRESSION)
        self.learning_rate = learning_rate
        self.max_iterations = max_iterations
        self.weights = None
        self.bias = None
    
    def fit(self, dataset: Dataset) -> 'LinearRegression':
        """è®­ç»ƒçº¿æ€§å›å½’æ¨¡å‹"""
        X, y = dataset.X, dataset.y
        if y is None:
            raise ValueError("çº¿æ€§å›å½’éœ€è¦æ ‡è®°æ•°æ®")
        
        n_samples, n_features = X.shape
        
        # åˆå§‹åŒ–å‚æ•°
        self.weights = np.zeros(n_features)
        self.bias = 0.0
        
        # æ¢¯åº¦ä¸‹é™
        for iteration in range(self.max_iterations):
            # å‰å‘ä¼ æ’­
            predictions = self._predict(X)
            
            # è®¡ç®—æ¢¯åº¦
            dw = (2 / n_samples) * np.dot(X.T, (predictions - y))
            db = (2 / n_samples) * np.sum(predictions - y)
            
            # æ›´æ–°å‚æ•°
            self.weights -= self.learning_rate * dw
            self.bias -= self.learning_rate * db
            
            # è®¡ç®—æŸå¤±
            if iteration % 100 == 0:
                loss = np.mean((predictions - y) ** 2)
                print(f"Iteration {iteration}, Loss: {loss:.4f}")
        
        self.is_trained = True
        return self
    
    def _predict(self, X: np.ndarray) -> np.ndarray:
        """å†…éƒ¨é¢„æµ‹æ–¹æ³•"""
        return np.dot(X, self.weights) + self.bias
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """é¢„æµ‹"""
        if not self.is_trained:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒ")
        return self._predict(X)
    
    def get_parameters(self) -> Dict[str, np.ndarray]:
        """è·å–æ¨¡å‹å‚æ•°"""
        return {
            "weights": self.weights,
            "bias": self.bias
        }

class LogisticRegression(Model):
    """é€»è¾‘å›å½’æ¨¡å‹"""
    
    def __init__(self, learning_rate: float = 0.01, max_iterations: int = 1000):
        super().__init__(ModelType.LOGISTIC_REGRESSION)
        self.learning_rate = learning_rate
        self.max_iterations = max_iterations
        self.weights = None
        self.bias = None
    
    def fit(self, dataset: Dataset) -> 'LogisticRegression':
        """è®­ç»ƒé€»è¾‘å›å½’æ¨¡å‹"""
        X, y = dataset.X, dataset.y
        if y is None:
            raise ValueError("é€»è¾‘å›å½’éœ€è¦æ ‡è®°æ•°æ®")
        
        n_samples, n_features = X.shape
        
        # åˆå§‹åŒ–å‚æ•°
        self.weights = np.zeros(n_features)
        self.bias = 0.0
        
        # æ¢¯åº¦ä¸‹é™
        for iteration in range(self.max_iterations):
            # å‰å‘ä¼ æ’­
            predictions = self._predict_proba(X)
            
            # è®¡ç®—æ¢¯åº¦
            dw = (1 / n_samples) * np.dot(X.T, (predictions - y))
            db = (1 / n_samples) * np.sum(predictions - y)
            
            # æ›´æ–°å‚æ•°
            self.weights -= self.learning_rate * dw
            self.bias -= self.learning_rate * db
            
            # è®¡ç®—æŸå¤±
            if iteration % 100 == 0:
                loss = self._compute_loss(y, predictions)
                print(f"Iteration {iteration}, Loss: {loss:.4f}")
        
        self.is_trained = True
        return self
    
    def _sigmoid(self, z: np.ndarray) -> np.ndarray:
        """Sigmoidå‡½æ•°"""
        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))
    
    def _predict_proba(self, X: np.ndarray) -> np.ndarray:
        """é¢„æµ‹æ¦‚ç‡"""
        z = np.dot(X, self.weights) + self.bias
        return self._sigmoid(z)
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """é¢„æµ‹"""
        if not self.is_trained:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒ")
        probabilities = self._predict_proba(X)
        return (probabilities >= 0.5).astype(int)
    
    def _compute_loss(self, y: np.ndarray, predictions: np.ndarray) -> float:
        """è®¡ç®—äº¤å‰ç†µæŸå¤±"""
        epsilon = 1e-15
        predictions = np.clip(predictions, epsilon, 1 - epsilon)
        return -np.mean(y * np.log(predictions) + (1 - y) * np.log(1 - predictions))

# ä½¿ç”¨ç¤ºä¾‹
def demonstrate_basic_ml():
    """æ¼”ç¤ºåŸºæœ¬æœºå™¨å­¦ä¹ """
    print("=== åŸºæœ¬æœºå™¨å­¦ä¹ æ¼”ç¤º ===\n")
    
    # ç”Ÿæˆç¤ºä¾‹æ•°æ®
    np.random.seed(42)
    n_samples = 1000
    n_features = 5
    
    # ç”Ÿæˆç‰¹å¾
    X = np.random.randn(n_samples, n_features)
    
    # ç”Ÿæˆçº¿æ€§å›å½’ç›®æ ‡
    true_weights = np.array([2.0, -1.5, 0.8, 1.2, -0.5])
    true_bias = 3.0
    y_regression = np.dot(X, true_weights) + true_bias + np.random.normal(0, 0.1, n_samples)
    
    # ç”Ÿæˆåˆ†ç±»ç›®æ ‡
    y_classification = (np.dot(X, true_weights) + true_bias > 0).astype(int)
    
    # åˆ›å»ºæ•°æ®é›†
    regression_dataset = Dataset(X, y_regression, 
                                [f"feature_{i}" for i in range(n_features)], "target")
    classification_dataset = Dataset(X, y_classification,
                                   [f"feature_{i}" for i in range(n_features)], "target")
    
    print(f"å›å½’æ•°æ®é›†: {regression_dataset}")
    print(f"åˆ†ç±»æ•°æ®é›†: {classification_dataset}")
    
    # åˆ†å‰²æ•°æ®é›†
    train_reg, test_reg = regression_dataset.split()
    train_clf, test_clf = classification_dataset.split()
    
    # è®­ç»ƒçº¿æ€§å›å½’æ¨¡å‹
    print("\n=== è®­ç»ƒçº¿æ€§å›å½’æ¨¡å‹ ===")
    lr_model = LinearRegression(learning_rate=0.01, max_iterations=500)
    lr_model.fit(train_reg)
    
    # è¯„ä¼°çº¿æ€§å›å½’æ¨¡å‹
    lr_metrics = lr_model.evaluate(test_reg)
    print(f"çº¿æ€§å›å½’è¯„ä¼°ç»“æœ: {lr_metrics}")
    
    # è®­ç»ƒé€»è¾‘å›å½’æ¨¡å‹
    print("\n=== è®­ç»ƒé€»è¾‘å›å½’æ¨¡å‹ ===")
    log_model = LogisticRegression(learning_rate=0.1, max_iterations=500)
    log_model.fit(train_clf)
    
    # è¯„ä¼°é€»è¾‘å›å½’æ¨¡å‹
    log_metrics = log_model.evaluate(test_clf)
    print(f"é€»è¾‘å›å½’è¯„ä¼°ç»“æœ: {log_metrics}")

if __name__ == "__main__":
    demonstrate_basic_ml()
```

## 3. å†³ç­–æ ‘ç®—æ³•

### 3.1 å†³ç­–æ ‘å®šä¹‰

**å®šä¹‰ 3.1** (å†³ç­–æ ‘)
å†³ç­–æ ‘æ˜¯ä¸€ç§æ ‘å½¢ç»“æ„ï¼Œæ¯ä¸ªå†…éƒ¨èŠ‚ç‚¹è¡¨ç¤ºç‰¹å¾æµ‹è¯•ï¼Œæ¯ä¸ªå¶èŠ‚ç‚¹è¡¨ç¤ºé¢„æµ‹ç»“æœã€‚

**å½¢å¼åŒ–å®šä¹‰**:
$$T = (N, E, F, L)$$
å…¶ä¸­ï¼š

- $N$ æ˜¯èŠ‚ç‚¹é›†åˆ
- $E$ æ˜¯è¾¹é›†åˆ
- $F$ æ˜¯ç‰¹å¾å‡½æ•°
- $L$ æ˜¯å¶èŠ‚ç‚¹æ ‡ç­¾

### 3.2 Pythonå®ç°

```python
@dataclass
class TreeNode:
    """å†³ç­–æ ‘èŠ‚ç‚¹"""
    feature_index: Optional[int] = None
    threshold: Optional[float] = None
    left_child: Optional['TreeNode'] = None
    right_child: Optional['TreeNode'] = None
    is_leaf: bool = False
    prediction: Optional[Any] = None
    samples_count: int = 0

class DecisionTree(Model):
    """å†³ç­–æ ‘æ¨¡å‹"""
    
    def __init__(self, max_depth: int = 10, min_samples_split: int = 2):
        super().__init__(ModelType.DECISION_TREE)
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.root = None
    
    def fit(self, dataset: Dataset) -> 'DecisionTree':
        """è®­ç»ƒå†³ç­–æ ‘"""
        X, y = dataset.X, dataset.y
        if y is None:
            raise ValueError("å†³ç­–æ ‘éœ€è¦æ ‡è®°æ•°æ®")
        
        self.root = self._build_tree(X, y, depth=0)
        self.is_trained = True
        return self
    
    def _build_tree(self, X: np.ndarray, y: np.ndarray, depth: int) -> TreeNode:
        """æ„å»ºå†³ç­–æ ‘"""
        n_samples, n_features = X.shape
        n_classes = len(np.unique(y))
        
        # åœæ­¢æ¡ä»¶
        if (depth >= self.max_depth or 
            n_samples < self.min_samples_split or 
            n_classes == 1):
            return self._create_leaf_node(y)
        
        # å¯»æ‰¾æœ€ä½³åˆ†å‰²
        best_feature, best_threshold, best_gain = self._find_best_split(X, y)
        
        if best_gain == 0:
            return self._create_leaf_node(y)
        
        # åˆ›å»ºåˆ†å‰²
        left_mask = X[:, best_feature] <= best_threshold
        right_mask = ~left_mask
        
        # åˆ›å»ºèŠ‚ç‚¹
        node = TreeNode(
            feature_index=best_feature,
            threshold=best_threshold,
            samples_count=n_samples
        )
        
        # é€’å½’æ„å»ºå­æ ‘
        node.left_child = self._build_tree(X[left_mask], y[left_mask], depth + 1)
        node.right_child = self._build_tree(X[right_mask], y[right_mask], depth + 1)
        
        return node
    
    def _find_best_split(self, X: np.ndarray, y: np.ndarray) -> Tuple[int, float, float]:
        """å¯»æ‰¾æœ€ä½³åˆ†å‰²ç‚¹"""
        n_samples, n_features = X.shape
        best_gain = 0
        best_feature = 0
        best_threshold = 0
        
        parent_entropy = self._compute_entropy(y)
        
        for feature in range(n_features):
            thresholds = np.unique(X[:, feature])
            
            for threshold in thresholds:
                left_mask = X[:, feature] <= threshold
                right_mask = ~left_mask
                
                if np.sum(left_mask) == 0 or np.sum(right_mask) == 0:
                    continue
                
                # è®¡ç®—ä¿¡æ¯å¢ç›Š
                left_entropy = self._compute_entropy(y[left_mask])
                right_entropy = self._compute_entropy(y[right_mask])
                
                left_weight = np.sum(left_mask) / n_samples
                right_weight = np.sum(right_mask) / n_samples
                
                gain = parent_entropy - (left_weight * left_entropy + right_weight * right_entropy)
                
                if gain > best_gain:
                    best_gain = gain
                    best_feature = feature
                    best_threshold = threshold
        
        return best_feature, best_threshold, best_gain
    
    def _compute_entropy(self, y: np.ndarray) -> float:
        """è®¡ç®—ç†µ"""
        _, counts = np.unique(y, return_counts=True)
        probabilities = counts / len(y)
        entropy = -np.sum(probabilities * np.log2(probabilities + 1e-10))
        return entropy
    
    def _create_leaf_node(self, y: np.ndarray) -> TreeNode:
        """åˆ›å»ºå¶èŠ‚ç‚¹"""
        if len(np.unique(y)) <= 10:  # åˆ†ç±»ä»»åŠ¡
            prediction = np.argmax(np.bincount(y))
        else:  # å›å½’ä»»åŠ¡
            prediction = np.mean(y)
        
        return TreeNode(
            is_leaf=True,
            prediction=prediction,
            samples_count=len(y)
        )
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """é¢„æµ‹"""
        if not self.is_trained:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒ")
        
        predictions = []
        for sample in X:
            prediction = self._predict_single(sample, self.root)
            predictions.append(prediction)
        
        return np.array(predictions)
    
    def _predict_single(self, sample: np.ndarray, node: TreeNode) -> Any:
        """é¢„æµ‹å•ä¸ªæ ·æœ¬"""
        if node.is_leaf:
            return node.prediction
        
        if sample[node.feature_index] <= node.threshold:
            return self._predict_single(sample, node.left_child)
        else:
            return self._predict_single(sample, node.right_child)
    
    def print_tree(self, node: Optional[TreeNode] = None, depth: int = 0):
        """æ‰“å°å†³ç­–æ ‘"""
        if node is None:
            node = self.root
        
        indent = "  " * depth
        
        if node.is_leaf:
            print(f"{indent}é¢„æµ‹: {node.prediction} (æ ·æœ¬æ•°: {node.samples_count})")
        else:
            print(f"{indent}ç‰¹å¾{node.feature_index} <= {node.threshold:.3f}")
            print(f"{indent}â”œâ”€ å·¦å­æ ‘:")
            self.print_tree(node.left_child, depth + 1)
            print(f"{indent}â””â”€ å³å­æ ‘:")
            self.print_tree(node.right_child, depth + 1)

def demonstrate_decision_tree():
    """æ¼”ç¤ºå†³ç­–æ ‘"""
    print("=== å†³ç­–æ ‘æ¼”ç¤º ===\n")
    
    # ç”Ÿæˆåˆ†ç±»æ•°æ®
    np.random.seed(42)
    n_samples = 200
    
    # åˆ›å»ºä¸¤ä¸ªç±»åˆ«çš„æ•°æ®
    X1 = np.random.multivariate_normal([0, 0], [[1, 0], [0, 1]], n_samples // 2)
    X2 = np.random.multivariate_normal([2, 2], [[1, 0], [0, 1]], n_samples // 2)
    
    X = np.vstack([X1, X2])
    y = np.hstack([np.zeros(n_samples // 2), np.ones(n_samples // 2)])
    
    # åˆ›å»ºæ•°æ®é›†
    dataset = Dataset(X, y, ["feature_1", "feature_2"], "class")
    train_dataset, test_dataset = dataset.split()
    
    # è®­ç»ƒå†³ç­–æ ‘
    dt_model = DecisionTree(max_depth=5, min_samples_split=10)
    dt_model.fit(train_dataset)
    
    # è¯„ä¼°æ¨¡å‹
    metrics = dt_model.evaluate(test_dataset)
    print(f"å†³ç­–æ ‘è¯„ä¼°ç»“æœ: {metrics}")
    
    # æ‰“å°æ ‘ç»“æ„
    print("\nå†³ç­–æ ‘ç»“æ„:")
    dt_model.print_tree()

if __name__ == "__main__":
    demonstrate_decision_tree()
```

## 4. æ”¯æŒå‘é‡æœº

### 4.1 SVMå®šä¹‰

**å®šä¹‰ 4.1** (æ”¯æŒå‘é‡æœº)
æ”¯æŒå‘é‡æœºæ˜¯ä¸€ç§å¯»æ‰¾æœ€ä¼˜è¶…å¹³é¢æ¥åˆ†ç¦»æ•°æ®çš„ç®—æ³•ã€‚

**å½¢å¼åŒ–å®šä¹‰**:
$$\min_{w,b} \frac{1}{2}||w||^2 \text{ s.t. } y_i(w^T x_i + b) \geq 1, \forall i$$

### 4.2 Pythonå®ç°

```python
class SVM(Model):
    """æ”¯æŒå‘é‡æœºæ¨¡å‹"""
    
    def __init__(self, C: float = 1.0, learning_rate: float = 0.01, max_iterations: int = 1000):
        super().__init__(ModelType.SVM)
        self.C = C
        self.learning_rate = learning_rate
        self.max_iterations = max_iterations
        self.weights = None
        self.bias = None
        self.support_vectors = None
    
    def fit(self, dataset: Dataset) -> 'SVM':
        """è®­ç»ƒSVMæ¨¡å‹"""
        X, y = dataset.X, dataset.y
        if y is None:
            raise ValueError("SVMéœ€è¦æ ‡è®°æ•°æ®")
        
        # å°†æ ‡ç­¾è½¬æ¢ä¸º{-1, 1}
        y_binary = 2 * y - 1
        
        n_samples, n_features = X.shape
        
        # åˆå§‹åŒ–å‚æ•°
        self.weights = np.zeros(n_features)
        self.bias = 0.0
        
        # æ¢¯åº¦ä¸‹é™
        for iteration in range(self.max_iterations):
            # è®¡ç®—æ¢¯åº¦
            dw = np.zeros(n_features)
            db = 0
            
            for i in range(n_samples):
                margin = y_binary[i] * (np.dot(self.weights, X[i]) + self.bias)
                
                if margin < 1:
                    dw += -y_binary[i] * X[i]
                    db += -y_binary[i]
            
            # æ·»åŠ æ­£åˆ™åŒ–é¡¹
            dw += self.weights / self.C
            
            # æ›´æ–°å‚æ•°
            self.weights -= self.learning_rate * dw
            self.bias -= self.learning_rate * db
            
            # è®¡ç®—æŸå¤±
            if iteration % 100 == 0:
                loss = self._compute_loss(X, y_binary)
                print(f"Iteration {iteration}, Loss: {loss:.4f}")
        
        # æ‰¾åˆ°æ”¯æŒå‘é‡
        self.support_vectors = self._find_support_vectors(X, y_binary)
        
        self.is_trained = True
        return self
    
    def _compute_loss(self, X: np.ndarray, y: np.ndarray) -> float:
        """è®¡ç®—é“°é“¾æŸå¤±"""
        loss = 0
        for i in range(len(X)):
            margin = y[i] * (np.dot(self.weights, X[i]) + self.bias)
            loss += max(0, 1 - margin)
        
        # æ·»åŠ æ­£åˆ™åŒ–é¡¹
        loss += 0.5 * np.dot(self.weights, self.weights) / self.C
        return loss / len(X)
    
    def _find_support_vectors(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:
        """æ‰¾åˆ°æ”¯æŒå‘é‡"""
        support_vectors = []
        for i in range(len(X)):
            margin = y[i] * (np.dot(self.weights, X[i]) + self.bias)
            if abs(margin - 1) < 1e-5:  # æ”¯æŒå‘é‡
                support_vectors.append(X[i])
        
        return np.array(support_vectors) if support_vectors else np.array([])
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """é¢„æµ‹"""
        if not self.is_trained:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒ")
        
        predictions = np.dot(X, self.weights) + self.bias
        return (predictions > 0).astype(int)
    
    def get_decision_boundary(self) -> Tuple[np.ndarray, float]:
        """è·å–å†³ç­–è¾¹ç•Œ"""
        return self.weights, self.bias

def demonstrate_svm():
    """æ¼”ç¤ºSVM"""
    print("=== SVMæ¼”ç¤º ===\n")
    
    # ç”Ÿæˆçº¿æ€§å¯åˆ†çš„æ•°æ®
    np.random.seed(42)
    n_samples = 100
    
    # åˆ›å»ºä¸¤ä¸ªç±»åˆ«çš„æ•°æ®
    X1 = np.random.multivariate_normal([0, 0], [[1, 0], [0, 1]], n_samples // 2)
    X2 = np.random.multivariate_normal([3, 3], [[1, 0], [0, 1]], n_samples // 2)
    
    X = np.vstack([X1, X2])
    y = np.hstack([np.zeros(n_samples // 2), np.ones(n_samples // 2)])
    
    # åˆ›å»ºæ•°æ®é›†
    dataset = Dataset(X, y, ["feature_1", "feature_2"], "class")
    train_dataset, test_dataset = dataset.split()
    
    # è®­ç»ƒSVM
    svm_model = SVM(C=1.0, learning_rate=0.01, max_iterations=500)
    svm_model.fit(train_dataset)
    
    # è¯„ä¼°æ¨¡å‹
    metrics = svm_model.evaluate(test_dataset)
    print(f"SVMè¯„ä¼°ç»“æœ: {metrics}")
    
    # è·å–å†³ç­–è¾¹ç•Œ
    weights, bias = svm_model.get_decision_boundary()
    print(f"å†³ç­–è¾¹ç•Œ: {weights[0]}*x1 + {weights[1]}*x2 + {bias} = 0")
    
    # æ”¯æŒå‘é‡æ•°é‡
    print(f"æ”¯æŒå‘é‡æ•°é‡: {len(svm_model.support_vectors)}")

if __name__ == "__main__":
    demonstrate_svm()
```

## 5. ç¥ç»ç½‘ç»œ

### 5.1 ç¥ç»ç½‘ç»œå®šä¹‰

**å®šä¹‰ 5.1** (ç¥ç»ç½‘ç»œ)
ç¥ç»ç½‘ç»œæ˜¯ç”±å¤šä¸ªç¥ç»å…ƒå±‚ç»„æˆçš„è®¡ç®—æ¨¡å‹ã€‚

**å½¢å¼åŒ–å®šä¹‰**:
$$f(x) = \sigma(W_L \sigma(W_{L-1} \ldots \sigma(W_1 x + b_1) \ldots + b_{L-1}) + b_L)$$

### 5.2 Pythonå®ç°

```python
class NeuralNetwork(Model):
    """ç¥ç»ç½‘ç»œæ¨¡å‹"""
    
    def __init__(self, layers: List[int], learning_rate: float = 0.01, 
                 max_iterations: int = 1000):
        super().__init__(ModelType.NEURAL_NETWORK)
        self.layers = layers
        self.learning_rate = learning_rate
        self.max_iterations = max_iterations
        self.weights = []
        self.biases = []
        self._initialize_parameters()
    
    def _initialize_parameters(self):
        """åˆå§‹åŒ–å‚æ•°"""
        for i in range(len(self.layers) - 1):
            w = np.random.randn(self.layers[i + 1], self.layers[i]) * 0.01
            b = np.zeros((self.layers[i + 1], 1))
            self.weights.append(w)
            self.biases.append(b)
    
    def _sigmoid(self, z: np.ndarray) -> np.ndarray:
        """Sigmoidæ¿€æ´»å‡½æ•°"""
        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))
    
    def _sigmoid_derivative(self, z: np.ndarray) -> np.ndarray:
        """Sigmoidå¯¼æ•°"""
        s = self._sigmoid(z)
        return s * (1 - s)
    
    def _forward_propagation(self, X: np.ndarray) -> Tuple[List[np.ndarray], List[np.ndarray]]:
        """å‰å‘ä¼ æ’­"""
        activations = [X.T]
        z_values = []
        
        for i in range(len(self.weights)):
            z = np.dot(self.weights[i], activations[-1]) + self.biases[i]
            z_values.append(z)
            activation = self._sigmoid(z)
            activations.append(activation)
        
        return activations, z_values
    
    def _backward_propagation(self, X: np.ndarray, y: np.ndarray, 
                            activations: List[np.ndarray], 
                            z_values: List[np.ndarray]) -> Tuple[List[np.ndarray], List[np.ndarray]]:
        """åå‘ä¼ æ’­"""
        m = X.shape[0]
        y = y.reshape(1, -1)
        
        # è®¡ç®—è¾“å‡ºå±‚è¯¯å·®
        delta = activations[-1] - y
        
        # åˆå§‹åŒ–æ¢¯åº¦
        weight_gradients = []
        bias_gradients = []
        
        # åå‘ä¼ æ’­è¯¯å·®
        for i in range(len(self.weights) - 1, -1, -1):
            # è®¡ç®—æƒé‡æ¢¯åº¦
            dW = np.dot(delta, activations[i].T) / m
            db = np.sum(delta, axis=1, keepdims=True) / m
            
            weight_gradients.insert(0, dW)
            bias_gradients.insert(0, db)
            
            # è®¡ç®—ä¸‹ä¸€å±‚çš„è¯¯å·®
            if i > 0:
                delta = np.dot(self.weights[i].T, delta) * self._sigmoid_derivative(z_values[i - 1])
        
        return weight_gradients, bias_gradients
    
    def fit(self, dataset: Dataset) -> 'NeuralNetwork':
        """è®­ç»ƒç¥ç»ç½‘ç»œ"""
        X, y = dataset.X, dataset.y
        if y is None:
            raise ValueError("ç¥ç»ç½‘ç»œéœ€è¦æ ‡è®°æ•°æ®")
        
        for iteration in range(self.max_iterations):
            # å‰å‘ä¼ æ’­
            activations, z_values = self._forward_propagation(X)
            
            # åå‘ä¼ æ’­
            weight_gradients, bias_gradients = self._backward_propagation(
                X, y, activations, z_values
            )
            
            # æ›´æ–°å‚æ•°
            for i in range(len(self.weights)):
                self.weights[i] -= self.learning_rate * weight_gradients[i]
                self.biases[i] -= self.learning_rate * bias_gradients[i]
            
            # è®¡ç®—æŸå¤±
            if iteration % 100 == 0:
                loss = self._compute_loss(X, y)
                print(f"Iteration {iteration}, Loss: {loss:.4f}")
        
        self.is_trained = True
        return self
    
    def _compute_loss(self, X: np.ndarray, y: np.ndarray) -> float:
        """è®¡ç®—äº¤å‰ç†µæŸå¤±"""
        activations, _ = self._forward_propagation(X)
        predictions = activations[-1].T
        
        epsilon = 1e-15
        predictions = np.clip(predictions, epsilon, 1 - epsilon)
        loss = -np.mean(y * np.log(predictions) + (1 - y) * np.log(1 - predictions))
        return loss
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """é¢„æµ‹"""
        if not self.is_trained:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒ")
        
        activations, _ = self._forward_propagation(X)
        predictions = activations[-1].T
        return (predictions >= 0.5).astype(int).flatten()

def demonstrate_neural_network():
    """æ¼”ç¤ºç¥ç»ç½‘ç»œ"""
    print("=== ç¥ç»ç½‘ç»œæ¼”ç¤º ===\n")
    
    # ç”ŸæˆXORæ•°æ®
    np.random.seed(42)
    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
    y = np.array([0, 1, 1, 0])
    
    # åˆ›å»ºæ•°æ®é›†
    dataset = Dataset(X, y, ["input_1", "input_2"], "output")
    
    # è®­ç»ƒç¥ç»ç½‘ç»œ
    nn_model = NeuralNetwork(layers=[2, 4, 1], learning_rate=0.1, max_iterations=1000)
    nn_model.fit(dataset)
    
    # è¯„ä¼°æ¨¡å‹
    metrics = nn_model.evaluate(dataset)
    print(f"ç¥ç»ç½‘ç»œè¯„ä¼°ç»“æœ: {metrics}")
    
    # æµ‹è¯•XOR
    print("\nXORæµ‹è¯•:")
    for i in range(len(X)):
        prediction = nn_model.predict(X[i:i+1])
        print(f"è¾“å…¥: {X[i]}, é¢„æµ‹: {prediction[0]}, å®é™…: {y[i]}")

if __name__ == "__main__":
    demonstrate_neural_network()
```

## 6. æ¨¡å‹è¯„ä¼°

### 6.1 è¯„ä¼°æŒ‡æ ‡

```python
class ModelEvaluator:
    """æ¨¡å‹è¯„ä¼°å™¨"""
    
    @staticmethod
    def cross_validation(model: Model, dataset: Dataset, k: int = 5) -> Dict[str, List[float]]:
        """KæŠ˜äº¤å‰éªŒè¯"""
        n_samples = len(dataset.X)
        fold_size = n_samples // k
        
        scores = []
        
        for i in range(k):
            # åˆ†å‰²æ•°æ®
            start_idx = i * fold_size
            end_idx = start_idx + fold_size if i < k - 1 else n_samples
            
            # åˆ›å»ºéªŒè¯é›†
            X_val = dataset.X[start_idx:end_idx]
            y_val = dataset.y[start_idx:end_idx] if dataset.y is not None else None
            val_dataset = Dataset(X_val, y_val, dataset.feature_names, dataset.target_name)
            
            # åˆ›å»ºè®­ç»ƒé›†
            X_train = np.vstack([dataset.X[:start_idx], dataset.X[end_idx:]])
            y_train = np.hstack([dataset.y[:start_idx], dataset.y[end_idx:]]) if dataset.y is not None else None
            train_dataset = Dataset(X_train, y_train, dataset.feature_names, dataset.target_name)
            
            # è®­ç»ƒå’Œè¯„ä¼°
            model_copy = type(model)()  # åˆ›å»ºæ¨¡å‹å‰¯æœ¬
            model_copy.fit(train_dataset)
            metrics = model_copy.evaluate(val_dataset)
            scores.append(metrics)
        
        # æ±‡æ€»ç»“æœ
        summary = {}
        for metric in scores[0].keys():
            values = [score[metric] for score in scores]
            summary[metric] = {
                'mean': np.mean(values),
                'std': np.std(values),
                'values': values
            }
        
        return summary
    
    @staticmethod
    def plot_learning_curve(model: Model, dataset: Dataset, 
                          train_sizes: List[float] = None) -> None:
        """ç»˜åˆ¶å­¦ä¹ æ›²çº¿"""
        if train_sizes is None:
            train_sizes = [0.1, 0.3, 0.5, 0.7, 0.9]
        
        train_scores = []
        val_scores = []
        
        for size in train_sizes:
            n_samples = int(len(dataset.X) * size)
            indices = np.random.choice(len(dataset.X), n_samples, replace=False)
            
            X_train = dataset.X[indices]
            y_train = dataset.y[indices] if dataset.y is not None else None
            train_dataset = Dataset(X_train, y_train, dataset.feature_names, dataset.target_name)
            
            # åˆ†å‰²éªŒè¯é›†
            train_subset, val_subset = train_dataset.split(test_size=0.2)
            
            # è®­ç»ƒæ¨¡å‹
            model_copy = type(model)()
            model_copy.fit(train_subset)
            
            # è¯„ä¼°
            train_metrics = model_copy.evaluate(train_subset)
            val_metrics = model_copy.evaluate(val_subset)
            
            train_scores.append(list(train_metrics.values())[0])
            val_scores.append(list(val_metrics.values())[0])
        
        # ç»˜åˆ¶æ›²çº¿
        plt.figure(figsize=(10, 6))
        plt.plot([len(dataset.X) * size for size in train_sizes], train_scores, 'o-', label='è®­ç»ƒé›†')
        plt.plot([len(dataset.X) * size for size in train_sizes], val_scores, 'o-', label='éªŒè¯é›†')
        plt.xlabel('è®­ç»ƒæ ·æœ¬æ•°')
        plt.ylabel('å‡†ç¡®ç‡')
        plt.title('å­¦ä¹ æ›²çº¿')
        plt.legend()
        plt.grid(True)
        plt.show()

def demonstrate_model_evaluation():
    """æ¼”ç¤ºæ¨¡å‹è¯„ä¼°"""
    print("=== æ¨¡å‹è¯„ä¼°æ¼”ç¤º ===\n")
    
    # ç”Ÿæˆæ•°æ®
    np.random.seed(42)
    n_samples = 1000
    n_features = 10
    
    X = np.random.randn(n_samples, n_features)
    y = (np.sum(X, axis=1) > 0).astype(int)
    
    dataset = Dataset(X, y, [f"feature_{i}" for i in range(n_features)], "target")
    
    # åˆ›å»ºæ¨¡å‹
    models = [
        LogisticRegression(),
        DecisionTree(max_depth=5),
        SVM(C=1.0)
    ]
    
    # äº¤å‰éªŒè¯
    for model in models:
        print(f"\n{model.model_type.value} äº¤å‰éªŒè¯ç»“æœ:")
        cv_results = ModelEvaluator.cross_validation(model, dataset, k=5)
        
        for metric, stats in cv_results.items():
            print(f"  {metric}: {stats['mean']:.4f} Â± {stats['std']:.4f}")
    
    # ç»˜åˆ¶å­¦ä¹ æ›²çº¿
    print("\nç»˜åˆ¶å­¦ä¹ æ›²çº¿...")
    ModelEvaluator.plot_learning_curve(LogisticRegression(), dataset)

if __name__ == "__main__":
    demonstrate_model_evaluation()
```

## 7. ç†è®ºè¯æ˜

### 7.1 å­¦ä¹ ç†è®º

**å®šç† 7.1** (VCç»´ç†è®º)
å¯¹äºå‡è®¾ç©ºé—´ $H$ï¼Œå¦‚æœVCç»´ä¸º $d$ï¼Œåˆ™æ³›åŒ–è¯¯å·®ä¸Šç•Œä¸ºï¼š
$$\epsilon \leq \sqrt{\frac{d \log(2n/d) + \log(1/\delta)}{n}}$$

**è¯æ˜æ€è·¯**:

1. VCç»´è¡¡é‡å‡è®¾ç©ºé—´çš„å¤æ‚åº¦
2. å¤æ‚åº¦è¶Šé«˜ï¼Œéœ€è¦çš„æ ·æœ¬è¶Šå¤š
3. æ³›åŒ–è¯¯å·®ä¸æ ·æœ¬æ•°å’Œå¤æ‚åº¦ç›¸å…³

### 7.2 æ”¶æ•›æ€§è¯æ˜

**å®šç† 7.2** (æ¢¯åº¦ä¸‹é™æ”¶æ•›æ€§)
å¯¹äºå‡¸å‡½æ•°ï¼Œæ¢¯åº¦ä¸‹é™ç®—æ³•èƒ½å¤Ÿæ”¶æ•›åˆ°å…¨å±€æœ€ä¼˜è§£ã€‚

**è¯æ˜**:

1. å‡¸å‡½æ•°å…·æœ‰å”¯ä¸€å…¨å±€æœ€ä¼˜è§£
2. æ¢¯åº¦æ–¹å‘æŒ‡å‘å‡½æ•°ä¸‹é™æ–¹å‘
3. é€‚å½“çš„å­¦ä¹ ç‡ä¿è¯æ”¶æ•›

## 8. åº”ç”¨å®ä¾‹

### 8.1 å›¾åƒåˆ†ç±»

```python
def demonstrate_image_classification():
    """æ¼”ç¤ºå›¾åƒåˆ†ç±»"""
    print("=== å›¾åƒåˆ†ç±»æ¼”ç¤º ===\n")
    
    # æ¨¡æ‹Ÿå›¾åƒæ•°æ®
    np.random.seed(42)
    n_samples = 1000
    image_size = 28 * 28  # 28x28åƒç´ 
    
    # ç”Ÿæˆæ¨¡æ‹Ÿå›¾åƒç‰¹å¾
    X = np.random.randn(n_samples, image_size)
    
    # ç”Ÿæˆæ ‡ç­¾ï¼ˆæ¨¡æ‹Ÿæ‰‹å†™æ•°å­—åˆ†ç±»ï¼‰
    y = np.random.randint(0, 10, n_samples)
    
    dataset = Dataset(X, y, [f"pixel_{i}" for i in range(image_size)], "digit")
    train_dataset, test_dataset = dataset.split()
    
    # è®­ç»ƒå¤šä¸ªæ¨¡å‹
    models = {
        "é€»è¾‘å›å½’": LogisticRegression(),
        "å†³ç­–æ ‘": DecisionTree(max_depth=10),
        "SVM": SVM(C=1.0),
        "ç¥ç»ç½‘ç»œ": NeuralNetwork(layers=[image_size, 128, 64, 10], learning_rate=0.01)
    }
    
    results = {}
    for name, model in models.items():
        print(f"è®­ç»ƒ {name}...")
        model.fit(train_dataset)
        metrics = model.evaluate(test_dataset)
        results[name] = metrics
        print(f"{name} å‡†ç¡®ç‡: {metrics.get('accuracy', 0):.4f}")
    
    # æ¯”è¾ƒç»“æœ
    print("\næ¨¡å‹æ¯”è¾ƒ:")
    for name, metrics in results.items():
        print(f"{name}: {metrics}")

if __name__ == "__main__":
    demonstrate_image_classification()
```

### 8.2 æ¨èç³»ç»Ÿ

```python
class RecommendationSystem:
    """æ¨èç³»ç»Ÿ"""
    
    def __init__(self, n_users: int, n_items: int, n_features: int = 10):
        self.n_users = n_users
        self.n_items = n_items
        self.n_features = n_features
        self.user_features = np.random.randn(n_users, n_features)
        self.item_features = np.random.randn(n_items, n_features)
    
    def predict_rating(self, user_id: int, item_id: int) -> float:
        """é¢„æµ‹ç”¨æˆ·å¯¹ç‰©å“çš„è¯„åˆ†"""
        return np.dot(self.user_features[user_id], self.item_features[item_id])
    
    def recommend_items(self, user_id: int, n_recommendations: int = 5) -> List[int]:
        """ä¸ºç”¨æˆ·æ¨èç‰©å“"""
        predictions = []
        for item_id in range(self.n_items):
            rating = self.predict_rating(user_id, item_id)
            predictions.append((item_id, rating))
        
        # æŒ‰è¯„åˆ†æ’åº
        predictions.sort(key=lambda x: x[1], reverse=True)
        return [item_id for item_id, _ in predictions[:n_recommendations]]
    
    def train(self, ratings: List[Tuple[int, int, float]], 
             learning_rate: float = 0.01, max_iterations: int = 100):
        """è®­ç»ƒæ¨èç³»ç»Ÿ"""
        for iteration in range(max_iterations):
            total_loss = 0
            
            for user_id, item_id, true_rating in ratings:
                # é¢„æµ‹è¯„åˆ†
                predicted_rating = self.predict_rating(user_id, item_id)
                
                # è®¡ç®—è¯¯å·®
                error = true_rating - predicted_rating
                total_loss += error ** 2
                
                # æ›´æ–°å‚æ•°
                self.user_features[user_id] += learning_rate * error * self.item_features[item_id]
                self.item_features[item_id] += learning_rate * error * self.user_features[user_id]
            
            if iteration % 10 == 0:
                print(f"Iteration {iteration}, Loss: {total_loss:.4f}")

def demonstrate_recommendation_system():
    """æ¼”ç¤ºæ¨èç³»ç»Ÿ"""
    print("=== æ¨èç³»ç»Ÿæ¼”ç¤º ===\n")
    
    # åˆ›å»ºæ¨èç³»ç»Ÿ
    n_users, n_items = 100, 50
    rec_system = RecommendationSystem(n_users, n_items)
    
    # ç”Ÿæˆæ¨¡æ‹Ÿè¯„åˆ†æ•°æ®
    np.random.seed(42)
    n_ratings = 1000
    ratings = []
    
    for _ in range(n_ratings):
        user_id = np.random.randint(0, n_users)
        item_id = np.random.randint(0, n_items)
        rating = np.random.randint(1, 6)  # 1-5åˆ†
        ratings.append((user_id, item_id, rating))
    
    # è®­ç»ƒæ¨èç³»ç»Ÿ
    print("è®­ç»ƒæ¨èç³»ç»Ÿ...")
    rec_system.train(ratings, learning_rate=0.01, max_iterations=50)
    
    # ä¸ºç”¨æˆ·æ¨èç‰©å“
    user_id = 0
    recommendations = rec_system.recommend_items(user_id, n_recommendations=5)
    print(f"ä¸ºç”¨æˆ· {user_id} æ¨èçš„ç‰©å“: {recommendations}")
    
    # é¢„æµ‹è¯„åˆ†
    item_id = 10
    predicted_rating = rec_system.predict_rating(user_id, item_id)
    print(f"ç”¨æˆ· {user_id} å¯¹ç‰©å“ {item_id} çš„é¢„æµ‹è¯„åˆ†: {predicted_rating:.2f}")

if __name__ == "__main__":
    demonstrate_recommendation_system()
```

## 9. æ€§èƒ½åˆ†æ

### 9.1 æ—¶é—´å¤æ‚åº¦

- **çº¿æ€§å›å½’**: $O(n \times d \times iterations)$
- **é€»è¾‘å›å½’**: $O(n \times d \times iterations)$
- **å†³ç­–æ ‘**: $O(n \times d \times \log(n))$
- **SVM**: $O(n^2 \times d \times iterations)$
- **ç¥ç»ç½‘ç»œ**: $O(n \times d \times L \times iterations)$

### 9.2 ç©ºé—´å¤æ‚åº¦

- **çº¿æ€§/é€»è¾‘å›å½’**: $O(d)$
- **å†³ç­–æ ‘**: $O(n \times d)$
- **SVM**: $O(n \times d)$
- **ç¥ç»ç½‘ç»œ**: $O(d \times L)$

## 10. æ€»ç»“

æœ¬æ–‡æ¡£ä»å½¢å¼åŒ–è§’åº¦é˜è¿°äº†æœºå™¨å­¦ä¹ çš„ç†è®ºåŸºç¡€ï¼ŒåŒ…æ‹¬ï¼š

1. **åŸºæœ¬æ¦‚å¿µ**: æœºå™¨å­¦ä¹ å®šä¹‰ã€å­¦ä¹ ç±»å‹ã€æ•°æ®é›†ç»“æ„
2. **ç»å…¸ç®—æ³•**: çº¿æ€§å›å½’ã€é€»è¾‘å›å½’ã€å†³ç­–æ ‘ã€SVMã€ç¥ç»ç½‘ç»œ
3. **æ¨¡å‹è¯„ä¼°**: äº¤å‰éªŒè¯ã€å­¦ä¹ æ›²çº¿ã€è¯„ä¼°æŒ‡æ ‡
4. **ç†è®ºè¯æ˜**: å­¦ä¹ ç†è®ºã€æ”¶æ•›æ€§åˆ†æ
5. **åº”ç”¨å®ä¾‹**: å›¾åƒåˆ†ç±»ã€æ¨èç³»ç»Ÿ

æ‰€æœ‰æ¦‚å¿µéƒ½æœ‰å®Œæ•´çš„Pythonå®ç°ï¼ŒåŒ…æ‹¬ï¼š

- å®Œæ•´çš„æœºå™¨å­¦ä¹ ç®—æ³•å®ç°
- æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°æ¡†æ¶
- æ•°æ®é¢„å¤„ç†å’Œç‰¹å¾å·¥ç¨‹
- å®é™…åº”ç”¨ç¤ºä¾‹
- æ€§èƒ½åˆ†æå’Œä¼˜åŒ–

æœºå™¨å­¦ä¹ ä¸ºäººå·¥æ™ºèƒ½åº”ç”¨æä¾›äº†å¼ºå¤§çš„ç†è®ºåŸºç¡€å’Œå®ç”¨å·¥å…·ã€‚

---

*æœ€åæ›´æ–°: 2024-12-19*
*ä¸‹æ¬¡æ›´æ–°: å®Œæˆæ·±åº¦å­¦ä¹ æ–‡æ¡£å*
