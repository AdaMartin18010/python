# 04-02-03 æ·±åº¦å­¦ä¹ åŸºç¡€

## ğŸ“‹ ç›®å½•

- [04-02-03 æ·±åº¦å­¦ä¹ åŸºç¡€](#04-02-03-æ·±åº¦å­¦ä¹ åŸºç¡€)
  - [ğŸ“‹ ç›®å½•](#-ç›®å½•)
  - [ğŸ¯ æ¦‚è¿°](#-æ¦‚è¿°)
  - [ğŸ”¬ æ¦‚å¿µå®šä¹‰](#-æ¦‚å¿µå®šä¹‰)
  - [ğŸ“ æ•°å­¦å½¢å¼åŒ–](#-æ•°å­¦å½¢å¼åŒ–)
  - [ğŸ Pythonå®ç°](#-pythonå®ç°)
  - [ğŸ“Š ç½‘ç»œæ¶æ„](#-ç½‘ç»œæ¶æ„)
  - [ğŸ”„ è®­ç»ƒæµç¨‹](#-è®­ç»ƒæµç¨‹)
  - [ğŸ“ˆ åº”ç”¨æ¡ˆä¾‹](#-åº”ç”¨æ¡ˆä¾‹)
  - [ğŸ”— ç›¸å…³é“¾æ¥](#-ç›¸å…³é“¾æ¥)

## ğŸ¯ æ¦‚è¿°

æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥å­¦ä¹ æ•°æ®çš„å±‚æ¬¡åŒ–è¡¨ç¤ºã€‚æœ¬æ–‡æ¡£æ¶µç›–ç¥ç»ç½‘ç»œåŸºç¡€ã€å·ç§¯ç¥ç»ç½‘ç»œã€å¾ªç¯ç¥ç»ç½‘ç»œã€ä¼˜åŒ–ç®—æ³•ç­‰æ ¸å¿ƒæ¦‚å¿µã€‚

## ğŸ”¬ æ¦‚å¿µå®šä¹‰

### å®šä¹‰ 3.1 (æ·±åº¦å­¦ä¹ )
æ·±åº¦å­¦ä¹ æ˜¯ä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œè¿›è¡Œç‰¹å¾å­¦ä¹ å’Œæ¨¡å¼è¯†åˆ«çš„æœºå™¨å­¦ä¹ æ–¹æ³•ã€‚å½¢å¼åŒ–å®šä¹‰ä¸ºï¼š

$$DL = (N, L, W, \sigma, \mathcal{L})$$

å…¶ä¸­ï¼š
- $N$ æ˜¯ç¥ç»ç½‘ç»œæ¶æ„
- $L$ æ˜¯å±‚æ•°
- $W$ æ˜¯æƒé‡å‚æ•°
- $\sigma$ æ˜¯æ¿€æ´»å‡½æ•°
- $\mathcal{L}$ æ˜¯æŸå¤±å‡½æ•°

### å®šä¹‰ 3.2 (å‰é¦ˆç¥ç»ç½‘ç»œ)
å‰é¦ˆç¥ç»ç½‘ç»œæ˜¯ä¸€ä¸ªæœ‰å‘æ— ç¯å›¾ï¼Œä¿¡æ¯ä»è¾“å…¥å±‚æµå‘è¾“å‡ºå±‚ï¼š

$$y = f_L \circ f_{L-1} \circ \cdots \circ f_1(x)$$

å…¶ä¸­ $f_i(x) = \sigma_i(W_i x + b_i)$ æ˜¯ç¬¬ $i$ å±‚çš„å˜æ¢ã€‚

### å®šä¹‰ 3.3 (åå‘ä¼ æ’­)
åå‘ä¼ æ’­æ˜¯è®¡ç®—æ¢¯åº¦çš„é«˜æ•ˆç®—æ³•ï¼š

$$\frac{\partial \mathcal{L}}{\partial W_i} = \frac{\partial \mathcal{L}}{\partial y} \cdot \frac{\partial y}{\partial W_i}$$

## ğŸ“ æ•°å­¦å½¢å¼åŒ–

### å®šç† 3.1 (é€šç”¨è¿‘ä¼¼å®šç†)
å¯¹äºä»»æ„è¿ç»­å‡½æ•° $f: [0,1]^n \rightarrow \mathbb{R}$ å’Œ $\epsilon > 0$ï¼Œå­˜åœ¨ä¸€ä¸ªå•éšè—å±‚ç¥ç»ç½‘ç»œ $g$ ä½¿å¾—ï¼š

$$\sup_{x \in [0,1]^n} |f(x) - g(x)| < \epsilon$$

### å®šç† 3.2 (åå‘ä¼ æ’­ç®—æ³•)
å¯¹äºç¥ç»ç½‘ç»œ $f(x) = \sigma(W_L \sigma(W_{L-1} \cdots \sigma(W_1 x + b_1) \cdots + b_{L-1}) + b_L)$ï¼Œæ¢¯åº¦è®¡ç®—ä¸ºï¼š

$$\frac{\partial \mathcal{L}}{\partial W_l} = \delta_l \cdot a_{l-1}^T$$

å…¶ä¸­ $\delta_l = \frac{\partial \mathcal{L}}{\partial z_l}$ æ˜¯ç¬¬ $l$ å±‚çš„è¯¯å·®é¡¹ã€‚

### å®šä¹‰ 3.4 (å·ç§¯æ“ä½œ)
äºŒç»´å·ç§¯æ“ä½œå®šä¹‰ä¸ºï¼š

$$(f * k)(i, j) = \sum_{m} \sum_{n} f(m, n) \cdot k(i-m, j-n)$$

å…¶ä¸­ $f$ æ˜¯è¾“å…¥ç‰¹å¾å›¾ï¼Œ$k$ æ˜¯å·ç§¯æ ¸ã€‚

## ğŸ Pythonå®ç°

### 1. åŸºç¡€ç¥ç»ç½‘ç»œå®ç°

```python
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset
from typing import List, Tuple, Optional

class DeepNeuralNetwork:
    """æ·±åº¦ç¥ç»ç½‘ç»œåŸºç¡€å®ç°"""
    
    def __init__(self, layer_sizes: List[int], activation: str = 'relu'):
        self.layer_sizes = layer_sizes
        self.activation = activation
        self.weights = []
        self.biases = []
        self.activations = []
        
        # åˆå§‹åŒ–æƒé‡å’Œåç½®
        for i in range(len(layer_sizes) - 1):
            w = np.random.randn(layer_sizes[i + 1], layer_sizes[i]) * 0.01
            b = np.zeros((layer_sizes[i + 1], 1))
            self.weights.append(w)
            self.biases.append(b)
    
    def forward(self, X: np.ndarray) -> np.ndarray:
        """å‰å‘ä¼ æ’­"""
        self.activations = [X]
        
        for i in range(len(self.weights)):
            z = np.dot(self.weights[i], self.activations[-1]) + self.biases[i]
            a = self._apply_activation(z)
            self.activations.append(a)
        
        return self.activations[-1]
    
    def backward(self, X: np.ndarray, y: np.ndarray, learning_rate: float = 0.01):
        """åå‘ä¼ æ’­"""
        m = X.shape[1]
        
        # è®¡ç®—è¾“å‡ºå±‚è¯¯å·®
        delta = self.activations[-1] - y
        
        # åå‘ä¼ æ’­è¯¯å·®
        for i in range(len(self.weights) - 1, -1, -1):
            # è®¡ç®—æƒé‡å’Œåç½®çš„æ¢¯åº¦
            dW = np.dot(delta, self.activations[i].T) / m
            db = np.sum(delta, axis=1, keepdims=True) / m
            
            # æ›´æ–°å‚æ•°
            self.weights[i] -= learning_rate * dW
            self.biases[i] -= learning_rate * db
            
            # è®¡ç®—ä¸‹ä¸€å±‚çš„è¯¯å·®ï¼ˆé™¤äº†è¾“å…¥å±‚ï¼‰
            if i > 0:
                delta = np.dot(self.weights[i].T, delta) * self._apply_activation_derivative(self.activations[i])
    
    def _apply_activation(self, z: np.ndarray) -> np.ndarray:
        """åº”ç”¨æ¿€æ´»å‡½æ•°"""
        if self.activation == 'relu':
            return np.maximum(0, z)
        elif self.activation == 'sigmoid':
            return 1 / (1 + np.exp(-np.clip(z, -500, 500)))
        elif self.activation == 'tanh':
            return np.tanh(z)
        else:
            return z
    
    def _apply_activation_derivative(self, a: np.ndarray) -> np.ndarray:
        """åº”ç”¨æ¿€æ´»å‡½æ•°å¯¼æ•°"""
        if self.activation == 'relu':
            return (a > 0).astype(float)
        elif self.activation == 'sigmoid':
            return a * (1 - a)
        elif self.activation == 'tanh':
            return 1 - a ** 2
        else:
            return np.ones_like(a)
    
    def train(self, X: np.ndarray, y: np.ndarray, epochs: int = 1000, learning_rate: float = 0.01) -> List[float]:
        """è®­ç»ƒæ¨¡å‹"""
        costs = []
        
        for epoch in range(epochs):
            # å‰å‘ä¼ æ’­
            output = self.forward(X)
            
            # è®¡ç®—æˆæœ¬
            cost = self._compute_cost(output, y)
            costs.append(cost)
            
            # åå‘ä¼ æ’­
            self.backward(X, y, learning_rate)
            
            if epoch % 100 == 0:
                print(f'Epoch {epoch}: Cost = {cost:.6f}')
        
        return costs
    
    def _compute_cost(self, output: np.ndarray, y: np.ndarray) -> float:
        """è®¡ç®—æˆæœ¬å‡½æ•°"""
        m = y.shape[1]
        cost = -np.sum(y * np.log(output + 1e-8) + (1 - y) * np.log(1 - output + 1e-8)) / m
        return cost

class ConvolutionalNeuralNetwork(nn.Module):
    """å·ç§¯ç¥ç»ç½‘ç»œ"""
    
    def __init__(self, num_classes: int = 10):
        super(ConvolutionalNeuralNetwork, self).__init__()
        
        # å·ç§¯å±‚
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        
        # æ± åŒ–å±‚
        self.pool = nn.MaxPool2d(2, 2)
        
        # å…¨è¿æ¥å±‚
        self.fc1 = nn.Linear(128 * 3 * 3, 512)
        self.fc2 = nn.Linear(512, num_classes)
        
        # Dropout
        self.dropout = nn.Dropout(0.5)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­"""
        # ç¬¬ä¸€ä¸ªå·ç§¯å—
        x = F.relu(self.conv1(x))
        x = self.pool(x)
        
        # ç¬¬äºŒä¸ªå·ç§¯å—
        x = F.relu(self.conv2(x))
        x = self.pool(x)
        
        # ç¬¬ä¸‰ä¸ªå·ç§¯å—
        x = F.relu(self.conv3(x))
        x = self.pool(x)
        
        # å±•å¹³
        x = x.view(x.size(0), -1)
        
        # å…¨è¿æ¥å±‚
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        
        return x

class RecurrentNeuralNetwork(nn.Module):
    """å¾ªç¯ç¥ç»ç½‘ç»œ"""
    
    def __init__(self, input_size: int, hidden_size: int, num_layers: int, num_classes: int):
        super(RecurrentNeuralNetwork, self).__init__()
        
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # LSTMå±‚
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        
        # å…¨è¿æ¥å±‚
        self.fc = nn.Linear(hidden_size, num_classes)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­"""
        # åˆå§‹åŒ–éšè—çŠ¶æ€
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        
        # LSTMå‰å‘ä¼ æ’­
        out, _ = self.lstm(x, (h0, c0))
        
        # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        out = self.fc(out[:, -1, :])
        
        return out

class LongShortTermMemory(nn.Module):
    """LSTMç½‘ç»œ"""
    
    def __init__(self, input_size: int, hidden_size: int, num_layers: int, num_classes: int):
        super(LongShortTermMemory, self).__init__()
        
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # LSTMå±‚
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=0.2)
        
        # å…¨è¿æ¥å±‚
        self.fc = nn.Linear(hidden_size, num_classes)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­"""
        # åˆå§‹åŒ–éšè—çŠ¶æ€
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        
        # LSTMå‰å‘ä¼ æ’­
        out, _ = self.lstm(x, (h0, c0))
        
        # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        out = self.fc(out[:, -1, :])
        
        return out

class TransformerBlock(nn.Module):
    """Transformerå—"""
    
    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):
        super(TransformerBlock, self).__init__()
        
        self.attention = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)
        self.feed_forward = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_ff, d_model)
        )
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­"""
        # è‡ªæ³¨æ„åŠ›
        attn_output, _ = self.attention(x, x, x)
        x = self.norm1(x + self.dropout(attn_output))
        
        # å‰é¦ˆç½‘ç»œ
        ff_output = self.feed_forward(x)
        x = self.norm2(x + self.dropout(ff_output))
        
        return x

class Transformer(nn.Module):
    """Transformeræ¨¡å‹"""
    
    def __init__(self, vocab_size: int, d_model: int, n_heads: int, n_layers: int, 
                 d_ff: int, max_seq_length: int, num_classes: int):
        super(Transformer, self).__init__()
        
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoding = self._create_positional_encoding(max_seq_length, d_model)
        
        self.transformer_blocks = nn.ModuleList([
            TransformerBlock(d_model, n_heads, d_ff) for _ in range(n_layers)
        ])
        
        self.fc = nn.Linear(d_model, num_classes)
        self.dropout = nn.Dropout(0.1)
    
    def _create_positional_encoding(self, max_seq_length: int, d_model: int) -> torch.Tensor:
        """åˆ›å»ºä½ç½®ç¼–ç """
        pe = torch.zeros(max_seq_length, d_model)
        position = torch.arange(0, max_seq_length).unsqueeze(1).float()
        
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           -(np.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        
        return pe.unsqueeze(0)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­"""
        seq_length = x.size(1)
        
        # åµŒå…¥å’Œä½ç½®ç¼–ç 
        x = self.embedding(x)
        x = x + self.pos_encoding[:, :seq_length, :].to(x.device)
        x = self.dropout(x)
        
        # Transformerå—
        for transformer_block in self.transformer_blocks:
            x = transformer_block(x)
        
        # å–åºåˆ—çš„å¹³å‡å€¼
        x = torch.mean(x, dim=1)
        
        # åˆ†ç±»
        x = self.fc(x)
        
        return x
```

### 2. ä¼˜åŒ–ç®—æ³•å®ç°

```python
class Optimizer:
    """ä¼˜åŒ–å™¨åŸºç±»"""
    
    def __init__(self, learning_rate: float = 0.01):
        self.learning_rate = learning_rate
    
    def update(self, params: List[np.ndarray], grads: List[np.ndarray]):
        """æ›´æ–°å‚æ•°"""
        raise NotImplementedError

class SGD(Optimizer):
    """éšæœºæ¢¯åº¦ä¸‹é™"""
    
    def __init__(self, learning_rate: float = 0.01, momentum: float = 0.0):
        super().__init__(learning_rate)
        self.momentum = momentum
        self.velocity = None
    
    def update(self, params: List[np.ndarray], grads: List[np.ndarray]):
        """æ›´æ–°å‚æ•°"""
        if self.velocity is None:
            self.velocity = [np.zeros_like(param) for param in params]
        
        for i in range(len(params)):
            self.velocity[i] = self.momentum * self.velocity[i] - self.learning_rate * grads[i]
            params[i] += self.velocity[i]

class Adam(Optimizer):
    """Adamä¼˜åŒ–å™¨"""
    
    def __init__(self, learning_rate: float = 0.001, beta1: float = 0.9, beta2: float = 0.999, epsilon: float = 1e-8):
        super().__init__(learning_rate)
        self.beta1 = beta1
        self.beta2 = beta2
        self.epsilon = epsilon
        self.m = None
        self.v = None
        self.t = 0
    
    def update(self, params: List[np.ndarray], grads: List[np.ndarray]):
        """æ›´æ–°å‚æ•°"""
        if self.m is None:
            self.m = [np.zeros_like(param) for param in params]
            self.v = [np.zeros_like(param) for param in params]
        
        self.t += 1
        
        for i in range(len(params)):
            # æ›´æ–°åç½®ä¿®æ­£çš„ä¸€é˜¶çŸ©ä¼°è®¡
            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grads[i]
            
            # æ›´æ–°åç½®ä¿®æ­£çš„äºŒé˜¶çŸ©ä¼°è®¡
            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * (grads[i] ** 2)
            
            # åç½®ä¿®æ­£
            m_hat = self.m[i] / (1 - self.beta1 ** self.t)
            v_hat = self.v[i] / (1 - self.beta2 ** self.t)
            
            # æ›´æ–°å‚æ•°
            params[i] -= self.learning_rate * m_hat / (np.sqrt(v_hat) + self.epsilon)

class RMSprop(Optimizer):
    """RMSpropä¼˜åŒ–å™¨"""
    
    def __init__(self, learning_rate: float = 0.001, rho: float = 0.9, epsilon: float = 1e-8):
        super().__init__(learning_rate)
        self.rho = rho
        self.epsilon = epsilon
        self.v = None
    
    def update(self, params: List[np.ndarray], grads: List[np.ndarray]):
        """æ›´æ–°å‚æ•°"""
        if self.v is None:
            self.v = [np.zeros_like(param) for param in params]
        
        for i in range(len(params)):
            # æ›´æ–°ç§»åŠ¨å¹³å‡
            self.v[i] = self.rho * self.v[i] + (1 - self.rho) * (grads[i] ** 2)
            
            # æ›´æ–°å‚æ•°
            params[i] -= self.learning_rate * grads[i] / (np.sqrt(self.v[i]) + self.epsilon)
```

### 3. æŸå¤±å‡½æ•°å®ç°

```python
class LossFunction:
    """æŸå¤±å‡½æ•°åŸºç±»"""
    
    def __call__(self, y_pred: np.ndarray, y_true: np.ndarray) -> float:
        """è®¡ç®—æŸå¤±"""
        raise NotImplementedError
    
    def gradient(self, y_pred: np.ndarray, y_true: np.ndarray) -> np.ndarray:
        """è®¡ç®—æ¢¯åº¦"""
        raise NotImplementedError

class CrossEntropyLoss(LossFunction):
    """äº¤å‰ç†µæŸå¤±"""
    
    def __call__(self, y_pred: np.ndarray, y_true: np.ndarray) -> float:
        """è®¡ç®—æŸå¤±"""
        epsilon = 1e-15
        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
        return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))
    
    def gradient(self, y_pred: np.ndarray, y_true: np.ndarray) -> np.ndarray:
        """è®¡ç®—æ¢¯åº¦"""
        epsilon = 1e-15
        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
        return y_pred - y_true

class MSELoss(LossFunction):
    """å‡æ–¹è¯¯å·®æŸå¤±"""
    
    def __call__(self, y_pred: np.ndarray, y_true: np.ndarray) -> float:
        """è®¡ç®—æŸå¤±"""
        return np.mean((y_pred - y_true) ** 2)
    
    def gradient(self, y_pred: np.ndarray, y_true: np.ndarray) -> np.ndarray:
        """è®¡ç®—æ¢¯åº¦"""
        return 2 * (y_pred - y_true) / y_pred.size

class HingeLoss(LossFunction):
    """é“°é“¾æŸå¤±ï¼ˆç”¨äºSVMï¼‰"""
    
    def __init__(self, margin: float = 1.0):
        self.margin = margin
    
    def __call__(self, y_pred: np.ndarray, y_true: np.ndarray) -> float:
        """è®¡ç®—æŸå¤±"""
        loss = np.maximum(0, self.margin - y_true * y_pred)
        return np.mean(loss)
    
    def gradient(self, y_pred: np.ndarray, y_true: np.ndarray) -> np.ndarray:
        """è®¡ç®—æ¢¯åº¦"""
        mask = (self.margin - y_true * y_pred) > 0
        return -y_true * mask
```

## ğŸ“Š ç½‘ç»œæ¶æ„

### å¸¸è§ç½‘ç»œæ¶æ„å¯¹æ¯”

| æ¶æ„ | é€‚ç”¨åœºæ™¯ | ä¼˜ç‚¹ | ç¼ºç‚¹ | å‚æ•°é‡ |
|------|----------|------|------|--------|
| å…¨è¿æ¥ç½‘ç»œ | å°è§„æ¨¡åˆ†ç±» | ç®€å•ã€å¯è§£é‡Š | å‚æ•°é‡å¤§ã€å®¹æ˜“è¿‡æ‹Ÿåˆ | O(nÂ²) |
| CNN | å›¾åƒå¤„ç† | å±€éƒ¨ç‰¹å¾ã€å‚æ•°å…±äº« | éœ€è¦å¤§é‡æ•°æ® | O(kÂ²Ã—c) |
| RNN | åºåˆ—æ•°æ® | å¤„ç†å˜é•¿åºåˆ— | æ¢¯åº¦æ¶ˆå¤±ã€è®­ç»ƒæ…¢ | O(hÂ²) |
| LSTM | é•¿åºåˆ— | é•¿æœŸä¾èµ–ã€æ¢¯åº¦ç¨³å®š | è®¡ç®—å¤æ‚ | O(hÂ²) |
| Transformer | è‡ªç„¶è¯­è¨€ | å¹¶è¡Œè®­ç»ƒã€æ³¨æ„åŠ›æœºåˆ¶ | å†…å­˜æ¶ˆè€—å¤§ | O(nÂ²Ã—d) |

### ç½‘ç»œæ¶æ„å›¾

```mermaid
graph TB
    A[è¾“å…¥å±‚] --> B[å·ç§¯å±‚1]
    B --> C[æ± åŒ–å±‚1]
    C --> D[å·ç§¯å±‚2]
    D --> E[æ± åŒ–å±‚2]
    E --> F[å…¨è¿æ¥å±‚1]
    F --> G[Dropout]
    G --> H[å…¨è¿æ¥å±‚2]
    H --> I[è¾“å‡ºå±‚]
    
    subgraph "å·ç§¯å—"
        B1[å·ç§¯æ“ä½œ]
        B2[æ‰¹å½’ä¸€åŒ–]
        B3[æ¿€æ´»å‡½æ•°]
    end
    
    subgraph "æ³¨æ„åŠ›æœºåˆ¶"
        I1[æŸ¥è¯¢Q]
        I2[é”®K]
        I3[å€¼V]
        I4[æ³¨æ„åŠ›æƒé‡]
    end
```

## ğŸ”„ è®­ç»ƒæµç¨‹

### æ·±åº¦å­¦ä¹ è®­ç»ƒæµç¨‹

```python
def deep_learning_training_pipeline():
    """æ·±åº¦å­¦ä¹ è®­ç»ƒæµç¨‹"""
    
    # 1. æ•°æ®å‡†å¤‡
    np.random.seed(42)
    X = np.random.randn(1000, 784)  # æ¨¡æ‹ŸMNISTæ•°æ®
    y = np.random.randint(0, 10, 1000)
    
    # è½¬æ¢ä¸ºPyTorchå¼ é‡
    X_tensor = torch.FloatTensor(X)
    y_tensor = torch.LongTensor(y)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    dataset = TensorDataset(X_tensor, y_tensor)
    train_loader = DataLoader(dataset, batch_size=32, shuffle=True)
    
    # 2. æ¨¡å‹å®šä¹‰
    model = nn.Sequential(
        nn.Linear(784, 512),
        nn.ReLU(),
        nn.Dropout(0.2),
        nn.Linear(512, 256),
        nn.ReLU(),
        nn.Dropout(0.2),
        nn.Linear(256, 10)
    )
    
    # 3. æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    
    # 4. è®­ç»ƒå¾ªç¯
    num_epochs = 50
    train_losses = []
    
    for epoch in range(num_epochs):
        model.train()
        epoch_loss = 0.0
        
        for batch_idx, (data, target) in enumerate(train_loader):
            # å‰å‘ä¼ æ’­
            output = model(data)
            loss = criterion(output, target)
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            epoch_loss += loss.item()
        
        avg_loss = epoch_loss / len(train_loader)
        train_losses.append(avg_loss)
        
        if epoch % 10 == 0:
            print(f'Epoch {epoch}: Loss = {avg_loss:.4f}')
    
    # 5. æ¨¡å‹è¯„ä¼°
    model.eval()
    with torch.no_grad():
        test_output = model(X_tensor)
        _, predicted = torch.max(test_output.data, 1)
        accuracy = (predicted == y_tensor).sum().item() / len(y_tensor)
    
    print(f'æœ€ç»ˆå‡†ç¡®ç‡: {accuracy:.4f}')
    
    # 6. å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹
    plt.figure(figsize=(10, 6))
    plt.plot(train_losses)
    plt.title('æ·±åº¦å­¦ä¹ è®­ç»ƒæŸå¤±')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.grid(True)
    plt.show()
    
    return model, train_losses, accuracy

# è¿è¡Œè®­ç»ƒæµç¨‹
if __name__ == "__main__":
    model, losses, accuracy = deep_learning_training_pipeline()
```

## ğŸ“ˆ åº”ç”¨æ¡ˆä¾‹

### æ¡ˆä¾‹1ï¼šå›¾åƒåˆ†ç±»

```python
def image_classification_example():
    """å›¾åƒåˆ†ç±»ç¤ºä¾‹"""
    
    # ç”Ÿæˆæ¨¡æ‹Ÿå›¾åƒæ•°æ®
    np.random.seed(42)
    batch_size = 32
    channels = 3
    height = 64
    width = 64
    num_classes = 10
    
    # æ¨¡æ‹Ÿå›¾åƒæ•°æ®
    X = np.random.randn(batch_size, channels, height, width)
    y = np.random.randint(0, num_classes, batch_size)
    
    # è½¬æ¢ä¸ºPyTorchå¼ é‡
    X_tensor = torch.FloatTensor(X)
    y_tensor = torch.LongTensor(y)
    
    # åˆ›å»ºCNNæ¨¡å‹
    model = nn.Sequential(
        # ç¬¬ä¸€ä¸ªå·ç§¯å—
        nn.Conv2d(channels, 32, kernel_size=3, padding=1),
        nn.ReLU(),
        nn.MaxPool2d(2, 2),
        
        # ç¬¬äºŒä¸ªå·ç§¯å—
        nn.Conv2d(32, 64, kernel_size=3, padding=1),
        nn.ReLU(),
        nn.MaxPool2d(2, 2),
        
        # ç¬¬ä¸‰ä¸ªå·ç§¯å—
        nn.Conv2d(64, 128, kernel_size=3, padding=1),
        nn.ReLU(),
        nn.MaxPool2d(2, 2),
        
        # å…¨è¿æ¥å±‚
        nn.Flatten(),
        nn.Linear(128 * 8 * 8, 512),
        nn.ReLU(),
        nn.Dropout(0.5),
        nn.Linear(512, num_classes)
    )
    
    # è®­ç»ƒæ¨¡å‹
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    
    num_epochs = 20
    losses = []
    
    for epoch in range(num_epochs):
        # å‰å‘ä¼ æ’­
        output = model(X_tensor)
        loss = criterion(output, y_tensor)
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        losses.append(loss.item())
        
        if epoch % 5 == 0:
            print(f'Epoch {epoch}: Loss = {loss.item():.4f}')
    
    # è¯„ä¼°æ¨¡å‹
    model.eval()
    with torch.no_grad():
        output = model(X_tensor)
        _, predicted = torch.max(output.data, 1)
        accuracy = (predicted == y_tensor).sum().item() / len(y_tensor)
    
    print(f'å›¾åƒåˆ†ç±»å‡†ç¡®ç‡: {accuracy:.4f}')
    
    # å¯è§†åŒ–
    plt.figure(figsize=(10, 6))
    plt.plot(losses)
    plt.title('CNNå›¾åƒåˆ†ç±»è®­ç»ƒæŸå¤±')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.grid(True)
    plt.show()
    
    return model, accuracy

### æ¡ˆä¾‹2ï¼šåºåˆ—é¢„æµ‹

```python
def sequence_prediction_example():
    """åºåˆ—é¢„æµ‹ç¤ºä¾‹"""
    
    # ç”Ÿæˆæ¨¡æ‹Ÿåºåˆ—æ•°æ®
    np.random.seed(42)
    seq_length = 20
    batch_size = 32
    input_size = 10
    hidden_size = 64
    num_classes = 5
    
    # ç”Ÿæˆåºåˆ—æ•°æ®
    X = np.random.randn(batch_size, seq_length, input_size)
    y = np.random.randint(0, num_classes, batch_size)
    
    # è½¬æ¢ä¸ºPyTorchå¼ é‡
    X_tensor = torch.FloatTensor(X)
    y_tensor = torch.LongTensor(y)
    
    # åˆ›å»ºLSTMæ¨¡å‹
    model = nn.Sequential(
        nn.LSTM(input_size, hidden_size, num_layers=2, batch_first=True, dropout=0.2),
        nn.Linear(hidden_size, num_classes)
    )
    
    # è‡ªå®šä¹‰å‰å‘ä¼ æ’­
    class LSTMModel(nn.Module):
        def __init__(self, input_size, hidden_size, num_layers, num_classes):
            super(LSTMModel, self).__init__()
            self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=0.2)
            self.fc = nn.Linear(hidden_size, num_classes)
        
        def forward(self, x):
            lstm_out, _ = self.lstm(x)
            # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
            out = self.fc(lstm_out[:, -1, :])
            return out
    
    model = LSTMModel(input_size, hidden_size, 2, num_classes)
    
    # è®­ç»ƒæ¨¡å‹
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    
    num_epochs = 30
    losses = []
    
    for epoch in range(num_epochs):
        # å‰å‘ä¼ æ’­
        output = model(X_tensor)
        loss = criterion(output, y_tensor)
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        losses.append(loss.item())
        
        if epoch % 5 == 0:
            print(f'Epoch {epoch}: Loss = {loss.item():.4f}')
    
    # è¯„ä¼°æ¨¡å‹
    model.eval()
    with torch.no_grad():
        output = model(X_tensor)
        _, predicted = torch.max(output.data, 1)
        accuracy = (predicted == y_tensor).sum().item() / len(y_tensor)
    
    print(f'åºåˆ—é¢„æµ‹å‡†ç¡®ç‡: {accuracy:.4f}')
    
    # å¯è§†åŒ–
    plt.figure(figsize=(10, 6))
    plt.plot(losses)
    plt.title('LSTMåºåˆ—é¢„æµ‹è®­ç»ƒæŸå¤±')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.grid(True)
    plt.show()
    
    return model, accuracy

### æ¡ˆä¾‹3ï¼šæ³¨æ„åŠ›æœºåˆ¶

```python
def attention_mechanism_example():
    """æ³¨æ„åŠ›æœºåˆ¶ç¤ºä¾‹"""
    
    # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
    np.random.seed(42)
    batch_size = 16
    seq_length = 10
    d_model = 64
    n_heads = 8
    
    # ç”Ÿæˆè¾“å…¥åºåˆ—
    X = np.random.randn(batch_size, seq_length, d_model)
    X_tensor = torch.FloatTensor(X)
    
    # åˆ›å»ºå¤šå¤´æ³¨æ„åŠ›å±‚
    attention = nn.MultiheadAttention(d_model, n_heads, batch_first=True)
    
    # åº”ç”¨æ³¨æ„åŠ›æœºåˆ¶
    with torch.no_grad():
        attn_output, attn_weights = attention(X_tensor, X_tensor, X_tensor)
    
    # å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡
    plt.figure(figsize=(12, 8))
    
    # æ˜¾ç¤ºç¬¬ä¸€ä¸ªæ ·æœ¬çš„æ³¨æ„åŠ›æƒé‡
    sample_idx = 0
    head_idx = 0
    
    attention_matrix = attn_weights[sample_idx, head_idx].numpy()
    
    plt.imshow(attention_matrix, cmap='viridis', aspect='auto')
    plt.colorbar()
    plt.title(f'æ³¨æ„åŠ›æƒé‡çŸ©é˜µ (æ ·æœ¬ {sample_idx}, å¤´ {head_idx})')
    plt.xlabel('é”®ä½ç½®')
    plt.ylabel('æŸ¥è¯¢ä½ç½®')
    plt.show()
    
    # åˆ†ææ³¨æ„åŠ›æ¨¡å¼
    print("æ³¨æ„åŠ›æƒé‡ç»Ÿè®¡:")
    print(f"å¹³å‡æƒé‡: {attn_weights.mean().item():.4f}")
    print(f"æœ€å¤§æƒé‡: {attn_weights.max().item():.4f}")
    print(f"æœ€å°æƒé‡: {attn_weights.min().item():.4f}")
    
    return attention, attn_weights

# è¿è¡Œæ‰€æœ‰ç¤ºä¾‹
if __name__ == "__main__":
    print("=== æ·±åº¦å­¦ä¹ åŸºç¡€ç¤ºä¾‹ ===")
    
    print("\n1. å›¾åƒåˆ†ç±»ç¤ºä¾‹:")
    cnn_model, cnn_acc = image_classification_example()
    
    print("\n2. åºåˆ—é¢„æµ‹ç¤ºä¾‹:")
    lstm_model, lstm_acc = sequence_prediction_example()
    
    print("\n3. æ³¨æ„åŠ›æœºåˆ¶ç¤ºä¾‹:")
    attention_model, attn_weights = attention_mechanism_example()
    
    print(f"\n=== æ€»ç»“ ===")
    print(f"CNNå›¾åƒåˆ†ç±»å‡†ç¡®ç‡: {cnn_acc:.4f}")
    print(f"LSTMåºåˆ—é¢„æµ‹å‡†ç¡®ç‡: {lstm_acc:.4f}")
```

## ğŸ”— ç›¸å…³é“¾æ¥

- [04-02-01-äººå·¥æ™ºèƒ½åŸºç¡€](./04-02-01-äººå·¥æ™ºèƒ½åŸºç¡€.md)
- [04-02-02-æœºå™¨å­¦ä¹ ç®—æ³•](./04-02-02-æœºå™¨å­¦ä¹ ç®—æ³•.md)
- [02-ç†è®ºåŸºç¡€/02-01-ç®—æ³•ç†è®º/02-01-01-ç®—æ³•åŸºç¡€](../02-ç†è®ºåŸºç¡€/02-01-ç®—æ³•ç†è®º/02-01-01-ç®—æ³•åŸºç¡€.md)
- [01-å½¢å¼ç§‘å­¦/01-01-æ•°å­¦åŸºç¡€/01-01-02-æ•°è®ºåŸºç¡€](../01-å½¢å¼ç§‘å­¦/01-01-æ•°å­¦åŸºç¡€/01-01-02-æ•°è®ºåŸºç¡€.md)

---

**æ–‡æ¡£ç‰ˆæœ¬**ï¼š1.0  
**æœ€åæ›´æ–°**ï¼š2024å¹´  
**ç»´æŠ¤è€…**ï¼šAIåŠ©æ‰‹
