# 04-02-01 äººå·¥æ™ºèƒ½åŸºç¡€

## ğŸ“‹ ç›®å½•

- [04-02-01 äººå·¥æ™ºèƒ½åŸºç¡€](#04-02-01-äººå·¥æ™ºèƒ½åŸºç¡€)
  - [ğŸ“‹ ç›®å½•](#-ç›®å½•)
  - [ğŸ¯ æ¦‚è¿°](#-æ¦‚è¿°)
  - [ğŸ”¬ æ¦‚å¿µå®šä¹‰](#-æ¦‚å¿µå®šä¹‰)
  - [ğŸ“ æ•°å­¦å½¢å¼åŒ–](#-æ•°å­¦å½¢å¼åŒ–)
  - [ğŸ Pythonå®ç°](#-pythonå®ç°)
  - [ğŸ“Š æ¶æ„è®¾è®¡](#-æ¶æ„è®¾è®¡)
  - [ğŸ”„ å·¥ä½œæµç¨‹](#-å·¥ä½œæµç¨‹)
  - [ğŸ“ˆ åº”ç”¨æ¡ˆä¾‹](#-åº”ç”¨æ¡ˆä¾‹)
  - [ğŸ”— ç›¸å…³é“¾æ¥](#-ç›¸å…³é“¾æ¥)

## ğŸ¯ æ¦‚è¿°

äººå·¥æ™ºèƒ½ï¼ˆArtificial Intelligence, AIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œæ—¨åœ¨åˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚æœ¬æ–‡æ¡£ä»å½¢å¼åŒ–å®šä¹‰ã€æ•°å­¦åŸºç¡€ã€Pythonå®ç°ç­‰å¤šä¸ªç»´åº¦å…¨é¢é˜è¿°äººå·¥æ™ºèƒ½çš„åŸºç¡€ç†è®ºã€‚

## ğŸ”¬ æ¦‚å¿µå®šä¹‰

### å®šä¹‰ 1.1 (äººå·¥æ™ºèƒ½)
äººå·¥æ™ºèƒ½æ˜¯ç ”ç©¶å¦‚ä½•ä½¿è®¡ç®—æœºç³»ç»Ÿèƒ½å¤Ÿæ¨¡æ‹Ÿã€å»¶ä¼¸å’Œæ‰©å±•äººç±»æ™ºèƒ½çš„ç§‘å­¦ä¸æŠ€æœ¯ã€‚å½¢å¼åŒ–å®šä¹‰ä¸ºï¼š

$$AI = (S, A, T, R, \gamma)$$

å…¶ä¸­ï¼š
- $S$ æ˜¯çŠ¶æ€ç©ºé—´
- $A$ æ˜¯åŠ¨ä½œç©ºé—´  
- $T: S \times A \rightarrow S$ æ˜¯çŠ¶æ€è½¬ç§»å‡½æ•°
- $R: S \times A \rightarrow \mathbb{R}$ æ˜¯å¥–åŠ±å‡½æ•°
- $\gamma \in [0,1]$ æ˜¯æŠ˜æ‰£å› å­

### å®šä¹‰ 1.2 (æ™ºèƒ½ç³»ç»Ÿ)
æ™ºèƒ½ç³»ç»Ÿæ˜¯ä¸€ä¸ªèƒ½å¤Ÿæ„ŸçŸ¥ç¯å¢ƒã€å­¦ä¹ ã€æ¨ç†ã€è§„åˆ’å’Œè¡ŒåŠ¨çš„è‡ªä¸»ç³»ç»Ÿï¼š

$$IS = (P, L, R, A, M)$$

å…¶ä¸­ï¼š
- $P$ æ˜¯æ„ŸçŸ¥æ¨¡å—
- $L$ æ˜¯å­¦ä¹ æ¨¡å—
- $R$ æ˜¯æ¨ç†æ¨¡å—
- $A$ æ˜¯è¡ŒåŠ¨æ¨¡å—
- $M$ æ˜¯è®°å¿†æ¨¡å—

## ğŸ“ æ•°å­¦å½¢å¼åŒ–

### å®šç† 1.1 (å›¾çµå®Œå¤‡æ€§)
ä»»ä½•å¯è®¡ç®—çš„å‡½æ•°éƒ½å¯ä»¥è¢«å›¾çµæœºè®¡ç®—ï¼Œäººå·¥æ™ºèƒ½ç³»ç»Ÿåœ¨ç†è®ºä¸Šå¯ä»¥è¾¾åˆ°å›¾çµå®Œå¤‡æ€§ã€‚

**è¯æ˜**ï¼š
è®¾ $f: \mathbb{N} \rightarrow \mathbb{N}$ æ˜¯å¯è®¡ç®—å‡½æ•°ï¼Œå­˜åœ¨å›¾çµæœº $M$ ä½¿å¾—ï¼š
$$\forall n \in \mathbb{N}, M(n) = f(n)$$

### å®šç† 1.2 (å­¦ä¹ ç†è®º)
ç»™å®šè®­ç»ƒé›† $D = \{(x_i, y_i)\}_{i=1}^n$ï¼Œå­¦ä¹ ç®—æ³•çš„æ³›åŒ–è¯¯å·®ä¸Šç•Œä¸ºï¼š

$$R(f) \leq \hat{R}(f) + \sqrt{\frac{\log(|\mathcal{F}|) + \log(1/\delta)}{2n}}$$

å…¶ä¸­ï¼š
- $R(f)$ æ˜¯çœŸå®é£é™©
- $\hat{R}(f)$ æ˜¯ç»éªŒé£é™©
- $|\mathcal{F}|$ æ˜¯å‡è®¾ç©ºé—´å¤§å°
- $\delta$ æ˜¯ç½®ä¿¡åº¦å‚æ•°

### å®šä¹‰ 1.3 (ç¥ç»ç½‘ç»œ)
äººå·¥ç¥ç»ç½‘ç»œæ˜¯ä¸€ä¸ªç”±ç¥ç»å…ƒç»„æˆçš„è®¡ç®—æ¨¡å‹ï¼š

$$y = \sigma(W^T x + b)$$

å…¶ä¸­ï¼š
- $x \in \mathbb{R}^n$ æ˜¯è¾“å…¥å‘é‡
- $W \in \mathbb{R}^{n \times m}$ æ˜¯æƒé‡çŸ©é˜µ
- $b \in \mathbb{R}^m$ æ˜¯åç½®å‘é‡
- $\sigma$ æ˜¯æ¿€æ´»å‡½æ•°

## ğŸ Pythonå®ç°

### 1. åŸºç¡€ç¥ç»ç½‘ç»œå®ç°

```python
import numpy as np
import matplotlib.pyplot as plt
from typing import List, Tuple, Optional
import torch
import torch.nn as nn
import torch.optim as optim

class ArtificialIntelligence:
    """äººå·¥æ™ºèƒ½åŸºç¡€ç±»"""
    
    def __init__(self, name: str, capabilities: List[str]):
        self.name = name
        self.capabilities = capabilities
        self.knowledge_base = {}
        self.learning_history = []
    
    def perceive(self, environment: dict) -> dict:
        """æ„ŸçŸ¥ç¯å¢ƒ"""
        perception = {
            'timestamp': environment.get('timestamp'),
            'sensors': environment.get('sensors', {}),
            'context': environment.get('context', {})
        }
        return perception
    
    def learn(self, data: np.ndarray, labels: np.ndarray) -> float:
        """å­¦ä¹ è¿‡ç¨‹"""
        # ç®€å•çš„çº¿æ€§å­¦ä¹ æ¨¡å‹
        X = np.column_stack([data, np.ones(len(data))])
        weights = np.linalg.lstsq(X, labels, rcond=None)[0]
        
        # è®¡ç®—å­¦ä¹ è¯¯å·®
        predictions = X @ weights
        error = np.mean((predictions - labels) ** 2)
        
        self.learning_history.append(error)
        return error
    
    def reason(self, problem: dict) -> dict:
        """æ¨ç†è¿‡ç¨‹"""
        # åŸºäºè§„åˆ™çš„æ¨ç†
        rules = self.knowledge_base.get('rules', [])
        conclusion = self._apply_rules(problem, rules)
        return {'conclusion': conclusion, 'confidence': 0.8}
    
    def act(self, action_plan: dict) -> dict:
        """æ‰§è¡Œè¡ŒåŠ¨"""
        result = {
            'action': action_plan.get('action'),
            'status': 'executed',
            'timestamp': action_plan.get('timestamp')
        }
        return result
    
    def _apply_rules(self, problem: dict, rules: List[dict]) -> str:
        """åº”ç”¨æ¨ç†è§„åˆ™"""
        for rule in rules:
            if self._match_condition(problem, rule['condition']):
                return rule['conclusion']
        return "no_conclusion"

class NeuralNetwork(nn.Module):
    """ç¥ç»ç½‘ç»œå®ç°"""
    
    def __init__(self, input_size: int, hidden_size: int, output_size: int):
        super(NeuralNetwork, self).__init__()
        self.layer1 = nn.Linear(input_size, hidden_size)
        self.layer2 = nn.Linear(hidden_size, hidden_size)
        self.layer3 = nn.Linear(hidden_size, output_size)
        self.activation = nn.ReLU()
        self.dropout = nn.Dropout(0.2)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­"""
        x = self.activation(self.layer1(x))
        x = self.dropout(x)
        x = self.activation(self.layer2(x))
        x = self.dropout(x)
        x = self.layer3(x)
        return x

class AISystem:
    """äººå·¥æ™ºèƒ½ç³»ç»Ÿ"""
    
    def __init__(self, config: dict):
        self.config = config
        self.neural_network = NeuralNetwork(
            input_size=config['input_size'],
            hidden_size=config['hidden_size'],
            output_size=config['output_size']
        )
        self.optimizer = optim.Adam(self.neural_network.parameters())
        self.criterion = nn.CrossEntropyLoss()
        self.ai = ArtificialIntelligence("AI_System", ["learning", "reasoning"])
    
    def train(self, data_loader: torch.utils.data.DataLoader, epochs: int) -> List[float]:
        """è®­ç»ƒæ¨¡å‹"""
        losses = []
        
        for epoch in range(epochs):
            epoch_loss = 0.0
            for batch_idx, (data, target) in enumerate(data_loader):
                self.optimizer.zero_grad()
                output = self.neural_network(data)
                loss = self.criterion(output, target)
                loss.backward()
                self.optimizer.step()
                epoch_loss += loss.item()
            
            avg_loss = epoch_loss / len(data_loader)
            losses.append(avg_loss)
            
            if epoch % 10 == 0:
                print(f'Epoch {epoch}: Loss = {avg_loss:.4f}')
        
        return losses
    
    def predict(self, input_data: torch.Tensor) -> torch.Tensor:
        """é¢„æµ‹"""
        self.neural_network.eval()
        with torch.no_grad():
            return self.neural_network(input_data)
    
    def evaluate(self, test_loader: torch.utils.data.DataLoader) -> dict:
        """è¯„ä¼°æ¨¡å‹"""
        self.neural_network.eval()
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, target in test_loader:
                output = self.neural_network(data)
                pred = output.argmax(dim=1, keepdim=True)
                correct += pred.eq(target.view_as(pred)).sum().item()
                total += target.size(0)
        
        accuracy = correct / total
        return {'accuracy': accuracy, 'correct': correct, 'total': total}

# ä½¿ç”¨ç¤ºä¾‹
def demonstrate_ai_basics():
    """æ¼”ç¤ºäººå·¥æ™ºèƒ½åŸºç¡€åŠŸèƒ½"""
    
    # 1. åˆ›å»ºAIç³»ç»Ÿ
    config = {
        'input_size': 10,
        'hidden_size': 64,
        'output_size': 3
    }
    ai_system = AISystem(config)
    
    # 2. ç”Ÿæˆç¤ºä¾‹æ•°æ®
    np.random.seed(42)
    X = np.random.randn(1000, 10)
    y = np.random.randint(0, 3, 1000)
    
    # è½¬æ¢ä¸ºPyTorchå¼ é‡
    X_tensor = torch.FloatTensor(X)
    y_tensor = torch.LongTensor(y)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    dataset = torch.utils.data.TensorDataset(X_tensor, y_tensor)
    data_loader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)
    
    # 3. è®­ç»ƒæ¨¡å‹
    print("å¼€å§‹è®­ç»ƒAIæ¨¡å‹...")
    losses = ai_system.train(data_loader, epochs=50)
    
    # 4. è¯„ä¼°æ¨¡å‹
    test_loader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=False)
    evaluation = ai_system.evaluate(test_loader)
    
    print(f"æ¨¡å‹å‡†ç¡®ç‡: {evaluation['accuracy']:.4f}")
    
    # 5. å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹
    plt.figure(figsize=(10, 6))
    plt.plot(losses)
    plt.title('AIæ¨¡å‹è®­ç»ƒæŸå¤±')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.grid(True)
    plt.show()
    
    return ai_system, losses, evaluation

if __name__ == "__main__":
    ai_system, losses, evaluation = demonstrate_ai_basics()
```

### 2. æ™ºèƒ½ä»£ç†å®ç°

```python
class IntelligentAgent:
    """æ™ºèƒ½ä»£ç†"""
    
    def __init__(self, agent_id: str, capabilities: List[str]):
        self.agent_id = agent_id
        self.capabilities = capabilities
        self.memory = {}
        self.beliefs = set()
        self.goals = []
        self.plans = []
    
    def observe(self, environment: dict) -> dict:
        """è§‚å¯Ÿç¯å¢ƒ"""
        observation = {
            'agent_id': self.agent_id,
            'timestamp': environment.get('timestamp'),
            'perceptions': environment.get('perceptions', {}),
            'beliefs_updated': []
        }
        
        # æ›´æ–°ä¿¡å¿µ
        for perception in observation['perceptions'].values():
            if self._validate_perception(perception):
                self.beliefs.add(perception)
                observation['beliefs_updated'].append(perception)
        
        return observation
    
    def plan(self, goal: str) -> List[str]:
        """åˆ¶å®šè®¡åˆ’"""
        # ç®€å•çš„è§„åˆ’ç®—æ³•
        plan = []
        
        if goal in self.goals:
            # æ£€æŸ¥å½“å‰çŠ¶æ€ä¸ç›®æ ‡çš„å·®è·
            current_state = self._get_current_state()
            target_state = self._get_target_state(goal)
            
            # ç”Ÿæˆè¡ŒåŠ¨è®¡åˆ’
            plan = self._generate_action_sequence(current_state, target_state)
        
        return plan
    
    def execute(self, action: str) -> dict:
        """æ‰§è¡Œè¡ŒåŠ¨"""
        result = {
            'agent_id': self.agent_id,
            'action': action,
            'status': 'executed',
            'timestamp': time.time(),
            'effects': {}
        }
        
        # æ‰§è¡Œå…·ä½“è¡ŒåŠ¨
        if action in self.capabilities:
            result['effects'] = self._perform_action(action)
            result['status'] = 'success'
        else:
            result['status'] = 'failed'
            result['effects'] = {'error': 'Action not supported'}
        
        return result
    
    def learn(self, experience: dict) -> None:
        """ä»ç»éªŒä¸­å­¦ä¹ """
        # æ›´æ–°è®°å¿†
        self.memory[experience['timestamp']] = experience
        
        # æ›´æ–°ä¿¡å¿µ
        if experience['status'] == 'success':
            self.beliefs.add(f"action_{experience['action']}_works")
        else:
            self.beliefs.add(f"action_{experience['action']}_fails")
    
    def _validate_perception(self, perception: str) -> bool:
        """éªŒè¯æ„ŸçŸ¥çš„æœ‰æ•ˆæ€§"""
        # ç®€å•çš„éªŒè¯é€»è¾‘
        return len(perception) > 0 and perception not in self.beliefs
    
    def _get_current_state(self) -> dict:
        """è·å–å½“å‰çŠ¶æ€"""
        return {
            'beliefs': list(self.beliefs),
            'goals': self.goals.copy(),
            'memory_size': len(self.memory)
        }
    
    def _get_target_state(self, goal: str) -> dict:
        """è·å–ç›®æ ‡çŠ¶æ€"""
        return {'goal_achieved': goal}
    
    def _generate_action_sequence(self, current: dict, target: dict) -> List[str]:
        """ç”Ÿæˆè¡ŒåŠ¨åºåˆ—"""
        # ç®€å•çš„è¡ŒåŠ¨ç”Ÿæˆé€»è¾‘
        actions = []
        
        if 'goal_achieved' in target:
            goal = target['goal_achieved']
            if goal not in current['goals']:
                actions.append(f"add_goal_{goal}")
            
            # æ ¹æ®ç›®æ ‡é€‰æ‹©åˆé€‚çš„è¡ŒåŠ¨
            if 'explore' in goal:
                actions.append('explore_environment')
            elif 'collect' in goal:
                actions.append('collect_data')
            elif 'analyze' in goal:
                actions.append('analyze_data')
        
        return actions
    
    def _perform_action(self, action: str) -> dict:
        """æ‰§è¡Œå…·ä½“è¡ŒåŠ¨"""
        effects = {}
        
        if action == 'explore_environment':
            effects['discovered_items'] = ['item1', 'item2', 'item3']
        elif action == 'collect_data':
            effects['data_collected'] = np.random.randn(10)
        elif action == 'analyze_data':
            effects['analysis_result'] = 'pattern_found'
        
        return effects
```

## ğŸ“Š æ¶æ„è®¾è®¡

### ç³»ç»Ÿæ¶æ„å›¾

```mermaid
graph TB
    A[æ„ŸçŸ¥æ¨¡å—] --> B[è®¤çŸ¥æ¨¡å—]
    B --> C[å†³ç­–æ¨¡å—]
    C --> D[è¡ŒåŠ¨æ¨¡å—]
    
    E[å­¦ä¹ æ¨¡å—] --> B
    F[è®°å¿†æ¨¡å—] --> B
    G[çŸ¥è¯†åº“] --> B
    
    H[ç¯å¢ƒ] --> A
    D --> H
    
    subgraph "è®¤çŸ¥æ¨¡å—"
        B1[æ¨ç†å¼•æ“]
        B2[æ¨¡å¼è¯†åˆ«]
        B3[é—®é¢˜æ±‚è§£]
    end
    
    subgraph "å­¦ä¹ æ¨¡å—"
        E1[ç›‘ç£å­¦ä¹ ]
        E2[æ— ç›‘ç£å­¦ä¹ ]
        E3[å¼ºåŒ–å­¦ä¹ ]
    end
```

### ç»„ä»¶å…³ç³»

1. **æ„ŸçŸ¥æ¨¡å—**ï¼šè´Ÿè´£æ¥æ”¶å’Œå¤„ç†ç¯å¢ƒä¿¡æ¯
2. **è®¤çŸ¥æ¨¡å—**ï¼šè¿›è¡Œæ¨ç†ã€åˆ†æå’Œå†³ç­–
3. **å­¦ä¹ æ¨¡å—**ï¼šä»ç»éªŒä¸­å­¦ä¹ å’Œæ”¹è¿›
4. **è®°å¿†æ¨¡å—**ï¼šå­˜å‚¨å’Œæ£€ç´¢ä¿¡æ¯
5. **è¡ŒåŠ¨æ¨¡å—**ï¼šæ‰§è¡Œå†³ç­–å’Œä¸ç¯å¢ƒäº¤äº’

## ğŸ”„ å·¥ä½œæµç¨‹

### æ™ºèƒ½ç³»ç»Ÿå·¥ä½œæµç¨‹

```python
def intelligent_system_workflow():
    """æ™ºèƒ½ç³»ç»Ÿå·¥ä½œæµç¨‹"""
    
    # 1. åˆå§‹åŒ–ç³»ç»Ÿ
    agent = IntelligentAgent("AI_Agent_001", ["explore", "learn", "reason"])
    
    # 2. æ„ŸçŸ¥-è®¤çŸ¥-è¡ŒåŠ¨å¾ªç¯
    for step in range(10):
        print(f"\n=== æ­¥éª¤ {step + 1} ===")
        
        # æ„ŸçŸ¥ç¯å¢ƒ
        environment = {
            'timestamp': time.time(),
            'perceptions': {
                'sensor1': f'data_{step}',
                'sensor2': f'status_{step % 3}'
            }
        }
        
        observation = agent.observe(environment)
        print(f"è§‚å¯Ÿç»“æœ: {observation['perceptions']}")
        
        # åˆ¶å®šè®¡åˆ’
        goal = f"achieve_objective_{step}"
        plan = agent.plan(goal)
        print(f"è¡ŒåŠ¨è®¡åˆ’: {plan}")
        
        # æ‰§è¡Œè¡ŒåŠ¨
        for action in plan:
            result = agent.execute(action)
            print(f"æ‰§è¡Œè¡ŒåŠ¨: {action} -> {result['status']}")
            
            # å­¦ä¹ ç»éªŒ
            agent.learn(result)
        
        # æ›´æ–°ç›®æ ‡
        agent.goals.append(goal)
    
    return agent

# è¿è¡Œå·¥ä½œæµç¨‹
if __name__ == "__main__":
    final_agent = intelligent_system_workflow()
    print(f"\næœ€ç»ˆä¿¡å¿µ: {final_agent.beliefs}")
    print(f"è®°å¿†å¤§å°: {len(final_agent.memory)}")
```

## ğŸ“ˆ åº”ç”¨æ¡ˆä¾‹

### æ¡ˆä¾‹1ï¼šæ™ºèƒ½æ¨èç³»ç»Ÿ

```python
class RecommendationSystem:
    """æ™ºèƒ½æ¨èç³»ç»Ÿ"""
    
    def __init__(self):
        self.user_profiles = {}
        self.item_features = {}
        self.interaction_matrix = None
    
    def update_user_profile(self, user_id: str, interactions: List[dict]):
        """æ›´æ–°ç”¨æˆ·ç”»åƒ"""
        if user_id not in self.user_profiles:
            self.user_profiles[user_id] = {
                'preferences': {},
                'interaction_history': [],
                'profile_vector': np.zeros(100)
            }
        
        # æ›´æ–°äº¤äº’å†å²
        self.user_profiles[user_id]['interaction_history'].extend(interactions)
        
        # æ›´æ–°åå¥½å‘é‡
        self._update_preference_vector(user_id, interactions)
    
    def recommend_items(self, user_id: str, n_recommendations: int = 5) -> List[str]:
        """æ¨èç‰©å“"""
        if user_id not in self.user_profiles:
            return []
        
        user_vector = self.user_profiles[user_id]['profile_vector']
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = []
        for item_id, item_vector in self.item_features.items():
            similarity = np.dot(user_vector, item_vector) / (
                np.linalg.norm(user_vector) * np.linalg.norm(item_vector)
            )
            similarities.append((item_id, similarity))
        
        # æ’åºå¹¶è¿”å›æ¨è
        similarities.sort(key=lambda x: x[1], reverse=True)
        return [item_id for item_id, _ in similarities[:n_recommendations]]
    
    def _update_preference_vector(self, user_id: str, interactions: List[dict]):
        """æ›´æ–°åå¥½å‘é‡"""
        profile = self.user_profiles[user_id]
        
        for interaction in interactions:
            item_id = interaction['item_id']
            rating = interaction.get('rating', 1.0)
            
            if item_id in self.item_features:
                item_vector = self.item_features[item_id]
                profile['profile_vector'] += rating * item_vector
        
        # å½’ä¸€åŒ–
        norm = np.linalg.norm(profile['profile_vector'])
        if norm > 0:
            profile['profile_vector'] /= norm
```

### æ¡ˆä¾‹2ï¼šè‡ªç„¶è¯­è¨€å¤„ç†

```python
class NaturalLanguageProcessor:
    """è‡ªç„¶è¯­è¨€å¤„ç†å™¨"""
    
    def __init__(self):
        self.vocabulary = {}
        self.word_embeddings = {}
        self.language_model = None
    
    def tokenize(self, text: str) -> List[str]:
        """åˆ†è¯"""
        # ç®€å•çš„åˆ†è¯å®ç°
        tokens = text.lower().split()
        return [token.strip('.,!?;:') for token in tokens if token.strip()]
    
    def extract_features(self, text: str) -> np.ndarray:
        """æå–ç‰¹å¾"""
        tokens = self.tokenize(text)
        feature_vector = np.zeros(len(self.vocabulary))
        
        for token in tokens:
            if token in self.vocabulary:
                feature_vector[self.vocabulary[token]] += 1
        
        return feature_vector
    
    def classify_sentiment(self, text: str) -> dict:
        """æƒ…æ„Ÿåˆ†æ"""
        features = self.extract_features(text)
        
        # ç®€å•çš„æƒ…æ„Ÿåˆ†æè§„åˆ™
        positive_words = ['good', 'great', 'excellent', 'amazing', 'wonderful']
        negative_words = ['bad', 'terrible', 'awful', 'horrible', 'disappointing']
        
        tokens = self.tokenize(text)
        positive_count = sum(1 for token in tokens if token in positive_words)
        negative_count = sum(1 for token in tokens if token in negative_words)
        
        if positive_count > negative_count:
            sentiment = 'positive'
            confidence = positive_count / (positive_count + negative_count + 1)
        elif negative_count > positive_count:
            sentiment = 'negative'
            confidence = negative_count / (positive_count + negative_count + 1)
        else:
            sentiment = 'neutral'
            confidence = 0.5
        
        return {
            'sentiment': sentiment,
            'confidence': confidence,
            'positive_score': positive_count,
            'negative_score': negative_count
        }
```

## ğŸ”— ç›¸å…³é“¾æ¥

- [04-02-02-æœºå™¨å­¦ä¹ ç®—æ³•](./04-02-02-æœºå™¨å­¦ä¹ ç®—æ³•.md)
- [04-02-03-æ·±åº¦å­¦ä¹ åŸºç¡€](./04-02-03-æ·±åº¦å­¦ä¹ åŸºç¡€.md)
- [02-ç†è®ºåŸºç¡€/02-01-ç®—æ³•ç†è®º/02-01-01-ç®—æ³•åŸºç¡€](../02-ç†è®ºåŸºç¡€/02-01-ç®—æ³•ç†è®º/02-01-01-ç®—æ³•åŸºç¡€.md)
- [01-å½¢å¼ç§‘å­¦/01-01-æ•°å­¦åŸºç¡€/01-01-01-é›†åˆè®ºåŸºç¡€](../01-å½¢å¼ç§‘å­¦/01-01-æ•°å­¦åŸºç¡€/01-01-01-é›†åˆè®ºåŸºç¡€.md)

---

**æ–‡æ¡£ç‰ˆæœ¬**ï¼š1.0  
**æœ€åæ›´æ–°**ï¼š2024å¹´  
**ç»´æŠ¤è€…**ï¼šAIåŠ©æ‰‹
