# 04-02-02 æœºå™¨å­¦ä¹ ç®—æ³•

## ğŸ“‹ ç›®å½•

- [04-02-02 æœºå™¨å­¦ä¹ ç®—æ³•](#04-02-02-æœºå™¨å­¦ä¹ ç®—æ³•)
  - [ğŸ“‹ ç›®å½•](#-ç›®å½•)
  - [ğŸ¯ æ¦‚è¿°](#-æ¦‚è¿°)
  - [ğŸ”¬ æ¦‚å¿µå®šä¹‰](#-æ¦‚å¿µå®šä¹‰)
  - [ğŸ“ æ•°å­¦å½¢å¼åŒ–](#-æ•°å­¦å½¢å¼åŒ–)
  - [ğŸ Pythonå®ç°](#-pythonå®ç°)
  - [ğŸ“Š ç®—æ³•æ¯”è¾ƒ](#-ç®—æ³•æ¯”è¾ƒ)
  - [ğŸ”„ å·¥ä½œæµç¨‹](#-å·¥ä½œæµç¨‹)
  - [ğŸ“ˆ åº”ç”¨æ¡ˆä¾‹](#-åº”ç”¨æ¡ˆä¾‹)
  - [ğŸ”— ç›¸å…³é“¾æ¥](#-ç›¸å…³é“¾æ¥)

## ğŸ¯ æ¦‚è¿°

æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒåˆ†æ”¯ï¼Œé€šè¿‡ç®—æ³•ä½¿è®¡ç®—æœºç³»ç»Ÿèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ å¹¶åšå‡ºé¢„æµ‹æˆ–å†³ç­–ã€‚æœ¬æ–‡æ¡£æ¶µç›–ç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ çš„ä¸»è¦ç®—æ³•ã€‚

## ğŸ”¬ æ¦‚å¿µå®šä¹‰

### å®šä¹‰ 2.1 (æœºå™¨å­¦ä¹ )
æœºå™¨å­¦ä¹ æ˜¯ç ”ç©¶è®¡ç®—æœºç®—æ³•å¦‚ä½•è‡ªåŠ¨æ”¹è¿›ç»éªŒçš„è¿‡ç¨‹ã€‚å½¢å¼åŒ–å®šä¹‰ä¸ºï¼š

$$ML = (D, H, L, A)$$

å…¶ä¸­ï¼š
- $D$ æ˜¯æ•°æ®é›†
- $H$ æ˜¯å‡è®¾ç©ºé—´
- $L$ æ˜¯å­¦ä¹ ç®—æ³•
- $A$ æ˜¯è¯„ä¼°æŒ‡æ ‡

### å®šä¹‰ 2.2 (ç›‘ç£å­¦ä¹ )
ç»™å®šè®­ç»ƒé›† $D = \{(x_i, y_i)\}_{i=1}^n$ï¼Œå­¦ä¹ å‡½æ•° $f: X \rightarrow Y$ ä½¿å¾— $f(x_i) \approx y_i$ã€‚

### å®šä¹‰ 2.3 (æ— ç›‘ç£å­¦ä¹ )
ç»™å®šæ•°æ®é›† $D = \{x_i\}_{i=1}^n$ï¼Œå‘ç°æ•°æ®ä¸­çš„éšè—æ¨¡å¼æˆ–ç»“æ„ã€‚

### å®šä¹‰ 2.4 (å¼ºåŒ–å­¦ä¹ )
æ™ºèƒ½ä½“é€šè¿‡ä¸ç¯å¢ƒäº¤äº’å­¦ä¹ æœ€ä¼˜ç­–ç•¥ $\pi^*: S \rightarrow A$ ä»¥æœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ±ã€‚

## ğŸ“ æ•°å­¦å½¢å¼åŒ–

### å®šç† 2.1 (VCç»´ç†è®º)
å¯¹äºå‡è®¾ç©ºé—´ $\mathcal{H}$ï¼Œå…¶VCç»´ä¸º $d$ï¼Œåˆ™æ³›åŒ–è¯¯å·®ä¸Šç•Œä¸ºï¼š

$$R(f) \leq \hat{R}(f) + \sqrt{\frac{d(\log(2n/d) + 1) + \log(4/\delta)}{n}}$$

### å®šç† 2.2 (è´å¶æ–¯å®šç†)
$$P(h|D) = \frac{P(D|h)P(h)}{P(D)}$$

å…¶ä¸­ï¼š
- $P(h|D)$ æ˜¯åéªŒæ¦‚ç‡
- $P(D|h)$ æ˜¯ä¼¼ç„¶å‡½æ•°
- $P(h)$ æ˜¯å…ˆéªŒæ¦‚ç‡
- $P(D)$ æ˜¯è¯æ®

### å®šä¹‰ 2.5 (æŸå¤±å‡½æ•°)
å¯¹äºé¢„æµ‹ $\hat{y}$ å’ŒçœŸå®å€¼ $y$ï¼ŒæŸå¤±å‡½æ•°å®šä¹‰ä¸ºï¼š

$$L(\hat{y}, y) = \frac{1}{2}(\hat{y} - y)^2 \quad \text{(å‡æ–¹è¯¯å·®)}$$

## ğŸ Pythonå®ç°

### 1. ç›‘ç£å­¦ä¹ ç®—æ³•

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, mean_squared_error
from sklearn.preprocessing import StandardScaler
import torch
import torch.nn as nn
import torch.optim as optim

class LinearRegression:
    """çº¿æ€§å›å½’ç®—æ³•"""
    
    def __init__(self, learning_rate: float = 0.01, max_iterations: int = 1000):
        self.learning_rate = learning_rate
        self.max_iterations = max_iterations
        self.weights = None
        self.bias = None
        self.cost_history = []
    
    def fit(self, X: np.ndarray, y: np.ndarray) -> 'LinearRegression':
        """è®­ç»ƒæ¨¡å‹"""
        n_samples, n_features = X.shape
        
        # åˆå§‹åŒ–å‚æ•°
        self.weights = np.zeros(n_features)
        self.bias = 0
        
        # æ¢¯åº¦ä¸‹é™
        for i in range(self.max_iterations):
            # å‰å‘ä¼ æ’­
            y_pred = self._predict(X)
            
            # è®¡ç®—æ¢¯åº¦
            dw = (1/n_samples) * np.dot(X.T, (y_pred - y))
            db = (1/n_samples) * np.sum(y_pred - y)
            
            # æ›´æ–°å‚æ•°
            self.weights -= self.learning_rate * dw
            self.bias -= self.learning_rate * db
            
            # è®°å½•æˆæœ¬
            cost = self._compute_cost(y_pred, y)
            self.cost_history.append(cost)
        
        return self
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """é¢„æµ‹"""
        return self._predict(X)
    
    def _predict(self, X: np.ndarray) -> np.ndarray:
        """å†…éƒ¨é¢„æµ‹æ–¹æ³•"""
        return np.dot(X, self.weights) + self.bias
    
    def _compute_cost(self, y_pred: np.ndarray, y_true: np.ndarray) -> float:
        """è®¡ç®—æˆæœ¬"""
        return np.mean((y_pred - y_true) ** 2)

class LogisticRegression:
    """é€»è¾‘å›å½’ç®—æ³•"""
    
    def __init__(self, learning_rate: float = 0.01, max_iterations: int = 1000):
        self.learning_rate = learning_rate
        self.max_iterations = max_iterations
        self.weights = None
        self.bias = None
        self.cost_history = []
    
    def fit(self, X: np.ndarray, y: np.ndarray) -> 'LogisticRegression':
        """è®­ç»ƒæ¨¡å‹"""
        n_samples, n_features = X.shape
        
        # åˆå§‹åŒ–å‚æ•°
        self.weights = np.zeros(n_features)
        self.bias = 0
        
        # æ¢¯åº¦ä¸‹é™
        for i in range(self.max_iterations):
            # å‰å‘ä¼ æ’­
            y_pred = self._sigmoid(self._predict(X))
            
            # è®¡ç®—æ¢¯åº¦
            dw = (1/n_samples) * np.dot(X.T, (y_pred - y))
            db = (1/n_samples) * np.sum(y_pred - y)
            
            # æ›´æ–°å‚æ•°
            self.weights -= self.learning_rate * dw
            self.bias -= self.learning_rate * db
            
            # è®°å½•æˆæœ¬
            cost = self._compute_cost(y_pred, y)
            self.cost_history.append(cost)
        
        return self
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """é¢„æµ‹"""
        y_pred = self._sigmoid(self._predict(X))
        return (y_pred >= 0.5).astype(int)
    
    def predict_proba(self, X: np.ndarray) -> np.ndarray:
        """é¢„æµ‹æ¦‚ç‡"""
        return self._sigmoid(self._predict(X))
    
    def _predict(self, X: np.ndarray) -> np.ndarray:
        """å†…éƒ¨é¢„æµ‹æ–¹æ³•"""
        return np.dot(X, self.weights) + self.bias
    
    def _sigmoid(self, z: np.ndarray) -> np.ndarray:
        """Sigmoidæ¿€æ´»å‡½æ•°"""
        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))
    
    def _compute_cost(self, y_pred: np.ndarray, y_true: np.ndarray) -> float:
        """è®¡ç®—æˆæœ¬ï¼ˆäº¤å‰ç†µï¼‰"""
        epsilon = 1e-15
        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))

class DecisionTree:
    """å†³ç­–æ ‘ç®—æ³•"""
    
    def __init__(self, max_depth: int = 5, min_samples_split: int = 2):
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.root = None
    
    def fit(self, X: np.ndarray, y: np.ndarray) -> 'DecisionTree':
        """è®­ç»ƒæ¨¡å‹"""
        self.root = self._build_tree(X, y, depth=0)
        return self
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """é¢„æµ‹"""
        return np.array([self._predict_single(x, self.root) for x in X])
    
    def _build_tree(self, X: np.ndarray, y: np.ndarray, depth: int) -> dict:
        """æ„å»ºå†³ç­–æ ‘"""
        n_samples, n_features = X.shape
        n_classes = len(np.unique(y))
        
        # åœæ­¢æ¡ä»¶
        if (depth >= self.max_depth or 
            n_samples < self.min_samples_split or 
            n_classes == 1):
            return {'type': 'leaf', 'value': self._most_common(y)}
        
        # å¯»æ‰¾æœ€ä½³åˆ†å‰²
        best_feature, best_threshold = self._find_best_split(X, y)
        
        if best_feature is None:
            return {'type': 'leaf', 'value': self._most_common(y)}
        
        # åˆ†å‰²æ•°æ®
        left_mask = X[:, best_feature] <= best_threshold
        right_mask = ~left_mask
        
        left_tree = self._build_tree(X[left_mask], y[left_mask], depth + 1)
        right_tree = self._build_tree(X[right_mask], y[right_mask], depth + 1)
        
        return {
            'type': 'node',
            'feature': best_feature,
            'threshold': best_threshold,
            'left': left_tree,
            'right': right_tree
        }
    
    def _find_best_split(self, X: np.ndarray, y: np.ndarray) -> tuple:
        """å¯»æ‰¾æœ€ä½³åˆ†å‰²ç‚¹"""
        best_gain = -1
        best_feature = None
        best_threshold = None
        
        n_samples, n_features = X.shape
        
        for feature in range(n_features):
            thresholds = np.unique(X[:, feature])
            
            for threshold in thresholds:
                left_mask = X[:, feature] <= threshold
                right_mask = ~left_mask
                
                if np.sum(left_mask) == 0 or np.sum(right_mask) == 0:
                    continue
                
                gain = self._information_gain(y, y[left_mask], y[right_mask])
                
                if gain > best_gain:
                    best_gain = gain
                    best_feature = feature
                    best_threshold = threshold
        
        return best_feature, best_threshold
    
    def _information_gain(self, parent: np.ndarray, left: np.ndarray, right: np.ndarray) -> float:
        """è®¡ç®—ä¿¡æ¯å¢ç›Š"""
        parent_entropy = self._entropy(parent)
        left_entropy = self._entropy(left)
        right_entropy = self._entropy(right)
        
        n_left = len(left)
        n_right = len(right)
        n_parent = len(parent)
        
        weighted_entropy = (n_left / n_parent) * left_entropy + (n_right / n_parent) * right_entropy
        
        return parent_entropy - weighted_entropy
    
    def _entropy(self, y: np.ndarray) -> float:
        """è®¡ç®—ç†µ"""
        _, counts = np.unique(y, return_counts=True)
        probabilities = counts / len(y)
        return -np.sum(probabilities * np.log2(probabilities + 1e-10))
    
    def _most_common(self, y: np.ndarray):
        """è¿”å›æœ€å¸¸è§çš„ç±»åˆ«"""
        values, counts = np.unique(y, return_counts=True)
        return values[np.argmax(counts)]
    
    def _predict_single(self, x: np.ndarray, node: dict):
        """é¢„æµ‹å•ä¸ªæ ·æœ¬"""
        if node['type'] == 'leaf':
            return node['value']
        
        if x[node['feature']] <= node['threshold']:
            return self._predict_single(x, node['left'])
        else:
            return self._predict_single(x, node['right'])

class RandomForest:
    """éšæœºæ£®æ—ç®—æ³•"""
    
    def __init__(self, n_estimators: int = 10, max_depth: int = 5, min_samples_split: int = 2):
        self.n_estimators = n_estimators
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.trees = []
    
    def fit(self, X: np.ndarray, y: np.ndarray) -> 'RandomForest':
        """è®­ç»ƒæ¨¡å‹"""
        self.trees = []
        
        for _ in range(self.n_estimators):
            # éšæœºé‡‡æ ·
            indices = np.random.choice(len(X), size=len(X), replace=True)
            X_bootstrap = X[indices]
            y_bootstrap = y[indices]
            
            # è®­ç»ƒå†³ç­–æ ‘
            tree = DecisionTree(max_depth=self.max_depth, min_samples_split=self.min_samples_split)
            tree.fit(X_bootstrap, y_bootstrap)
            self.trees.append(tree)
        
        return self
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """é¢„æµ‹"""
        predictions = np.array([tree.predict(X) for tree in self.trees])
        return np.array([self._most_common(pred) for pred in predictions.T])
    
    def _most_common(self, predictions: np.ndarray):
        """è¿”å›æœ€å¸¸è§çš„é¢„æµ‹"""
        values, counts = np.unique(predictions, return_counts=True)
        return values[np.argmax(counts)]
```

### 2. æ— ç›‘ç£å­¦ä¹ ç®—æ³•

```python
class KMeans:
    """K-meansèšç±»ç®—æ³•"""
    
    def __init__(self, n_clusters: int = 3, max_iterations: int = 100):
        self.n_clusters = n_clusters
        self.max_iterations = max_iterations
        self.centroids = None
        self.labels = None
    
    def fit(self, X: np.ndarray) -> 'KMeans':
        """è®­ç»ƒæ¨¡å‹"""
        n_samples, n_features = X.shape
        
        # éšæœºåˆå§‹åŒ–èšç±»ä¸­å¿ƒ
        indices = np.random.choice(n_samples, self.n_clusters, replace=False)
        self.centroids = X[indices]
        
        for _ in range(self.max_iterations):
            # åˆ†é…æ ·æœ¬åˆ°æœ€è¿‘çš„èšç±»ä¸­å¿ƒ
            self.labels = self._assign_clusters(X)
            
            # æ›´æ–°èšç±»ä¸­å¿ƒ
            new_centroids = np.array([X[self.labels == k].mean(axis=0) 
                                     for k in range(self.n_clusters)])
            
            # æ£€æŸ¥æ”¶æ•›
            if np.allclose(self.centroids, new_centroids):
                break
            
            self.centroids = new_centroids
        
        return self
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """é¢„æµ‹èšç±»æ ‡ç­¾"""
        return self._assign_clusters(X)
    
    def _assign_clusters(self, X: np.ndarray) -> np.ndarray:
        """åˆ†é…æ ·æœ¬åˆ°èšç±»"""
        distances = np.sqrt(((X - self.centroids[:, np.newaxis]) ** 2).sum(axis=2))
        return np.argmin(distances, axis=0)

class PrincipalComponentAnalysis:
    """ä¸»æˆåˆ†åˆ†æç®—æ³•"""
    
    def __init__(self, n_components: int = 2):
        self.n_components = n_components
        self.components = None
        self.mean = None
        self.explained_variance_ratio = None
    
    def fit(self, X: np.ndarray) -> 'PrincipalComponentAnalysis':
        """è®­ç»ƒæ¨¡å‹"""
        # ä¸­å¿ƒåŒ–æ•°æ®
        self.mean = np.mean(X, axis=0)
        X_centered = X - self.mean
        
        # è®¡ç®—åæ–¹å·®çŸ©é˜µ
        cov_matrix = np.cov(X_centered.T)
        
        # ç‰¹å¾å€¼åˆ†è§£
        eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)
        
        # æ’åºå¹¶é€‰æ‹©å‰n_componentsä¸ªä¸»æˆåˆ†
        indices = np.argsort(eigenvalues)[::-1]
        self.components = eigenvectors[:, indices[:self.n_components]]
        
        # è®¡ç®—è§£é‡Šæ–¹å·®æ¯”
        total_variance = np.sum(eigenvalues)
        self.explained_variance_ratio = eigenvalues[indices[:self.n_components]] / total_variance
        
        return self
    
    def transform(self, X: np.ndarray) -> np.ndarray:
        """è½¬æ¢æ•°æ®"""
        X_centered = X - self.mean
        return np.dot(X_centered, self.components)
    
    def fit_transform(self, X: np.ndarray) -> np.ndarray:
        """è®­ç»ƒå¹¶è½¬æ¢æ•°æ®"""
        return self.fit(X).transform(X)

class DBSCAN:
    """DBSCANå¯†åº¦èšç±»ç®—æ³•"""
    
    def __init__(self, eps: float = 0.5, min_samples: int = 5):
        self.eps = eps
        self.min_samples = min_samples
        self.labels = None
    
    def fit(self, X: np.ndarray) -> 'DBSCAN':
        """è®­ç»ƒæ¨¡å‹"""
        n_samples = X.shape[0]
        self.labels = np.full(n_samples, -1)  # -1è¡¨ç¤ºå™ªå£°ç‚¹
        cluster_id = 0
        
        for i in range(n_samples):
            if self.labels[i] != -1:
                continue
            
            # æ‰¾åˆ°æ ¸å¿ƒç‚¹
            neighbors = self._find_neighbors(X, i)
            
            if len(neighbors) < self.min_samples:
                self.labels[i] = -1  # æ ‡è®°ä¸ºå™ªå£°ç‚¹
                continue
            
            # å¼€å§‹æ–°çš„èšç±»
            cluster_id += 1
            self.labels[i] = cluster_id
            
            # æ‰©å±•èšç±»
            self._expand_cluster(X, neighbors, cluster_id)
        
        return self
    
    def _find_neighbors(self, X: np.ndarray, point_idx: int) -> list:
        """æ‰¾åˆ°é‚»åŸŸå†…çš„ç‚¹"""
        neighbors = []
        for i in range(len(X)):
            if i != point_idx and np.linalg.norm(X[point_idx] - X[i]) <= self.eps:
                neighbors.append(i)
        return neighbors
    
    def _expand_cluster(self, X: np.ndarray, neighbors: list, cluster_id: int):
        """æ‰©å±•èšç±»"""
        i = 0
        while i < len(neighbors):
            neighbor_idx = neighbors[i]
            
            if self.labels[neighbor_idx] == -1:
                self.labels[neighbor_idx] = cluster_id
            
            elif self.labels[neighbor_idx] == 0:
                self.labels[neighbor_idx] = cluster_id
                
                # æ‰¾åˆ°æ–°çš„é‚»å±…
                new_neighbors = self._find_neighbors(X, neighbor_idx)
                
                if len(new_neighbors) >= self.min_samples:
                    neighbors.extend(new_neighbors)
            
            i += 1
```

### 3. å¼ºåŒ–å­¦ä¹ ç®—æ³•

```python
class QLearning:
    """Q-learningç®—æ³•"""
    
    def __init__(self, n_states: int, n_actions: int, learning_rate: float = 0.1, 
                 discount_factor: float = 0.95, epsilon: float = 0.1):
        self.n_states = n_states
        self.n_actions = n_actions
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.epsilon = epsilon
        self.q_table = np.zeros((n_states, n_actions))
    
    def choose_action(self, state: int) -> int:
        """é€‰æ‹©åŠ¨ä½œï¼ˆÎµ-è´ªå©ªç­–ç•¥ï¼‰"""
        if np.random.random() < self.epsilon:
            return np.random.randint(self.n_actions)
        else:
            return np.argmax(self.q_table[state])
    
    def learn(self, state: int, action: int, reward: float, next_state: int):
        """å­¦ä¹ æ›´æ–°Qå€¼"""
        current_q = self.q_table[state, action]
        max_next_q = np.max(self.q_table[next_state])
        
        # Q-learningæ›´æ–°å…¬å¼
        new_q = current_q + self.learning_rate * (reward + 
                                                 self.discount_factor * max_next_q - current_q)
        self.q_table[state, action] = new_q
    
    def get_policy(self) -> np.ndarray:
        """è·å–æœ€ä¼˜ç­–ç•¥"""
        return np.argmax(self.q_table, axis=1)

class PolicyGradient:
    """ç­–ç•¥æ¢¯åº¦ç®—æ³•"""
    
    def __init__(self, n_states: int, n_actions: int, learning_rate: float = 0.01):
        self.n_states = n_states
        self.n_actions = n_actions
        self.learning_rate = learning_rate
        
        # ç­–ç•¥ç½‘ç»œå‚æ•°
        self.theta = np.random.randn(n_states, n_actions) * 0.01
    
    def get_policy(self, state: int) -> np.ndarray:
        """è·å–ç­–ç•¥æ¦‚ç‡åˆ†å¸ƒ"""
        logits = self.theta[state]
        exp_logits = np.exp(logits - np.max(logits))  # æ•°å€¼ç¨³å®šæ€§
        return exp_logits / np.sum(exp_logits)
    
    def choose_action(self, state: int) -> int:
        """æ ¹æ®ç­–ç•¥é€‰æ‹©åŠ¨ä½œ"""
        policy = self.get_policy(state)
        return np.random.choice(self.n_actions, p=policy)
    
    def update_policy(self, states: list, actions: list, rewards: list):
        """æ›´æ–°ç­–ç•¥å‚æ•°"""
        # è®¡ç®—æŠ˜æ‰£å¥–åŠ±
        discounted_rewards = self._compute_discounted_rewards(rewards)
        
        # æ ‡å‡†åŒ–å¥–åŠ±
        discounted_rewards = (discounted_rewards - np.mean(discounted_rewards)) / (np.std(discounted_rewards) + 1e-8)
        
        # æ›´æ–°å‚æ•°
        for state, action, reward in zip(states, actions, discounted_rewards):
            policy = self.get_policy(state)
            
            # è®¡ç®—æ¢¯åº¦
            gradient = np.zeros(self.n_actions)
            gradient[action] = 1 / (policy[action] + 1e-8)
            
            # æ›´æ–°theta
            self.theta[state] += self.learning_rate * reward * gradient
    
    def _compute_discounted_rewards(self, rewards: list, gamma: float = 0.95) -> np.ndarray:
        """è®¡ç®—æŠ˜æ‰£å¥–åŠ±"""
        discounted_rewards = np.zeros_like(rewards, dtype=float)
        running_reward = 0
        
        for i in reversed(range(len(rewards))):
            running_reward = rewards[i] + gamma * running_reward
            discounted_rewards[i] = running_reward
        
        return discounted_rewards
```

## ğŸ“Š ç®—æ³•æ¯”è¾ƒ

### æ€§èƒ½å¯¹æ¯”è¡¨

| ç®—æ³• | æ—¶é—´å¤æ‚åº¦ | ç©ºé—´å¤æ‚åº¦ | é€‚ç”¨åœºæ™¯ | ä¼˜ç‚¹ | ç¼ºç‚¹ |
|------|------------|------------|----------|------|------|
| çº¿æ€§å›å½’ | O(nÂ²) | O(n) | è¿ç»­å€¼é¢„æµ‹ | ç®€å•ã€å¯è§£é‡Š | å‡è®¾çº¿æ€§å…³ç³» |
| é€»è¾‘å›å½’ | O(nÂ²) | O(n) | äºŒåˆ†ç±» | æ¦‚ç‡è¾“å‡ºã€å¯è§£é‡Š | å‡è®¾çº¿æ€§å¯åˆ† |
| å†³ç­–æ ‘ | O(n log n) | O(n) | åˆ†ç±»å›å½’ | å¯è§£é‡Šã€éçº¿æ€§ | å®¹æ˜“è¿‡æ‹Ÿåˆ |
| éšæœºæ£®æ— | O(n log n Ã— m) | O(n Ã— m) | åˆ†ç±»å›å½’ | æŠ—è¿‡æ‹Ÿåˆã€ç‰¹å¾é‡è¦æ€§ | é»‘ç›’æ¨¡å‹ |
| K-means | O(n Ã— k Ã— i) | O(n + k) | èšç±» | ç®€å•ã€é«˜æ•ˆ | éœ€è¦æŒ‡å®škå€¼ |
| PCA | O(nÂ³) | O(nÂ²) | é™ç»´ | ä¿ç•™ä¸»è¦ä¿¡æ¯ | çº¿æ€§é™ç»´ |
| Q-learning | O(nÂ²) | O(nÂ²) | å¼ºåŒ–å­¦ä¹  | æ”¶æ•›æ€§å¥½ | éœ€è¦æ¢ç´¢ |

## ğŸ”„ å·¥ä½œæµç¨‹

### æœºå™¨å­¦ä¹ é¡¹ç›®å·¥ä½œæµç¨‹

```python
def machine_learning_workflow():
    """æœºå™¨å­¦ä¹ é¡¹ç›®å®Œæ•´å·¥ä½œæµç¨‹"""
    
    # 1. æ•°æ®ç”Ÿæˆ
    np.random.seed(42)
    X = np.random.randn(1000, 10)
    y = np.random.randint(0, 3, 1000)
    
    # 2. æ•°æ®é¢„å¤„ç†
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # 3. æ•°æ®åˆ†å‰²
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y, test_size=0.2, random_state=42
    )
    
    # 4. æ¨¡å‹è®­ç»ƒ
    models = {
        'Logistic Regression': LogisticRegression(),
        'Decision Tree': DecisionTree(),
        'Random Forest': RandomForest(n_estimators=10)
    }
    
    results = {}
    
    for name, model in models.items():
        print(f"\nè®­ç»ƒ {name}...")
        
        # è®­ç»ƒæ¨¡å‹
        model.fit(X_train, y_train)
        
        # é¢„æµ‹
        y_pred = model.predict(X_test)
        
        # è¯„ä¼°
        accuracy = accuracy_score(y_test, y_pred)
        results[name] = accuracy
        
        print(f"{name} å‡†ç¡®ç‡: {accuracy:.4f}")
    
    # 5. ç»“æœå¯è§†åŒ–
    plt.figure(figsize=(10, 6))
    names = list(results.keys())
    accuracies = list(results.values())
    
    plt.bar(names, accuracies)
    plt.title('æœºå™¨å­¦ä¹ ç®—æ³•æ€§èƒ½æ¯”è¾ƒ')
    plt.ylabel('å‡†ç¡®ç‡')
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()
    
    return results

# è¿è¡Œå·¥ä½œæµç¨‹
if __name__ == "__main__":
    results = machine_learning_workflow()
    print(f"\næœ€ä½³æ¨¡å‹: {max(results, key=results.get)}")
```

## ğŸ“ˆ åº”ç”¨æ¡ˆä¾‹

### æ¡ˆä¾‹1ï¼šæˆ¿ä»·é¢„æµ‹

```python
def house_price_prediction():
    """æˆ¿ä»·é¢„æµ‹æ¡ˆä¾‹"""
    
    # ç”Ÿæˆæ¨¡æ‹Ÿæˆ¿ä»·æ•°æ®
    np.random.seed(42)
    n_samples = 1000
    
    # ç‰¹å¾ï¼šé¢ç§¯ã€æˆ¿é—´æ•°ã€å¹´é¾„ã€ä½ç½®è¯„åˆ†
    area = np.random.uniform(50, 200, n_samples)
    rooms = np.random.randint(1, 6, n_samples)
    age = np.random.randint(0, 50, n_samples)
    location_score = np.random.uniform(1, 10, n_samples)
    
    # ç›®æ ‡ï¼šæˆ¿ä»·ï¼ˆåŠ å…¥ä¸€äº›å™ªå£°ï¼‰
    price = (area * 1000 + rooms * 50000 + location_score * 20000 - age * 1000 + 
             np.random.normal(0, 10000, n_samples))
    
    X = np.column_stack([area, rooms, age, location_score])
    y = price
    
    # æ•°æ®é¢„å¤„ç†
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # åˆ†å‰²æ•°æ®
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y, test_size=0.2, random_state=42
    )
    
    # è®­ç»ƒçº¿æ€§å›å½’æ¨¡å‹
    model = LinearRegression(learning_rate=0.01, max_iterations=1000)
    model.fit(X_train, y_train)
    
    # é¢„æµ‹
    y_pred = model.predict(X_test)
    
    # è¯„ä¼°
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    
    print(f"æˆ¿ä»·é¢„æµ‹ RMSE: ${rmse:.2f}")
    
    # å¯è§†åŒ–
    plt.figure(figsize=(12, 4))
    
    plt.subplot(1, 2, 1)
    plt.plot(model.cost_history)
    plt.title('è®­ç»ƒæŸå¤±')
    plt.xlabel('è¿­ä»£æ¬¡æ•°')
    plt.ylabel('æŸå¤±')
    
    plt.subplot(1, 2, 2)
    plt.scatter(y_test, y_pred, alpha=0.5)
    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
    plt.xlabel('çœŸå®æˆ¿ä»·')
    plt.ylabel('é¢„æµ‹æˆ¿ä»·')
    plt.title('é¢„æµ‹ vs çœŸå®å€¼')
    
    plt.tight_layout()
    plt.show()
    
    return model, rmse
```

### æ¡ˆä¾‹2ï¼šå®¢æˆ·èšç±»åˆ†æ

```python
def customer_segmentation():
    """å®¢æˆ·èšç±»åˆ†ææ¡ˆä¾‹"""
    
    # ç”Ÿæˆæ¨¡æ‹Ÿå®¢æˆ·æ•°æ®
    np.random.seed(42)
    n_customers = 500
    
    # ç‰¹å¾ï¼šå¹´é¾„ã€æ”¶å…¥ã€æ¶ˆè´¹é¢‘ç‡ã€å¹³å‡æ¶ˆè´¹é‡‘é¢
    age = np.random.normal(35, 10, n_customers)
    income = np.random.normal(50000, 20000, n_customers)
    frequency = np.random.poisson(5, n_customers)
    avg_spending = np.random.normal(200, 50, n_customers)
    
    X = np.column_stack([age, income, frequency, avg_spending])
    
    # æ•°æ®é¢„å¤„ç†
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # K-meansèšç±»
    kmeans = KMeans(n_clusters=3, max_iterations=100)
    kmeans.fit(X_scaled)
    
    # è·å–èšç±»æ ‡ç­¾
    labels = kmeans.predict(X_scaled)
    
    # åˆ†æèšç±»ç»“æœ
    cluster_centers = scaler.inverse_transform(kmeans.centroids)
    
    print("å®¢æˆ·èšç±»åˆ†æç»“æœ:")
    print("=" * 50)
    
    for i in range(3):
        cluster_size = np.sum(labels == i)
        print(f"\nèšç±» {i+1} (å®¢æˆ·æ•°: {cluster_size}):")
        print(f"  å¹³å‡å¹´é¾„: {cluster_centers[i, 0]:.1f}")
        print(f"  å¹³å‡æ”¶å…¥: ${cluster_centers[i, 1]:.0f}")
        print(f"  å¹³å‡æ¶ˆè´¹é¢‘ç‡: {cluster_centers[i, 2]:.1f}")
        print(f"  å¹³å‡æ¶ˆè´¹é‡‘é¢: ${cluster_centers[i, 3]:.0f}")
    
    # å¯è§†åŒ–
    fig = plt.figure(figsize=(15, 5))
    
    # å¹´é¾„ vs æ”¶å…¥
    plt.subplot(1, 3, 1)
    for i in range(3):
        mask = labels == i
        plt.scatter(age[mask], income[mask], alpha=0.6, label=f'èšç±» {i+1}')
    plt.xlabel('å¹´é¾„')
    plt.ylabel('æ”¶å…¥')
    plt.title('å¹´é¾„ vs æ”¶å…¥')
    plt.legend()
    
    # æ¶ˆè´¹é¢‘ç‡ vs å¹³å‡æ¶ˆè´¹
    plt.subplot(1, 3, 2)
    for i in range(3):
        mask = labels == i
        plt.scatter(frequency[mask], avg_spending[mask], alpha=0.6, label=f'èšç±» {i+1}')
    plt.xlabel('æ¶ˆè´¹é¢‘ç‡')
    plt.ylabel('å¹³å‡æ¶ˆè´¹é‡‘é¢')
    plt.title('æ¶ˆè´¹é¢‘ç‡ vs å¹³å‡æ¶ˆè´¹')
    plt.legend()
    
    # èšç±»ä¸­å¿ƒ
    plt.subplot(1, 3, 3)
    features = ['å¹´é¾„', 'æ”¶å…¥', 'é¢‘ç‡', 'æ¶ˆè´¹']
    x = np.arange(len(features))
    width = 0.25
    
    for i in range(3):
        plt.bar(x + i*width, cluster_centers[i], width, label=f'èšç±» {i+1}')
    
    plt.xlabel('ç‰¹å¾')
    plt.ylabel('æ ‡å‡†åŒ–å€¼')
    plt.title('èšç±»ä¸­å¿ƒç‰¹å¾å¯¹æ¯”')
    plt.xticks(x + width, features)
    plt.legend()
    
    plt.tight_layout()
    plt.show()
    
    return kmeans, labels
```

## ğŸ”— ç›¸å…³é“¾æ¥

- [04-02-01-äººå·¥æ™ºèƒ½åŸºç¡€](./04-02-01-äººå·¥æ™ºèƒ½åŸºç¡€.md)
- [04-02-03-æ·±åº¦å­¦ä¹ åŸºç¡€](./04-02-03-æ·±åº¦å­¦ä¹ åŸºç¡€.md)
- [02-ç†è®ºåŸºç¡€/02-01-ç®—æ³•ç†è®º/02-01-02-ç®—æ³•å¤æ‚åº¦åˆ†æ](../02-ç†è®ºåŸºç¡€/02-01-ç®—æ³•ç†è®º/02-01-02-ç®—æ³•å¤æ‚åº¦åˆ†æ.md)
- [01-å½¢å¼ç§‘å­¦/01-01-æ•°å­¦åŸºç¡€/01-01-03-ä»£æ•°åŸºç¡€](../01-å½¢å¼ç§‘å­¦/01-01-æ•°å­¦åŸºç¡€/01-01-03-ä»£æ•°åŸºç¡€.md)

---

**æ–‡æ¡£ç‰ˆæœ¬**ï¼š1.0  
**æœ€åæ›´æ–°**ï¼š2024å¹´  
**ç»´æŠ¤è€…**ï¼šAIåŠ©æ‰‹
