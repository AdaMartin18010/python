# 04-02-02 机器学习算法

## 📋 目录

- [04-02-02 机器学习算法](#04-02-02-机器学习算法)
  - [📋 目录](#-目录)
  - [🎯 概述](#-概述)
  - [🔬 概念定义](#-概念定义)
  - [📐 数学形式化](#-数学形式化)
  - [🐍 Python实现](#-python实现)
  - [📊 算法比较](#-算法比较)
  - [🔄 工作流程](#-工作流程)
  - [📈 应用案例](#-应用案例)
  - [🔗 相关链接](#-相关链接)

## 🎯 概述

机器学习是人工智能的核心分支，通过算法使计算机系统能够从数据中学习并做出预测或决策。本文档涵盖监督学习、无监督学习和强化学习的主要算法。

## 🔬 概念定义

### 定义 2.1 (机器学习)
机器学习是研究计算机算法如何自动改进经验的过程。形式化定义为：

$$ML = (D, H, L, A)$$

其中：
- $D$ 是数据集
- $H$ 是假设空间
- $L$ 是学习算法
- $A$ 是评估指标

### 定义 2.2 (监督学习)
给定训练集 $D = \{(x_i, y_i)\}_{i=1}^n$，学习函数 $f: X \rightarrow Y$ 使得 $f(x_i) \approx y_i$。

### 定义 2.3 (无监督学习)
给定数据集 $D = \{x_i\}_{i=1}^n$，发现数据中的隐藏模式或结构。

### 定义 2.4 (强化学习)
智能体通过与环境交互学习最优策略 $\pi^*: S \rightarrow A$ 以最大化累积奖励。

## 📐 数学形式化

### 定理 2.1 (VC维理论)
对于假设空间 $\mathcal{H}$，其VC维为 $d$，则泛化误差上界为：

$$R(f) \leq \hat{R}(f) + \sqrt{\frac{d(\log(2n/d) + 1) + \log(4/\delta)}{n}}$$

### 定理 2.2 (贝叶斯定理)
$$P(h|D) = \frac{P(D|h)P(h)}{P(D)}$$

其中：
- $P(h|D)$ 是后验概率
- $P(D|h)$ 是似然函数
- $P(h)$ 是先验概率
- $P(D)$ 是证据

### 定义 2.5 (损失函数)
对于预测 $\hat{y}$ 和真实值 $y$，损失函数定义为：

$$L(\hat{y}, y) = \frac{1}{2}(\hat{y} - y)^2 \quad \text{(均方误差)}$$

## 🐍 Python实现

### 1. 监督学习算法

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, mean_squared_error
from sklearn.preprocessing import StandardScaler
import torch
import torch.nn as nn
import torch.optim as optim

class LinearRegression:
    """线性回归算法"""
    
    def __init__(self, learning_rate: float = 0.01, max_iterations: int = 1000):
        self.learning_rate = learning_rate
        self.max_iterations = max_iterations
        self.weights = None
        self.bias = None
        self.cost_history = []
    
    def fit(self, X: np.ndarray, y: np.ndarray) -> 'LinearRegression':
        """训练模型"""
        n_samples, n_features = X.shape
        
        # 初始化参数
        self.weights = np.zeros(n_features)
        self.bias = 0
        
        # 梯度下降
        for i in range(self.max_iterations):
            # 前向传播
            y_pred = self._predict(X)
            
            # 计算梯度
            dw = (1/n_samples) * np.dot(X.T, (y_pred - y))
            db = (1/n_samples) * np.sum(y_pred - y)
            
            # 更新参数
            self.weights -= self.learning_rate * dw
            self.bias -= self.learning_rate * db
            
            # 记录成本
            cost = self._compute_cost(y_pred, y)
            self.cost_history.append(cost)
        
        return self
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """预测"""
        return self._predict(X)
    
    def _predict(self, X: np.ndarray) -> np.ndarray:
        """内部预测方法"""
        return np.dot(X, self.weights) + self.bias
    
    def _compute_cost(self, y_pred: np.ndarray, y_true: np.ndarray) -> float:
        """计算成本"""
        return np.mean((y_pred - y_true) ** 2)

class LogisticRegression:
    """逻辑回归算法"""
    
    def __init__(self, learning_rate: float = 0.01, max_iterations: int = 1000):
        self.learning_rate = learning_rate
        self.max_iterations = max_iterations
        self.weights = None
        self.bias = None
        self.cost_history = []
    
    def fit(self, X: np.ndarray, y: np.ndarray) -> 'LogisticRegression':
        """训练模型"""
        n_samples, n_features = X.shape
        
        # 初始化参数
        self.weights = np.zeros(n_features)
        self.bias = 0
        
        # 梯度下降
        for i in range(self.max_iterations):
            # 前向传播
            y_pred = self._sigmoid(self._predict(X))
            
            # 计算梯度
            dw = (1/n_samples) * np.dot(X.T, (y_pred - y))
            db = (1/n_samples) * np.sum(y_pred - y)
            
            # 更新参数
            self.weights -= self.learning_rate * dw
            self.bias -= self.learning_rate * db
            
            # 记录成本
            cost = self._compute_cost(y_pred, y)
            self.cost_history.append(cost)
        
        return self
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """预测"""
        y_pred = self._sigmoid(self._predict(X))
        return (y_pred >= 0.5).astype(int)
    
    def predict_proba(self, X: np.ndarray) -> np.ndarray:
        """预测概率"""
        return self._sigmoid(self._predict(X))
    
    def _predict(self, X: np.ndarray) -> np.ndarray:
        """内部预测方法"""
        return np.dot(X, self.weights) + self.bias
    
    def _sigmoid(self, z: np.ndarray) -> np.ndarray:
        """Sigmoid激活函数"""
        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))
    
    def _compute_cost(self, y_pred: np.ndarray, y_true: np.ndarray) -> float:
        """计算成本（交叉熵）"""
        epsilon = 1e-15
        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))

class DecisionTree:
    """决策树算法"""
    
    def __init__(self, max_depth: int = 5, min_samples_split: int = 2):
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.root = None
    
    def fit(self, X: np.ndarray, y: np.ndarray) -> 'DecisionTree':
        """训练模型"""
        self.root = self._build_tree(X, y, depth=0)
        return self
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """预测"""
        return np.array([self._predict_single(x, self.root) for x in X])
    
    def _build_tree(self, X: np.ndarray, y: np.ndarray, depth: int) -> dict:
        """构建决策树"""
        n_samples, n_features = X.shape
        n_classes = len(np.unique(y))
        
        # 停止条件
        if (depth >= self.max_depth or 
            n_samples < self.min_samples_split or 
            n_classes == 1):
            return {'type': 'leaf', 'value': self._most_common(y)}
        
        # 寻找最佳分割
        best_feature, best_threshold = self._find_best_split(X, y)
        
        if best_feature is None:
            return {'type': 'leaf', 'value': self._most_common(y)}
        
        # 分割数据
        left_mask = X[:, best_feature] <= best_threshold
        right_mask = ~left_mask
        
        left_tree = self._build_tree(X[left_mask], y[left_mask], depth + 1)
        right_tree = self._build_tree(X[right_mask], y[right_mask], depth + 1)
        
        return {
            'type': 'node',
            'feature': best_feature,
            'threshold': best_threshold,
            'left': left_tree,
            'right': right_tree
        }
    
    def _find_best_split(self, X: np.ndarray, y: np.ndarray) -> tuple:
        """寻找最佳分割点"""
        best_gain = -1
        best_feature = None
        best_threshold = None
        
        n_samples, n_features = X.shape
        
        for feature in range(n_features):
            thresholds = np.unique(X[:, feature])
            
            for threshold in thresholds:
                left_mask = X[:, feature] <= threshold
                right_mask = ~left_mask
                
                if np.sum(left_mask) == 0 or np.sum(right_mask) == 0:
                    continue
                
                gain = self._information_gain(y, y[left_mask], y[right_mask])
                
                if gain > best_gain:
                    best_gain = gain
                    best_feature = feature
                    best_threshold = threshold
        
        return best_feature, best_threshold
    
    def _information_gain(self, parent: np.ndarray, left: np.ndarray, right: np.ndarray) -> float:
        """计算信息增益"""
        parent_entropy = self._entropy(parent)
        left_entropy = self._entropy(left)
        right_entropy = self._entropy(right)
        
        n_left = len(left)
        n_right = len(right)
        n_parent = len(parent)
        
        weighted_entropy = (n_left / n_parent) * left_entropy + (n_right / n_parent) * right_entropy
        
        return parent_entropy - weighted_entropy
    
    def _entropy(self, y: np.ndarray) -> float:
        """计算熵"""
        _, counts = np.unique(y, return_counts=True)
        probabilities = counts / len(y)
        return -np.sum(probabilities * np.log2(probabilities + 1e-10))
    
    def _most_common(self, y: np.ndarray):
        """返回最常见的类别"""
        values, counts = np.unique(y, return_counts=True)
        return values[np.argmax(counts)]
    
    def _predict_single(self, x: np.ndarray, node: dict):
        """预测单个样本"""
        if node['type'] == 'leaf':
            return node['value']
        
        if x[node['feature']] <= node['threshold']:
            return self._predict_single(x, node['left'])
        else:
            return self._predict_single(x, node['right'])

class RandomForest:
    """随机森林算法"""
    
    def __init__(self, n_estimators: int = 10, max_depth: int = 5, min_samples_split: int = 2):
        self.n_estimators = n_estimators
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.trees = []
    
    def fit(self, X: np.ndarray, y: np.ndarray) -> 'RandomForest':
        """训练模型"""
        self.trees = []
        
        for _ in range(self.n_estimators):
            # 随机采样
            indices = np.random.choice(len(X), size=len(X), replace=True)
            X_bootstrap = X[indices]
            y_bootstrap = y[indices]
            
            # 训练决策树
            tree = DecisionTree(max_depth=self.max_depth, min_samples_split=self.min_samples_split)
            tree.fit(X_bootstrap, y_bootstrap)
            self.trees.append(tree)
        
        return self
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """预测"""
        predictions = np.array([tree.predict(X) for tree in self.trees])
        return np.array([self._most_common(pred) for pred in predictions.T])
    
    def _most_common(self, predictions: np.ndarray):
        """返回最常见的预测"""
        values, counts = np.unique(predictions, return_counts=True)
        return values[np.argmax(counts)]
```

### 2. 无监督学习算法

```python
class KMeans:
    """K-means聚类算法"""
    
    def __init__(self, n_clusters: int = 3, max_iterations: int = 100):
        self.n_clusters = n_clusters
        self.max_iterations = max_iterations
        self.centroids = None
        self.labels = None
    
    def fit(self, X: np.ndarray) -> 'KMeans':
        """训练模型"""
        n_samples, n_features = X.shape
        
        # 随机初始化聚类中心
        indices = np.random.choice(n_samples, self.n_clusters, replace=False)
        self.centroids = X[indices]
        
        for _ in range(self.max_iterations):
            # 分配样本到最近的聚类中心
            self.labels = self._assign_clusters(X)
            
            # 更新聚类中心
            new_centroids = np.array([X[self.labels == k].mean(axis=0) 
                                     for k in range(self.n_clusters)])
            
            # 检查收敛
            if np.allclose(self.centroids, new_centroids):
                break
            
            self.centroids = new_centroids
        
        return self
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """预测聚类标签"""
        return self._assign_clusters(X)
    
    def _assign_clusters(self, X: np.ndarray) -> np.ndarray:
        """分配样本到聚类"""
        distances = np.sqrt(((X - self.centroids[:, np.newaxis]) ** 2).sum(axis=2))
        return np.argmin(distances, axis=0)

class PrincipalComponentAnalysis:
    """主成分分析算法"""
    
    def __init__(self, n_components: int = 2):
        self.n_components = n_components
        self.components = None
        self.mean = None
        self.explained_variance_ratio = None
    
    def fit(self, X: np.ndarray) -> 'PrincipalComponentAnalysis':
        """训练模型"""
        # 中心化数据
        self.mean = np.mean(X, axis=0)
        X_centered = X - self.mean
        
        # 计算协方差矩阵
        cov_matrix = np.cov(X_centered.T)
        
        # 特征值分解
        eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)
        
        # 排序并选择前n_components个主成分
        indices = np.argsort(eigenvalues)[::-1]
        self.components = eigenvectors[:, indices[:self.n_components]]
        
        # 计算解释方差比
        total_variance = np.sum(eigenvalues)
        self.explained_variance_ratio = eigenvalues[indices[:self.n_components]] / total_variance
        
        return self
    
    def transform(self, X: np.ndarray) -> np.ndarray:
        """转换数据"""
        X_centered = X - self.mean
        return np.dot(X_centered, self.components)
    
    def fit_transform(self, X: np.ndarray) -> np.ndarray:
        """训练并转换数据"""
        return self.fit(X).transform(X)

class DBSCAN:
    """DBSCAN密度聚类算法"""
    
    def __init__(self, eps: float = 0.5, min_samples: int = 5):
        self.eps = eps
        self.min_samples = min_samples
        self.labels = None
    
    def fit(self, X: np.ndarray) -> 'DBSCAN':
        """训练模型"""
        n_samples = X.shape[0]
        self.labels = np.full(n_samples, -1)  # -1表示噪声点
        cluster_id = 0
        
        for i in range(n_samples):
            if self.labels[i] != -1:
                continue
            
            # 找到核心点
            neighbors = self._find_neighbors(X, i)
            
            if len(neighbors) < self.min_samples:
                self.labels[i] = -1  # 标记为噪声点
                continue
            
            # 开始新的聚类
            cluster_id += 1
            self.labels[i] = cluster_id
            
            # 扩展聚类
            self._expand_cluster(X, neighbors, cluster_id)
        
        return self
    
    def _find_neighbors(self, X: np.ndarray, point_idx: int) -> list:
        """找到邻域内的点"""
        neighbors = []
        for i in range(len(X)):
            if i != point_idx and np.linalg.norm(X[point_idx] - X[i]) <= self.eps:
                neighbors.append(i)
        return neighbors
    
    def _expand_cluster(self, X: np.ndarray, neighbors: list, cluster_id: int):
        """扩展聚类"""
        i = 0
        while i < len(neighbors):
            neighbor_idx = neighbors[i]
            
            if self.labels[neighbor_idx] == -1:
                self.labels[neighbor_idx] = cluster_id
            
            elif self.labels[neighbor_idx] == 0:
                self.labels[neighbor_idx] = cluster_id
                
                # 找到新的邻居
                new_neighbors = self._find_neighbors(X, neighbor_idx)
                
                if len(new_neighbors) >= self.min_samples:
                    neighbors.extend(new_neighbors)
            
            i += 1
```

### 3. 强化学习算法

```python
class QLearning:
    """Q-learning算法"""
    
    def __init__(self, n_states: int, n_actions: int, learning_rate: float = 0.1, 
                 discount_factor: float = 0.95, epsilon: float = 0.1):
        self.n_states = n_states
        self.n_actions = n_actions
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.epsilon = epsilon
        self.q_table = np.zeros((n_states, n_actions))
    
    def choose_action(self, state: int) -> int:
        """选择动作（ε-贪婪策略）"""
        if np.random.random() < self.epsilon:
            return np.random.randint(self.n_actions)
        else:
            return np.argmax(self.q_table[state])
    
    def learn(self, state: int, action: int, reward: float, next_state: int):
        """学习更新Q值"""
        current_q = self.q_table[state, action]
        max_next_q = np.max(self.q_table[next_state])
        
        # Q-learning更新公式
        new_q = current_q + self.learning_rate * (reward + 
                                                 self.discount_factor * max_next_q - current_q)
        self.q_table[state, action] = new_q
    
    def get_policy(self) -> np.ndarray:
        """获取最优策略"""
        return np.argmax(self.q_table, axis=1)

class PolicyGradient:
    """策略梯度算法"""
    
    def __init__(self, n_states: int, n_actions: int, learning_rate: float = 0.01):
        self.n_states = n_states
        self.n_actions = n_actions
        self.learning_rate = learning_rate
        
        # 策略网络参数
        self.theta = np.random.randn(n_states, n_actions) * 0.01
    
    def get_policy(self, state: int) -> np.ndarray:
        """获取策略概率分布"""
        logits = self.theta[state]
        exp_logits = np.exp(logits - np.max(logits))  # 数值稳定性
        return exp_logits / np.sum(exp_logits)
    
    def choose_action(self, state: int) -> int:
        """根据策略选择动作"""
        policy = self.get_policy(state)
        return np.random.choice(self.n_actions, p=policy)
    
    def update_policy(self, states: list, actions: list, rewards: list):
        """更新策略参数"""
        # 计算折扣奖励
        discounted_rewards = self._compute_discounted_rewards(rewards)
        
        # 标准化奖励
        discounted_rewards = (discounted_rewards - np.mean(discounted_rewards)) / (np.std(discounted_rewards) + 1e-8)
        
        # 更新参数
        for state, action, reward in zip(states, actions, discounted_rewards):
            policy = self.get_policy(state)
            
            # 计算梯度
            gradient = np.zeros(self.n_actions)
            gradient[action] = 1 / (policy[action] + 1e-8)
            
            # 更新theta
            self.theta[state] += self.learning_rate * reward * gradient
    
    def _compute_discounted_rewards(self, rewards: list, gamma: float = 0.95) -> np.ndarray:
        """计算折扣奖励"""
        discounted_rewards = np.zeros_like(rewards, dtype=float)
        running_reward = 0
        
        for i in reversed(range(len(rewards))):
            running_reward = rewards[i] + gamma * running_reward
            discounted_rewards[i] = running_reward
        
        return discounted_rewards
```

## 📊 算法比较

### 性能对比表

| 算法 | 时间复杂度 | 空间复杂度 | 适用场景 | 优点 | 缺点 |
|------|------------|------------|----------|------|------|
| 线性回归 | O(n²) | O(n) | 连续值预测 | 简单、可解释 | 假设线性关系 |
| 逻辑回归 | O(n²) | O(n) | 二分类 | 概率输出、可解释 | 假设线性可分 |
| 决策树 | O(n log n) | O(n) | 分类回归 | 可解释、非线性 | 容易过拟合 |
| 随机森林 | O(n log n × m) | O(n × m) | 分类回归 | 抗过拟合、特征重要性 | 黑盒模型 |
| K-means | O(n × k × i) | O(n + k) | 聚类 | 简单、高效 | 需要指定k值 |
| PCA | O(n³) | O(n²) | 降维 | 保留主要信息 | 线性降维 |
| Q-learning | O(n²) | O(n²) | 强化学习 | 收敛性好 | 需要探索 |

## 🔄 工作流程

### 机器学习项目工作流程

```python
def machine_learning_workflow():
    """机器学习项目完整工作流程"""
    
    # 1. 数据生成
    np.random.seed(42)
    X = np.random.randn(1000, 10)
    y = np.random.randint(0, 3, 1000)
    
    # 2. 数据预处理
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # 3. 数据分割
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y, test_size=0.2, random_state=42
    )
    
    # 4. 模型训练
    models = {
        'Logistic Regression': LogisticRegression(),
        'Decision Tree': DecisionTree(),
        'Random Forest': RandomForest(n_estimators=10)
    }
    
    results = {}
    
    for name, model in models.items():
        print(f"\n训练 {name}...")
        
        # 训练模型
        model.fit(X_train, y_train)
        
        # 预测
        y_pred = model.predict(X_test)
        
        # 评估
        accuracy = accuracy_score(y_test, y_pred)
        results[name] = accuracy
        
        print(f"{name} 准确率: {accuracy:.4f}")
    
    # 5. 结果可视化
    plt.figure(figsize=(10, 6))
    names = list(results.keys())
    accuracies = list(results.values())
    
    plt.bar(names, accuracies)
    plt.title('机器学习算法性能比较')
    plt.ylabel('准确率')
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()
    
    return results

# 运行工作流程
if __name__ == "__main__":
    results = machine_learning_workflow()
    print(f"\n最佳模型: {max(results, key=results.get)}")
```

## 📈 应用案例

### 案例1：房价预测

```python
def house_price_prediction():
    """房价预测案例"""
    
    # 生成模拟房价数据
    np.random.seed(42)
    n_samples = 1000
    
    # 特征：面积、房间数、年龄、位置评分
    area = np.random.uniform(50, 200, n_samples)
    rooms = np.random.randint(1, 6, n_samples)
    age = np.random.randint(0, 50, n_samples)
    location_score = np.random.uniform(1, 10, n_samples)
    
    # 目标：房价（加入一些噪声）
    price = (area * 1000 + rooms * 50000 + location_score * 20000 - age * 1000 + 
             np.random.normal(0, 10000, n_samples))
    
    X = np.column_stack([area, rooms, age, location_score])
    y = price
    
    # 数据预处理
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # 分割数据
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y, test_size=0.2, random_state=42
    )
    
    # 训练线性回归模型
    model = LinearRegression(learning_rate=0.01, max_iterations=1000)
    model.fit(X_train, y_train)
    
    # 预测
    y_pred = model.predict(X_test)
    
    # 评估
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    
    print(f"房价预测 RMSE: ${rmse:.2f}")
    
    # 可视化
    plt.figure(figsize=(12, 4))
    
    plt.subplot(1, 2, 1)
    plt.plot(model.cost_history)
    plt.title('训练损失')
    plt.xlabel('迭代次数')
    plt.ylabel('损失')
    
    plt.subplot(1, 2, 2)
    plt.scatter(y_test, y_pred, alpha=0.5)
    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
    plt.xlabel('真实房价')
    plt.ylabel('预测房价')
    plt.title('预测 vs 真实值')
    
    plt.tight_layout()
    plt.show()
    
    return model, rmse
```

### 案例2：客户聚类分析

```python
def customer_segmentation():
    """客户聚类分析案例"""
    
    # 生成模拟客户数据
    np.random.seed(42)
    n_customers = 500
    
    # 特征：年龄、收入、消费频率、平均消费金额
    age = np.random.normal(35, 10, n_customers)
    income = np.random.normal(50000, 20000, n_customers)
    frequency = np.random.poisson(5, n_customers)
    avg_spending = np.random.normal(200, 50, n_customers)
    
    X = np.column_stack([age, income, frequency, avg_spending])
    
    # 数据预处理
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # K-means聚类
    kmeans = KMeans(n_clusters=3, max_iterations=100)
    kmeans.fit(X_scaled)
    
    # 获取聚类标签
    labels = kmeans.predict(X_scaled)
    
    # 分析聚类结果
    cluster_centers = scaler.inverse_transform(kmeans.centroids)
    
    print("客户聚类分析结果:")
    print("=" * 50)
    
    for i in range(3):
        cluster_size = np.sum(labels == i)
        print(f"\n聚类 {i+1} (客户数: {cluster_size}):")
        print(f"  平均年龄: {cluster_centers[i, 0]:.1f}")
        print(f"  平均收入: ${cluster_centers[i, 1]:.0f}")
        print(f"  平均消费频率: {cluster_centers[i, 2]:.1f}")
        print(f"  平均消费金额: ${cluster_centers[i, 3]:.0f}")
    
    # 可视化
    fig = plt.figure(figsize=(15, 5))
    
    # 年龄 vs 收入
    plt.subplot(1, 3, 1)
    for i in range(3):
        mask = labels == i
        plt.scatter(age[mask], income[mask], alpha=0.6, label=f'聚类 {i+1}')
    plt.xlabel('年龄')
    plt.ylabel('收入')
    plt.title('年龄 vs 收入')
    plt.legend()
    
    # 消费频率 vs 平均消费
    plt.subplot(1, 3, 2)
    for i in range(3):
        mask = labels == i
        plt.scatter(frequency[mask], avg_spending[mask], alpha=0.6, label=f'聚类 {i+1}')
    plt.xlabel('消费频率')
    plt.ylabel('平均消费金额')
    plt.title('消费频率 vs 平均消费')
    plt.legend()
    
    # 聚类中心
    plt.subplot(1, 3, 3)
    features = ['年龄', '收入', '频率', '消费']
    x = np.arange(len(features))
    width = 0.25
    
    for i in range(3):
        plt.bar(x + i*width, cluster_centers[i], width, label=f'聚类 {i+1}')
    
    plt.xlabel('特征')
    plt.ylabel('标准化值')
    plt.title('聚类中心特征对比')
    plt.xticks(x + width, features)
    plt.legend()
    
    plt.tight_layout()
    plt.show()
    
    return kmeans, labels
```

## 🔗 相关链接

- [04-02-01-人工智能基础](./04-02-01-人工智能基础.md)
- [04-02-03-深度学习基础](./04-02-03-深度学习基础.md)
- [02-理论基础/02-01-算法理论/02-01-02-算法复杂度分析](../02-理论基础/02-01-算法理论/02-01-02-算法复杂度分析.md)
- [01-形式科学/01-01-数学基础/01-01-03-代数基础](../01-形式科学/01-01-数学基础/01-01-03-代数基础.md)

---

**文档版本**：1.0  
**最后更新**：2024年  
**维护者**：AI助手
