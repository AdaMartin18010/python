# 04.01 人工智能与机器学习

## 概述

人工智能与机器学习是现代软件工程的重要领域，涉及大规模数据处理、模型训练、推理服务等核心问题。本文档从形式化角度阐述AI/ML的理论基础和实践应用。

## 1. 机器学习理论基础

### 1.1 形式化定义

设 $X$ 为输入空间，$Y$ 为输出空间，$D$ 为数据分布，则机器学习问题可定义为：

$$\text{ML} = \langle X, Y, D, \mathcal{H}, \mathcal{L} \rangle$$

其中：
- $X \subseteq \mathbb{R}^d$ 为 $d$ 维输入空间
- $Y$ 为输出空间（分类：$Y = \{1, 2, ..., k\}$，回归：$Y = \mathbb{R}$）
- $D$ 为数据分布 $D: X \times Y \rightarrow [0, 1]$
- $\mathcal{H}$ 为假设空间
- $\mathcal{L}$ 为损失函数

### 1.2 学习目标

机器学习的目标是找到最优假设 $h^* \in \mathcal{H}$：

$$h^* = \arg\min_{h \in \mathcal{H}} \mathbb{E}_{(x,y) \sim D}[\mathcal{L}(h(x), y)]$$

### 1.3 Python 形式化实现

```python
from abc import ABC, abstractmethod
from typing import TypeVar, Generic, List, Tuple, Optional, Callable
import numpy as np
from dataclasses import dataclass

T = TypeVar('T')
U = TypeVar('U')

@dataclass
class DataPoint:
    """数据点"""
    features: np.ndarray
    label: Optional[U] = None

@dataclass
class Dataset:
    """数据集"""
    X: np.ndarray  # 特征矩阵
    y: Optional[np.ndarray] = None  # 标签向量
    
    def __post_init__(self):
        if self.y is not None:
            assert len(self.X) == len(self.y), "Features and labels must have same length"
    
    def __len__(self) -> int:
        return len(self.X)
    
    def split(self, ratio: float = 0.8) -> Tuple['Dataset', 'Dataset']:
        """分割数据集"""
        n = len(self)
        split_idx = int(n * ratio)
        
        train_X = self.X[:split_idx]
        train_y = self.y[:split_idx] if self.y is not None else None
        test_X = self.X[split_idx:]
        test_y = self.y[split_idx:] if self.y is not None else None
        
        return (
            Dataset(train_X, train_y),
            Dataset(test_X, test_y)
        )

class Hypothesis(ABC, Generic[T, U]):
    """假设空间抽象基类"""
    
    @abstractmethod
    def predict(self, x: T) -> U:
        """预测方法"""
        pass
    
    @abstractmethod
    def fit(self, dataset: Dataset) -> None:
        """训练方法"""
        pass

class LossFunction(ABC):
    """损失函数抽象基类"""
    
    @abstractmethod
    def compute(self, y_pred: np.ndarray, y_true: np.ndarray) -> float:
        """计算损失"""
        pass
    
    @abstractmethod
    def gradient(self, y_pred: np.ndarray, y_true: np.ndarray) -> np.ndarray:
        """计算梯度"""
        pass

class MeanSquaredError(LossFunction):
    """均方误差损失"""
    
    def compute(self, y_pred: np.ndarray, y_true: np.ndarray) -> float:
        return np.mean((y_pred - y_true) ** 2)
    
    def gradient(self, y_pred: np.ndarray, y_true: np.ndarray) -> np.ndarray:
        return 2 * (y_pred - y_true) / len(y_pred)

class CrossEntropyLoss(LossFunction):
    """交叉熵损失"""
    
    def compute(self, y_pred: np.ndarray, y_true: np.ndarray) -> float:
        # 添加数值稳定性
        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)
        return -np.mean(y_true * np.log(y_pred))
    
    def gradient(self, y_pred: np.ndarray, y_true: np.ndarray) -> np.ndarray:
        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)
        return -(y_true / y_pred) / len(y_pred)

class MachineLearningModel:
    """机器学习模型基类"""
    
    def __init__(self, hypothesis: Hypothesis, loss_function: LossFunction):
        self.hypothesis = hypothesis
        self.loss_function = loss_function
        self.training_history: List[float] = []
    
    def train(self, dataset: Dataset, epochs: int = 100, learning_rate: float = 0.01) -> None:
        """训练模型"""
        for epoch in range(epochs):
            # 前向传播
            y_pred = self.hypothesis.predict(dataset.X)
            
            # 计算损失
            loss = self.loss_function.compute(y_pred, dataset.y)
            self.training_history.append(loss)
            
            # 反向传播（如果假设支持）
            if hasattr(self.hypothesis, 'update_parameters'):
                gradient = self.loss_function.gradient(y_pred, dataset.y)
                self.hypothesis.update_parameters(gradient, learning_rate)
            
            if epoch % 10 == 0:
                print(f"Epoch {epoch}, Loss: {loss:.4f}")
    
    def evaluate(self, dataset: Dataset) -> float:
        """评估模型"""
        y_pred = self.hypothesis.predict(dataset.X)
        return self.loss_function.compute(y_pred, dataset.y)
```

## 2. 线性回归模型

### 2.1 数学定义

线性回归模型假设输出是输入的线性组合：

$$h_w(x) = w^T x + b$$

其中 $w \in \mathbb{R}^d$ 为权重向量，$b \in \mathbb{R}$ 为偏置项。

### 2.2 损失函数

使用均方误差作为损失函数：

$$\mathcal{L}(w, b) = \frac{1}{n} \sum_{i=1}^{n} (h_w(x_i) - y_i)^2$$

### 2.3 Python 实现

```python
class LinearRegression(Hypothesis[np.ndarray, np.ndarray]):
    """线性回归模型"""
    
    def __init__(self, input_dim: int):
        self.input_dim = input_dim
        self.weights = np.random.randn(input_dim)
        self.bias = np.random.randn()
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """预测"""
        return X @ self.weights + self.bias
    
    def fit(self, dataset: Dataset) -> None:
        """使用最小二乘法拟合"""
        # 添加偏置项
        X_augmented = np.column_stack([dataset.X, np.ones(len(dataset.X))])
        
        # 最小二乘解
        theta = np.linalg.inv(X_augmented.T @ X_augmented) @ X_augmented.T @ dataset.y
        
        self.weights = theta[:-1]
        self.bias = theta[-1]
    
    def update_parameters(self, gradient: np.ndarray, learning_rate: float) -> None:
        """更新参数（用于梯度下降）"""
        # 这里简化处理，实际需要根据梯度更新权重和偏置
        pass

# 使用示例
def test_linear_regression():
    """测试线性回归"""
    # 生成数据
    np.random.seed(42)
    n_samples = 100
    X = np.random.randn(n_samples, 2)
    true_weights = np.array([2.0, -1.5])
    true_bias = 1.0
    y = X @ true_weights + true_bias + 0.1 * np.random.randn(n_samples)
    
    # 创建数据集
    dataset = Dataset(X, y)
    train_dataset, test_dataset = dataset.split(0.8)
    
    # 训练模型
    model = LinearRegression(input_dim=2)
    model.fit(train_dataset)
    
    # 评估
    train_loss = model.evaluate(train_dataset)
    test_loss = model.evaluate(test_dataset)
    
    print(f"Train Loss: {train_loss:.4f}")
    print(f"Test Loss: {test_loss:.4f}")
    print(f"True weights: {true_weights}")
    print(f"Learned weights: {model.weights}")
    print(f"True bias: {true_bias}")
    print(f"Learned bias: {model.bias}")
```

## 3. 神经网络模型

### 3.1 数学定义

神经网络是由多个层组成的函数：

$$f(x) = \sigma_L(W_L \sigma_{L-1}(W_{L-1} ... \sigma_1(W_1 x + b_1) ... + b_{L-1}) + b_L)$$

其中：
- $W_i$ 为第 $i$ 层的权重矩阵
- $b_i$ 为第 $i$ 层的偏置向量
- $\sigma_i$ 为第 $i$ 层的激活函数

### 3.2 Python 实现

```python
from typing import List, Tuple

class Layer(ABC):
    """神经网络层抽象基类"""
    
    @abstractmethod
    def forward(self, x: np.ndarray) -> np.ndarray:
        """前向传播"""
        pass
    
    @abstractmethod
    def backward(self, grad_output: np.ndarray) -> np.ndarray:
        """反向传播"""
        pass

class DenseLayer(Layer):
    """全连接层"""
    
    def __init__(self, input_dim: int, output_dim: int):
        self.input_dim = input_dim
        self.output_dim = output_dim
        
        # 初始化权重
        self.weights = np.random.randn(input_dim, output_dim) * 0.01
        self.bias = np.zeros(output_dim)
        
        # 缓存
        self.last_input = None
        self.last_output = None
    
    def forward(self, x: np.ndarray) -> np.ndarray:
        """前向传播"""
        self.last_input = x
        self.last_output = x @ self.weights + self.bias
        return self.last_output
    
    def backward(self, grad_output: np.ndarray) -> np.ndarray:
        """反向传播"""
        grad_input = grad_output @ self.weights.T
        grad_weights = self.last_input.T @ grad_output
        grad_bias = np.sum(grad_output, axis=0)
        
        # 更新参数
        self.weights -= 0.01 * grad_weights
        self.bias -= 0.01 * grad_bias
        
        return grad_input

class ReLU(Layer):
    """ReLU激活函数"""
    
    def __init__(self):
        self.last_input = None
    
    def forward(self, x: np.ndarray) -> np.ndarray:
        """前向传播"""
        self.last_input = x
        return np.maximum(0, x)
    
    def backward(self, grad_output: np.ndarray) -> np.ndarray:
        """反向传播"""
        return grad_output * (self.last_input > 0)

class NeuralNetwork(Hypothesis[np.ndarray, np.ndarray]):
    """神经网络模型"""
    
    def __init__(self, layers: List[Layer]):
        self.layers = layers
    
    def forward(self, x: np.ndarray) -> np.ndarray:
        """前向传播"""
        output = x
        for layer in self.layers:
            output = layer.forward(output)
        return output
    
    def backward(self, grad_output: np.ndarray) -> None:
        """反向传播"""
        for layer in reversed(self.layers):
            grad_output = layer.backward(grad_output)
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """预测"""
        return self.forward(X)
    
    def fit(self, dataset: Dataset) -> None:
        """训练（简化版本）"""
        # 这里简化处理，实际需要实现完整的训练循环
        pass
    
    def update_parameters(self, gradient: np.ndarray, learning_rate: float) -> None:
        """更新参数"""
        self.backward(gradient)

# 使用示例
def test_neural_network():
    """测试神经网络"""
    # 创建网络
    layers = [
        DenseLayer(2, 10),
        ReLU(),
        DenseLayer(10, 1)
    ]
    
    network = NeuralNetwork(layers)
    
    # 生成数据
    np.random.seed(42)
    X = np.random.randn(100, 2)
    y = np.sum(X, axis=1, keepdims=True) + 0.1 * np.random.randn(100, 1)
    
    # 前向传播
    output = network.predict(X)
    print(f"Output shape: {output.shape}")
    print(f"First few outputs: {output[:5].flatten()}")
```

## 4. 深度学习框架设计

### 4.1 计算图模型

深度学习框架的核心是计算图，可形式化为：

$$G = \langle V, E, \mathcal{F} \rangle$$

其中：
- $V$ 为节点集合（操作）
- $E$ 为边集合（数据流）
- $\mathcal{F}$ 为操作函数集合

### 4.2 Python 实现

```python
from typing import Dict, Any, Set, List
from dataclasses import dataclass
from enum import Enum

class OperationType(Enum):
    """操作类型"""
    ADD = "add"
    MULTIPLY = "multiply"
    RELU = "relu"
    LINEAR = "linear"

@dataclass
class Tensor:
    """张量"""
    data: np.ndarray
    requires_grad: bool = False
    grad: Optional[np.ndarray] = None
    
    def __post_init__(self):
        if self.requires_grad:
            self.grad = np.zeros_like(self.data)

@dataclass
class Operation:
    """操作节点"""
    op_type: OperationType
    inputs: List['Tensor']
    output: Optional['Tensor'] = None
    backward_fn: Optional[Callable] = None

class ComputationalGraph:
    """计算图"""
    
    def __init__(self):
        self.operations: List[Operation] = []
        self.tensors: Set[Tensor] = set()
    
    def add_operation(self, operation: Operation) -> None:
        """添加操作"""
        self.operations.append(operation)
        self.tensors.update(operation.inputs)
        if operation.output:
            self.tensors.add(operation.output)
    
    def forward(self) -> None:
        """前向传播"""
        for op in self.operations:
            if op.op_type == OperationType.ADD:
                op.output = self._add(op.inputs[0], op.inputs[1])
            elif op.op_type == OperationType.MULTIPLY:
                op.output = self._multiply(op.inputs[0], op.inputs[1])
            elif op.op_type == OperationType.RELU:
                op.output = self._relu(op.inputs[0])
    
    def backward(self) -> None:
        """反向传播"""
        # 从最后一个操作开始反向传播
        for op in reversed(self.operations):
            if op.backward_fn:
                op.backward_fn()
    
    def _add(self, a: Tensor, b: Tensor) -> Tensor:
        """加法操作"""
        result = Tensor(a.data + b.data, requires_grad=a.requires_grad or b.requires_grad)
        
        if result.requires_grad:
            def backward():
                if a.requires_grad:
                    a.grad += result.grad
                if b.requires_grad:
                    b.grad += result.grad
            
            result.backward_fn = backward
        
        return result
    
    def _multiply(self, a: Tensor, b: Tensor) -> Tensor:
        """乘法操作"""
        result = Tensor(a.data * b.data, requires_grad=a.requires_grad or b.requires_grad)
        
        if result.requires_grad:
            def backward():
                if a.requires_grad:
                    a.grad += result.grad * b.data
                if b.requires_grad:
                    b.grad += result.grad * a.data
            
            result.backward_fn = backward
        
        return result
    
    def _relu(self, x: Tensor) -> Tensor:
        """ReLU操作"""
        result = Tensor(np.maximum(0, x.data), requires_grad=x.requires_grad)
        
        if result.requires_grad:
            def backward():
                if x.requires_grad:
                    x.grad += result.grad * (x.data > 0)
            
            result.backward_fn = backward
        
        return result

# 使用示例
def test_computational_graph():
    """测试计算图"""
    # 创建张量
    x = Tensor(np.array([2.0]), requires_grad=True)
    y = Tensor(np.array([3.0]), requires_grad=True)
    
    # 创建计算图
    graph = ComputationalGraph()
    
    # 构建计算：z = relu(x * y + 1)
    one = Tensor(np.array([1.0]))
    
    # x * y
    mul_op = Operation(OperationType.MULTIPLY, [x, y])
    graph.add_operation(mul_op)
    
    # (x * y) + 1
    add_op = Operation(OperationType.ADD, [mul_op.output, one])
    graph.add_operation(add_op)
    
    # relu((x * y) + 1)
    relu_op = Operation(OperationType.RELU, [add_op.output])
    graph.add_operation(relu_op)
    
    # 前向传播
    graph.forward()
    
    print(f"x: {x.data}, y: {y.data}")
    print(f"Result: {relu_op.output.data}")
    
    # 反向传播
    relu_op.output.grad = np.array([1.0])
    graph.backward()
    
    print(f"Gradient of x: {x.grad}")
    print(f"Gradient of y: {y.grad}")
```

## 5. 总结

本文档从形式化角度阐述了人工智能与机器学习的理论基础，包括机器学习定义、线性回归、神经网络、深度学习框架等内容。通过Python实现展示了这些理论的具体应用。

---

**相关链接**:
- [02.01 算法与数据结构理论](../02-理论基础/02.01-算法与数据结构理论.md)
- [05.02 微服务架构](../05-架构领域/05.02-微服务架构.md)
- [06.01 核心算法实现](../06-组件算法/06.01-核心算法实现.md) 