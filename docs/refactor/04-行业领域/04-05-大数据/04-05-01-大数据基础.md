# å¤§æ•°æ®åŸºç¡€ç†è®º

## ğŸ“‹ æ¦‚è¿°

å¤§æ•°æ®æ˜¯æŒ‡è§„æ¨¡åºå¤§ã€ç±»å‹å¤šæ ·ã€å¤„ç†é€Ÿåº¦å¿«çš„æ•°æ®é›†åˆï¼Œéœ€è¦ç‰¹æ®Šçš„æŠ€æœ¯å’Œæ–¹æ³•æ¥å¤„ç†ã€åˆ†æå’ŒæŒ–æ˜ã€‚æœ¬æ–‡æ¡£ä»å½¢å¼åŒ–ç†è®ºè§’åº¦é˜è¿°å¤§æ•°æ®çš„åŸºç¡€æ¦‚å¿µã€å¤„ç†æ¨¡å‹å’Œæ ¸å¿ƒç®—æ³•ã€‚

## 1. å½¢å¼åŒ–å®šä¹‰

### 1.1 å¤§æ•°æ®å®šä¹‰

**å®šä¹‰ 1.1** (å¤§æ•°æ®)
å¤§æ•°æ®æ˜¯ä¸€ä¸ªäº”å…ƒç»„ï¼š
$$\text{BigData} = (V, V, V, V, V)$$

å…¶ä¸­ï¼š
- $V_1$ æ˜¯æ•°æ®é‡ï¼ˆVolumeï¼‰
- $V_2$ æ˜¯æ•°æ®é€Ÿåº¦ï¼ˆVelocityï¼‰
- $V_3$ æ˜¯æ•°æ®å¤šæ ·æ€§ï¼ˆVarietyï¼‰
- $V_4$ æ˜¯æ•°æ®çœŸå®æ€§ï¼ˆVeracityï¼‰
- $V_5$ æ˜¯æ•°æ®ä»·å€¼ï¼ˆValueï¼‰

### 1.2 æ•°æ®å¤„ç†æ¨¡å‹

**å®šä¹‰ 1.2** (æ•°æ®å¤„ç†æ¨¡å‹)
æ•°æ®å¤„ç†æ¨¡å‹ï¼š
$$\text{DataProcessing} = (I, P, O, T)$$

å…¶ä¸­ï¼š
- $I$ æ˜¯è¾“å…¥æ•°æ®é›†åˆ
- $P$ æ˜¯å¤„ç†å‡½æ•°é›†åˆ
- $O$ æ˜¯è¾“å‡ºç»“æœé›†åˆ
- $T$ æ˜¯æ—¶é—´çº¦æŸ

### 1.3 MapReduceæ¨¡å‹

**å®šä¹‰ 1.3** (MapReduce)
MapReduceæ˜¯ä¸€ä¸ªä¸‰å…ƒç»„ï¼š
$$\text{MapReduce} = (\text{Map}, \text{Reduce}, \text{Shuffle})$$

å…¶ä¸­ï¼š
- $\text{Map}: K_1 \times V_1 \rightarrow K_2 \times V_2$
- $\text{Reduce}: K_2 \times [V_2] \rightarrow K_3 \times V_3$
- $\text{Shuffle}: [K_2 \times V_2] \rightarrow K_2 \times [V_2]$

## 2. æ ¸å¿ƒæ¦‚å¿µ

### 2.1 æ•°æ®æµå¤„ç†

**å®šä¹‰ 2.1** (æ•°æ®æµ)
æ•°æ®æµæ˜¯ä¸€ä¸ªæ—¶é—´åºåˆ—ï¼š
$$\text{DataStream} = \{(t_i, d_i) \mid i \in \mathbb{N}\}$$

å…¶ä¸­ $t_i$ æ˜¯æ—¶é—´æˆ³ï¼Œ$d_i$ æ˜¯æ•°æ®é¡¹ã€‚

### 2.2 åˆ†å¸ƒå¼è®¡ç®—

**å®šä¹‰ 2.2** (åˆ†å¸ƒå¼è®¡ç®—)
åˆ†å¸ƒå¼è®¡ç®—æ¨¡å‹ï¼š
$$\text{DistributedComputing} = (N, C, S)$$

å…¶ä¸­ï¼š
- $N$ æ˜¯èŠ‚ç‚¹é›†åˆ
- $C$ æ˜¯é€šä¿¡æ¨¡å¼
- $S$ æ˜¯åŒæ­¥ç­–ç•¥

## 3. Pythonå®ç°

### 3.1 MapReduceå®ç°

```python
from typing import List, Dict, Any, Callable, Iterator, Tuple
from dataclasses import dataclass
from abc import ABC, abstractmethod
import time
import threading
from collections import defaultdict

@dataclass
class KeyValue:
    """é”®å€¼å¯¹"""
    key: Any
    value: Any

class MapReduce:
    """MapReduceæ¡†æ¶"""
    
    def __init__(self, num_reducers: int = 4):
        self.num_reducers = num_reducers
        self.intermediate_data: Dict[int, List[KeyValue]] = defaultdict(list)
        self.final_results: Dict[Any, Any] = {}
    
    def map_reduce(self, data: List[Any], 
                   map_func: Callable[[Any], List[KeyValue]],
                   reduce_func: Callable[[Any, List[Any]], Any]) -> Dict[Any, Any]:
        """
        æ‰§è¡ŒMapReduce
        
        å‚æ•°:
            data: è¾“å…¥æ•°æ®
            map_func: Mapå‡½æ•°
            reduce_func: Reduceå‡½æ•°
            
        è¿”å›:
            å¤„ç†ç»“æœ
        """
        # Mapé˜¶æ®µ
        mapped_data = self._map_phase(data, map_func)
        
        # Shuffleé˜¶æ®µ
        shuffled_data = self._shuffle_phase(mapped_data)
        
        # Reduceé˜¶æ®µ
        results = self._reduce_phase(shuffled_data, reduce_func)
        
        return results
    
    def _map_phase(self, data: List[Any], map_func: Callable) -> List[KeyValue]:
        """Mapé˜¶æ®µ"""
        mapped_data = []
        for item in data:
            mapped_items = map_func(item)
            mapped_data.extend(mapped_items)
        return mapped_data
    
    def _shuffle_phase(self, mapped_data: List[KeyValue]) -> Dict[Any, List[Any]]:
        """Shuffleé˜¶æ®µ"""
        shuffled = defaultdict(list)
        for kv in mapped_data:
            shuffled[kv.key].append(kv.value)
        return dict(shuffled)
    
    def _reduce_phase(self, shuffled_data: Dict[Any, List[Any]], 
                     reduce_func: Callable) -> Dict[Any, Any]:
        """Reduceé˜¶æ®µ"""
        results = {}
        for key, values in shuffled_data.items():
            result = reduce_func(key, values)
            results[key] = result
        return results

# ç¤ºä¾‹ï¼šè¯é¢‘ç»Ÿè®¡
def word_count_map(text: str) -> List[KeyValue]:
    """è¯é¢‘ç»Ÿè®¡Mapå‡½æ•°"""
    words = text.lower().split()
    return [KeyValue(word, 1) for word in words]

def word_count_reduce(word: str, counts: List[int]) -> int:
    """è¯é¢‘ç»Ÿè®¡Reduceå‡½æ•°"""
    return sum(counts)

# ä½¿ç”¨ç¤ºä¾‹
def word_count_example():
    """è¯é¢‘ç»Ÿè®¡ç¤ºä¾‹"""
    # è¾“å…¥æ•°æ®
    texts = [
        "hello world hello",
        "world hello python",
        "python is great",
        "hello python world"
    ]
    
    # åˆ›å»ºMapReduceå®ä¾‹
    mr = MapReduce()
    
    # æ‰§è¡ŒMapReduce
    results = mr.map_reduce(texts, word_count_map, word_count_reduce)
    
    print("è¯é¢‘ç»Ÿè®¡ç»“æœ:")
    for word, count in sorted(results.items()):
        print(f"{word}: {count}")

if __name__ == "__main__":
    word_count_example()
```

### 3.2 æ•°æ®æµå¤„ç†å®ç°

```python
class DataStreamProcessor:
    """æ•°æ®æµå¤„ç†å™¨"""
    
    def __init__(self, window_size: int = 100):
        self.window_size = window_size
        self.data_buffer: List[Tuple[float, Any]] = []
        self.processors: List[StreamProcessor] = []
    
    def add_processor(self, processor: 'StreamProcessor'):
        """æ·»åŠ å¤„ç†å™¨"""
        self.processors.append(processor)
    
    def process_stream(self, data_stream: Iterator[Tuple[float, Any]]):
        """å¤„ç†æ•°æ®æµ"""
        for timestamp, data in data_stream:
            # æ·»åŠ æ•°æ®åˆ°ç¼“å†²åŒº
            self.data_buffer.append((timestamp, data))
            
            # ç»´æŠ¤æ»‘åŠ¨çª—å£
            self._maintain_window(timestamp)
            
            # å¤„ç†å½“å‰çª—å£æ•°æ®
            window_data = [d for _, d in self.data_buffer]
            for processor in self.processors:
                processor.process(window_data, timestamp)
    
    def _maintain_window(self, current_time: float):
        """ç»´æŠ¤æ»‘åŠ¨çª—å£"""
        cutoff_time = current_time - self.window_size
        self.data_buffer = [
            (t, d) for t, d in self.data_buffer 
            if t >= cutoff_time
        ]

class StreamProcessor(ABC):
    """æµå¤„ç†å™¨æŠ½è±¡åŸºç±»"""
    
    @abstractmethod
    def process(self, data: List[Any], timestamp: float):
        """å¤„ç†æ•°æ®"""
        pass

class AverageProcessor(StreamProcessor):
    """å¹³å‡å€¼å¤„ç†å™¨"""
    
    def process(self, data: List[Any], timestamp: float):
        """è®¡ç®—å¹³å‡å€¼"""
        if data:
            avg = sum(data) / len(data)
            print(f"æ—¶é—´ {timestamp}: å¹³å‡å€¼ = {avg:.2f}")

class MaxProcessor(StreamProcessor):
    """æœ€å¤§å€¼å¤„ç†å™¨"""
    
    def process(self, data: List[Any], timestamp: float):
        """è®¡ç®—æœ€å¤§å€¼"""
        if data:
            max_val = max(data)
            print(f"æ—¶é—´ {timestamp}: æœ€å¤§å€¼ = {max_val}")
```

### 3.3 åˆ†å¸ƒå¼è®¡ç®—å®ç°

```python
class DistributedNode:
    """åˆ†å¸ƒå¼èŠ‚ç‚¹"""
    
    def __init__(self, node_id: str):
        self.node_id = node_id
        self.data: Dict[str, Any] = {}
        self.neighbors: List[str] = []
        self.status = "active"
    
    def store_data(self, key: str, value: Any):
        """å­˜å‚¨æ•°æ®"""
        self.data[key] = value
    
    def get_data(self, key: str) -> Any:
        """è·å–æ•°æ®"""
        return self.data.get(key)
    
    def add_neighbor(self, neighbor_id: str):
        """æ·»åŠ é‚»å±…èŠ‚ç‚¹"""
        if neighbor_id not in self.neighbors:
            self.neighbors.append(neighbor_id)

class DistributedSystem:
    """åˆ†å¸ƒå¼ç³»ç»Ÿ"""
    
    def __init__(self):
        self.nodes: Dict[str, DistributedNode] = {}
        self.communication_pattern = "broadcast"
    
    def add_node(self, node_id: str) -> DistributedNode:
        """æ·»åŠ èŠ‚ç‚¹"""
        node = DistributedNode(node_id)
        self.nodes[node_id] = node
        return node
    
    def broadcast(self, message: Any, sender_id: str):
        """å¹¿æ’­æ¶ˆæ¯"""
        for node_id, node in self.nodes.items():
            if node_id != sender_id and node.status == "active":
                node.store_data(f"message_{time.time()}", message)
    
    def distributed_computation(self, computation_func: Callable, 
                               data_keys: List[str]) -> Dict[str, Any]:
        """åˆ†å¸ƒå¼è®¡ç®—"""
        results = {}
        
        # åœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸Šæ‰§è¡Œè®¡ç®—
        for node_id, node in self.nodes.items():
            if node.status == "active":
                node_data = {key: node.get_data(key) for key in data_keys}
                result = computation_func(node_data)
                results[node_id] = result
        
        return results

# ç¤ºä¾‹ï¼šåˆ†å¸ƒå¼æ±‚å’Œ
def distributed_sum(data: Dict[str, Any]) -> float:
    """åˆ†å¸ƒå¼æ±‚å’Œå‡½æ•°"""
    return sum(value for value in data.values() if isinstance(value, (int, float)))

def distributed_computation_example():
    """åˆ†å¸ƒå¼è®¡ç®—ç¤ºä¾‹"""
    # åˆ›å»ºåˆ†å¸ƒå¼ç³»ç»Ÿ
    system = DistributedSystem()
    
    # æ·»åŠ èŠ‚ç‚¹
    nodes = ["node1", "node2", "node3", "node4"]
    for node_id in nodes:
        system.add_node(node_id)
    
    # å­˜å‚¨æ•°æ®
    for i, node_id in enumerate(nodes):
        node = system.nodes[node_id]
        node.store_data("value", i + 1)
    
    # æ‰§è¡Œåˆ†å¸ƒå¼è®¡ç®—
    results = system.distributed_computation(distributed_sum, ["value"])
    
    print("åˆ†å¸ƒå¼è®¡ç®—ç»“æœ:")
    for node_id, result in results.items():
        print(f"{node_id}: {result}")
    
    # è®¡ç®—æ€»å’Œ
    total_sum = sum(results.values())
    print(f"æ€»å’Œ: {total_sum}")

if __name__ == "__main__":
    distributed_computation_example()
```

## 4. ç†è®ºè¯æ˜

### 4.1 MapReduceæ­£ç¡®æ€§

**å®šç† 4.1** (MapReduceæ­£ç¡®æ€§)
MapReduceç®—æ³•èƒ½å¤Ÿæ­£ç¡®è®¡ç®—åˆ†å¸ƒå¼æ•°æ®ã€‚

**è¯æ˜**:
1. **Mapé˜¶æ®µ**: æ¯ä¸ªæ•°æ®é¡¹è¢«ç‹¬ç«‹å¤„ç†
2. **Shuffleé˜¶æ®µ**: ç›¸åŒé”®çš„æ•°æ®è¢«åˆ†ç»„
3. **Reduceé˜¶æ®µ**: æ¯ä¸ªé”®çš„æ‰€æœ‰å€¼è¢«èšåˆ
4. **ç»“è®º**: ç»“æœä¸ä¸²è¡Œå¤„ç†ç­‰ä»·

### 4.2 æ•°æ®æµå¤„ç†å®æ—¶æ€§

**å®šç† 4.2** (æ•°æ®æµå¤„ç†å®æ—¶æ€§)
æ»‘åŠ¨çª—å£ç®—æ³•èƒ½å¤Ÿåœ¨å¸¸æ•°æ—¶é—´å†…å¤„ç†æ–°æ•°æ®ã€‚

**è¯æ˜**:
1. **çª—å£ç»´æŠ¤**: $O(1)$ æ—¶é—´æ·»åŠ æ–°æ•°æ®
2. **è¿‡æœŸæ•°æ®æ¸…ç†**: $O(w)$ æ—¶é—´ï¼Œå…¶ä¸­ $w$ æ˜¯çª—å£å¤§å°
3. **å¤„ç†æ—¶é—´**: $O(1)$ æ—¶é—´è®¡ç®—ç»Ÿè®¡é‡
4. **ç»“è®º**: æ€»ä½“æ—¶é—´å¤æ‚åº¦ä¸º $O(w)$

## 5. æ€§èƒ½åˆ†æ

### 5.1 æ—¶é—´å¤æ‚åº¦

- **Mapé˜¶æ®µ**: $O(n)$ å…¶ä¸­ $n$ æ˜¯æ•°æ®é‡
- **Shuffleé˜¶æ®µ**: $O(n \log n)$ æ’åºå’Œåˆ†ç»„
- **Reduceé˜¶æ®µ**: $O(m)$ å…¶ä¸­ $m$ æ˜¯å”¯ä¸€é”®æ•°
- **æ•°æ®æµå¤„ç†**: $O(w)$ å…¶ä¸­ $w$ æ˜¯çª—å£å¤§å°

### 5.2 ç©ºé—´å¤æ‚åº¦

- **MapReduce**: $O(n)$ å­˜å‚¨ä¸­é—´ç»“æœ
- **æ•°æ®æµå¤„ç†**: $O(w)$ å­˜å‚¨çª—å£æ•°æ®
- **åˆ†å¸ƒå¼è®¡ç®—**: $O(n)$ å­˜å‚¨èŠ‚ç‚¹æ•°æ®

## 6. åº”ç”¨ç¤ºä¾‹

### 6.1 å®æ—¶æ—¥å¿—åˆ†æ

```python
def real_time_log_analysis():
    """å®æ—¶æ—¥å¿—åˆ†æ"""
    # åˆ›å»ºæ•°æ®æµå¤„ç†å™¨
    processor = DataStreamProcessor(window_size=60)  # 60ç§’çª—å£
    
    # æ·»åŠ å¤„ç†å™¨
    processor.add_processor(AverageProcessor())
    processor.add_processor(MaxProcessor())
    
    # æ¨¡æ‹Ÿæ—¥å¿—æ•°æ®æµ
    import random
    def log_stream():
        for i in range(100):
            timestamp = time.time() + i
            log_level = random.choice(["INFO", "WARNING", "ERROR"])
            response_time = random.uniform(10, 1000)
            yield (timestamp, response_time)
    
    # å¤„ç†æ•°æ®æµ
    processor.process_stream(log_stream())

if __name__ == "__main__":
    real_time_log_analysis()
```

## 7. æ€»ç»“

æœ¬æ–‡æ¡£ä»å½¢å¼åŒ–ç†è®ºè§’åº¦é˜è¿°äº†å¤§æ•°æ®çš„æ ¸å¿ƒæ¦‚å¿µï¼š

1. **å½¢å¼åŒ–å®šä¹‰**: å¤§æ•°æ®å’Œå¤„ç†æ¨¡å‹çš„æ•°å­¦å®šä¹‰
2. **æ ¸å¿ƒæ¦‚å¿µ**: æ•°æ®æµå¤„ç†å’Œåˆ†å¸ƒå¼è®¡ç®—
3. **Pythonå®ç°**: MapReduceã€æ•°æ®æµå¤„ç†ã€åˆ†å¸ƒå¼è®¡ç®—
4. **ç†è®ºè¯æ˜**: ç®—æ³•æ­£ç¡®æ€§å’Œæ€§èƒ½åˆ†æ
5. **åº”ç”¨ç¤ºä¾‹**: å®æ—¶æ—¥å¿—åˆ†æ

å¤§æ•°æ®æŠ€æœ¯ä¸ºå¤„ç†æµ·é‡æ•°æ®æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡åˆ†å¸ƒå¼è®¡ç®—å’Œæµå¤„ç†æŠ€æœ¯ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°åˆ†æå’ŒæŒ–æ˜æ•°æ®ä»·å€¼ã€‚

---

*æœ¬æ–‡æ¡£æ˜¯è½¯ä»¶å·¥ç¨‹ä¸è®¡ç®—ç§‘å­¦çŸ¥è¯†ä½“ç³»é‡æ„é¡¹ç›®çš„ä¸€éƒ¨åˆ†ï¼Œéµå¾ªä¸¥æ ¼çš„å½¢å¼åŒ–è§„èŒƒå’Œå­¦æœ¯æ ‡å‡†ã€‚* 