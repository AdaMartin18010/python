# åˆ†å¸ƒå¼æ•°æ®ç®¡ç†

## ğŸ“‹ æ¦‚è¿°

åˆ†å¸ƒå¼æ•°æ®ç®¡ç†æ˜¯åˆ†å¸ƒå¼ç³»ç»Ÿçš„æ ¸å¿ƒç»„ä»¶ï¼Œè´Ÿè´£æ•°æ®çš„å­˜å‚¨ã€å¤åˆ¶ã€åˆ†ç‰‡å’Œä¸€è‡´æ€§ä¿è¯ã€‚æœ¬æ–‡æ¡£ä»‹ç»åˆ†å¸ƒå¼æ•°æ®ç®¡ç†çš„ç†è®ºåŸºç¡€ã€å®ç°æ–¹æ³•å’Œæœ€ä½³å®è·µã€‚

## 1. å½¢å¼åŒ–å®šä¹‰

### 1.1 åˆ†å¸ƒå¼æ•°æ®æ¨¡å‹

**å®šä¹‰ 1.1** (åˆ†å¸ƒå¼æ•°æ®æ¨¡å‹)
åˆ†å¸ƒå¼æ•°æ®æ¨¡å‹æ˜¯ä¸€ä¸ªå…­å…ƒç»„ $\mathcal{D} = (K, V, N, R, C, T)$ï¼Œå…¶ä¸­ï¼š

- $K$ æ˜¯é”®ç©ºé—´
- $V$ æ˜¯å€¼ç©ºé—´
- $N = \{n_1, n_2, \ldots, n_k\}$ æ˜¯èŠ‚ç‚¹é›†åˆ
- $R: K \rightarrow 2^N$ æ˜¯å¤åˆ¶æ˜ å°„å‡½æ•°
- $C: K \rightarrow N$ æ˜¯åˆ†ç‰‡æ˜ å°„å‡½æ•°
- $T: K \times V \rightarrow \mathbb{R}$ æ˜¯æ—¶é—´æˆ³å‡½æ•°

### 1.2 æ•°æ®ä¸€è‡´æ€§æ¨¡å‹

**å®šä¹‰ 1.2** (ä¸€è‡´æ€§æ¨¡å‹)
ä¸€è‡´æ€§æ¨¡å‹æ˜¯ä¸€ä¸ªå››å…ƒç»„ $\mathcal{C} = (S, O, \sim, \rightarrow)$ï¼Œå…¶ä¸­ï¼š

- $S$ æ˜¯çŠ¶æ€ç©ºé—´
- $O$ æ˜¯æ“ä½œé›†åˆ
- $\sim$ æ˜¯ç­‰ä»·å…³ç³»
- $\rightarrow$ æ˜¯çŠ¶æ€è½¬æ¢å…³ç³»

**å®šä¹‰ 1.3** (å¼ºä¸€è‡´æ€§)
å¯¹äºä»»æ„ä¸¤ä¸ªæ“ä½œ $o_1, o_2$ï¼Œå¦‚æœ $o_1 \rightarrow o_2$ï¼Œåˆ™æ‰€æœ‰èŠ‚ç‚¹çœ‹åˆ°çš„çŠ¶æ€æ»¡è¶³ $s_1 \sim s_2$ã€‚

**å®šä¹‰ 1.4** (æœ€ç»ˆä¸€è‡´æ€§)
å­˜åœ¨ä¸€ä¸ªæ—¶é—´ç‚¹ $t$ï¼Œä½¿å¾—å¯¹äºä»»æ„èŠ‚ç‚¹ $n$ï¼Œåœ¨ $t$ ä¹‹åçš„çŠ¶æ€éƒ½æ”¶æ•›åˆ°ç›¸åŒå€¼ã€‚

### 1.3 æ•°æ®åˆ†ç‰‡ç†è®º

**å®šä¹‰ 1.5** (æ•°æ®åˆ†ç‰‡)
æ•°æ®åˆ†ç‰‡æ˜¯ä¸€ä¸ªä¸‰å…ƒç»„ $\mathcal{P} = (K, N, h)$ï¼Œå…¶ä¸­ï¼š

- $K$ æ˜¯é”®ç©ºé—´
- $N$ æ˜¯èŠ‚ç‚¹é›†åˆ
- $h: K \rightarrow N$ æ˜¯å“ˆå¸Œå‡½æ•°

**å®šç† 1.1** (åˆ†ç‰‡å‡åŒ€æ€§)
å¦‚æœå“ˆå¸Œå‡½æ•° $h$ æ˜¯å‡åŒ€çš„ï¼Œåˆ™æ•°æ®åˆ†ç‰‡çš„è´Ÿè½½åˆ†å¸ƒæœŸæœ›ä¸º $\frac{|K|}{|N|}$ã€‚

**è¯æ˜**:

1. å¯¹äºå‡åŒ€å“ˆå¸Œå‡½æ•°ï¼Œæ¯ä¸ªé”®æ˜ å°„åˆ°ä»»æ„èŠ‚ç‚¹çš„æ¦‚ç‡ç›¸ç­‰
2. æœŸæœ›è´Ÿè½½ = æ€»é”®æ•° Ã— æ¯ä¸ªèŠ‚ç‚¹çš„æ¦‚ç‡
3. æ¯ä¸ªèŠ‚ç‚¹çš„æ¦‚ç‡ = $\frac{1}{|N|}$
4. å› æ­¤æœŸæœ›è´Ÿè½½ = $\frac{|K|}{|N|}$

## 2. Pythonå®ç°

### 2.1 åˆ†å¸ƒå¼æ•°æ®å­˜å‚¨

```python
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from typing import Dict, List, Set, Optional, Any, Tuple, Callable
from enum import Enum
import hashlib
import time
import uuid
import json
from collections import defaultdict
import asyncio

class DataConsistency(Enum):
    """æ•°æ®ä¸€è‡´æ€§çº§åˆ«"""
    STRONG = "strong"
    EVENTUAL = "eventual"
    WEAK = "weak"

class OperationType(Enum):
    """æ“ä½œç±»å‹"""
    READ = "read"
    WRITE = "write"
    DELETE = "delete"

@dataclass
class DataEntry:
    """æ•°æ®æ¡ç›®"""
    key: str
    value: Any
    timestamp: float
    version: int
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class DataOperation:
    """æ•°æ®æ“ä½œ"""
    operation_id: str
    operation_type: OperationType
    key: str
    value: Optional[Any] = None
    timestamp: float = field(default_factory=time.time)
    client_id: str = ""
    consistency_level: DataConsistency = DataConsistency.EVENTUAL

class ConsistentHashRing:
    """ä¸€è‡´æ€§å“ˆå¸Œç¯"""
    
    def __init__(self, virtual_nodes: int = 150):
        self.virtual_nodes = virtual_nodes
        self.ring: Dict[int, str] = {}
        self.nodes: Set[str] = set()
        
    def add_node(self, node_id: str) -> None:
        """æ·»åŠ èŠ‚ç‚¹"""
        self.nodes.add(node_id)
        for i in range(self.virtual_nodes):
            virtual_key = f"{node_id}-{i}"
            hash_value = self._hash(virtual_key)
            self.ring[hash_value] = node_id
            
    def remove_node(self, node_id: str) -> None:
        """ç§»é™¤èŠ‚ç‚¹"""
        self.nodes.discard(node_id)
        # ç§»é™¤æ‰€æœ‰è™šæ‹ŸèŠ‚ç‚¹
        keys_to_remove = []
        for hash_value, node in self.ring.items():
            if node == node_id:
                keys_to_remove.append(hash_value)
        for key in keys_to_remove:
            del self.ring[key]
            
    def get_node(self, key: str) -> str:
        """è·å–è´Ÿè´£çš„èŠ‚ç‚¹"""
        if not self.ring:
            raise ValueError("å“ˆå¸Œç¯ä¸ºç©º")
            
        hash_value = self._hash(key)
        sorted_hashes = sorted(self.ring.keys())
        
        # æ‰¾åˆ°ç¬¬ä¸€ä¸ªå¤§äºç­‰äºhash_valueçš„èŠ‚ç‚¹
        for h in sorted_hashes:
            if h >= hash_value:
                return self.ring[h]
        
        # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œè¿”å›ç¬¬ä¸€ä¸ªèŠ‚ç‚¹ï¼ˆç¯çš„èµ·ç‚¹ï¼‰
        return self.ring[sorted_hashes[0]]
        
    def get_replicas(self, key: str, num_replicas: int) -> List[str]:
        """è·å–å‰¯æœ¬èŠ‚ç‚¹"""
        primary_node = self.get_node(key)
        replicas = [primary_node]
        
        # è·å–åç»­èŠ‚ç‚¹ä½œä¸ºå‰¯æœ¬
        sorted_hashes = sorted(self.ring.keys())
        primary_hash = None
        for h, node in self.ring.items():
            if node == primary_node:
                primary_hash = h
                break
                
        if primary_hash is not None:
            start_idx = sorted_hashes.index(primary_hash)
            for i in range(1, num_replicas):
                idx = (start_idx + i) % len(sorted_hashes)
                replica_node = self.ring[sorted_hashes[idx]]
                if replica_node not in replicas:
                    replicas.append(replica_node)
                    
        return replicas[:num_replicas]
        
    def _hash(self, key: str) -> int:
        """è®¡ç®—å“ˆå¸Œå€¼"""
        return int(hashlib.md5(key.encode()).hexdigest(), 16)

class DistributedDataStore:
    """åˆ†å¸ƒå¼æ•°æ®å­˜å‚¨"""
    
    def __init__(self, replication_factor: int = 3):
        self.replication_factor = replication_factor
        self.hash_ring = ConsistentHashRing()
        self.data_stores: Dict[str, Dict[str, DataEntry]] = defaultdict(dict)
        self.operation_log: List[DataOperation] = []
        self.consistency_level = DataConsistency.EVENTUAL
        
    def add_node(self, node_id: str) -> None:
        """æ·»åŠ èŠ‚ç‚¹"""
        self.hash_ring.add_node(node_id)
        self.data_stores[node_id] = {}
        
    def remove_node(self, node_id: str) -> None:
        """ç§»é™¤èŠ‚ç‚¹"""
        self.hash_ring.remove_node(node_id)
        # é‡æ–°åˆ†é…æ•°æ®
        self._redistribute_data(node_id)
        
    def put(self, key: str, value: Any, consistency: DataConsistency = None) -> bool:
        """å­˜å‚¨æ•°æ®"""
        if consistency is None:
            consistency = self.consistency_level
            
        operation = DataOperation(
            operation_id=str(uuid.uuid4()),
            operation_type=OperationType.WRITE,
            key=key,
            value=value,
            consistency_level=consistency
        )
        
        # è·å–å‰¯æœ¬èŠ‚ç‚¹
        replica_nodes = self.hash_ring.get_replicas(key, self.replication_factor)
        
        # æ ¹æ®ä¸€è‡´æ€§çº§åˆ«å†³å®šå†™å…¥ç­–ç•¥
        if consistency == DataConsistency.STRONG:
            return self._strong_write(operation, replica_nodes)
        else:
            return self._eventual_write(operation, replica_nodes)
            
    def get(self, key: str, consistency: DataConsistency = None) -> Optional[Any]:
        """è·å–æ•°æ®"""
        if consistency is None:
            consistency = self.consistency_level
            
        # è·å–å‰¯æœ¬èŠ‚ç‚¹
        replica_nodes = self.hash_ring.get_replicas(key, self.replication_factor)
        
        if consistency == DataConsistency.STRONG:
            return self._strong_read(key, replica_nodes)
        else:
            return self._eventual_read(key, replica_nodes)
            
    def delete(self, key: str, consistency: DataConsistency = None) -> bool:
        """åˆ é™¤æ•°æ®"""
        if consistency is None:
            consistency = self.consistency_level
            
        operation = DataOperation(
            operation_id=str(uuid.uuid4()),
            operation_type=OperationType.DELETE,
            key=key,
            consistency_level=consistency
        )
        
        replica_nodes = self.hash_ring.get_replicas(key, self.replication_factor)
        
        if consistency == DataConsistency.STRONG:
            return self._strong_delete(operation, replica_nodes)
        else:
            return self._eventual_delete(operation, replica_nodes)
            
    def _strong_write(self, operation: DataOperation, replica_nodes: List[str]) -> bool:
        """å¼ºä¸€è‡´æ€§å†™å…¥"""
        # éœ€è¦å¤šæ•°èŠ‚ç‚¹ç¡®è®¤
        quorum_size = (len(replica_nodes) // 2) + 1
        successful_writes = 0
        
        for node_id in replica_nodes:
            if node_id in self.data_stores:
                entry = DataEntry(
                    key=operation.key,
                    value=operation.value,
                    timestamp=operation.timestamp,
                    version=len(self.data_stores[node_id]) + 1
                )
                self.data_stores[node_id][operation.key] = entry
                successful_writes += 1
                
        self.operation_log.append(operation)
        return successful_writes >= quorum_size
        
    def _eventual_write(self, operation: DataOperation, replica_nodes: List[str]) -> bool:
        """æœ€ç»ˆä¸€è‡´æ€§å†™å…¥"""
        # å¼‚æ­¥å†™å…¥æ‰€æœ‰å‰¯æœ¬
        for node_id in replica_nodes:
            if node_id in self.data_stores:
                entry = DataEntry(
                    key=operation.key,
                    value=operation.value,
                    timestamp=operation.timestamp,
                    version=len(self.data_stores[node_id]) + 1
                )
                self.data_stores[node_id][operation.key] = entry
                
        self.operation_log.append(operation)
        return True
        
    def _strong_read(self, key: str, replica_nodes: List[str]) -> Optional[Any]:
        """å¼ºä¸€è‡´æ€§è¯»å–"""
        # ä»å¤šæ•°èŠ‚ç‚¹è¯»å–ï¼Œé€‰æ‹©æœ€æ–°ç‰ˆæœ¬
        quorum_size = (len(replica_nodes) // 2) + 1
        entries = []
        
        for node_id in replica_nodes:
            if node_id in self.data_stores and key in self.data_stores[node_id]:
                entries.append(self.data_stores[node_id][key])
                
        if len(entries) >= quorum_size:
            # é€‰æ‹©æœ€æ–°ç‰ˆæœ¬
            latest_entry = max(entries, key=lambda x: x.timestamp)
            return latest_entry.value
        return None
        
    def _eventual_read(self, key: str, replica_nodes: List[str]) -> Optional[Any]:
        """æœ€ç»ˆä¸€è‡´æ€§è¯»å–"""
        # ä»ä»»æ„å¯ç”¨èŠ‚ç‚¹è¯»å–
        for node_id in replica_nodes:
            if node_id in self.data_stores and key in self.data_stores[node_id]:
                return self.data_stores[node_id][key].value
        return None
        
    def _strong_delete(self, operation: DataOperation, replica_nodes: List[str]) -> bool:
        """å¼ºä¸€è‡´æ€§åˆ é™¤"""
        quorum_size = (len(replica_nodes) // 2) + 1
        successful_deletes = 0
        
        for node_id in replica_nodes:
            if node_id in self.data_stores and operation.key in self.data_stores[node_id]:
                del self.data_stores[node_id][operation.key]
                successful_deletes += 1
                
        self.operation_log.append(operation)
        return successful_deletes >= quorum_size
        
    def _eventual_delete(self, operation: DataOperation, replica_nodes: List[str]) -> bool:
        """æœ€ç»ˆä¸€è‡´æ€§åˆ é™¤"""
        for node_id in replica_nodes:
            if node_id in self.data_stores and operation.key in self.data_stores[node_id]:
                del self.data_stores[node_id][operation.key]
                
        self.operation_log.append(operation)
        return True
        
    def _redistribute_data(self, removed_node: str) -> None:
        """é‡æ–°åˆ†é…æ•°æ®"""
        # è·å–è¢«ç§»é™¤èŠ‚ç‚¹çš„æ•°æ®
        removed_data = self.data_stores.get(removed_node, {})
        
        for key, entry in removed_data.items():
            # é‡æ–°è®¡ç®—è´Ÿè´£èŠ‚ç‚¹
            new_replicas = self.hash_ring.get_replicas(key, self.replication_factor)
            
            # å¤åˆ¶åˆ°æ–°èŠ‚ç‚¹
            for node_id in new_replicas:
                if node_id != removed_node and node_id in self.data_stores:
                    self.data_stores[node_id][key] = entry
                    
        # æ¸…ç†è¢«ç§»é™¤èŠ‚ç‚¹çš„æ•°æ®
        if removed_node in self.data_stores:
            del self.data_stores[removed_node]
```

### 2.2 æ•°æ®åŒæ­¥æœºåˆ¶

```python
class DataSynchronizer:
    """æ•°æ®åŒæ­¥å™¨"""
    
    def __init__(self, data_store: DistributedDataStore):
        self.data_store = data_store
        self.sync_interval = 5.0  # ç§’
        self.version_vectors: Dict[str, Dict[str, int]] = defaultdict(lambda: defaultdict(int))
        
    def get_version_vector(self, node_id: str) -> Dict[str, int]:
        """è·å–ç‰ˆæœ¬å‘é‡"""
        return self.version_vectors[node_id].copy()
        
    def update_version_vector(self, node_id: str, key: str, version: int) -> None:
        """æ›´æ–°ç‰ˆæœ¬å‘é‡"""
        self.version_vectors[node_id][key] = max(
            self.version_vectors[node_id][key], version
        )
        
    def detect_conflicts(self, node1_id: str, node2_id: str) -> List[str]:
        """æ£€æµ‹å†²çª"""
        conflicts = []
        vector1 = self.version_vectors[node1_id]
        vector2 = self.version_vectors[node2_id]
        
        all_keys = set(vector1.keys()) | set(vector2.keys())
        
        for key in all_keys:
            v1 = vector1.get(key, 0)
            v2 = vector2.get(key, 0)
            
            # å¦‚æœä¸¤ä¸ªç‰ˆæœ¬éƒ½ä¸ä¸º0ä¸”ä¸ç›¸ç­‰ï¼Œåˆ™å­˜åœ¨å†²çª
            if v1 > 0 and v2 > 0 and v1 != v2:
                conflicts.append(key)
                
        return conflicts
        
    def resolve_conflict(self, key: str, node1_id: str, node2_id: str) -> Any:
        """è§£å†³å†²çª"""
        # ç®€å•çš„å†²çªè§£å†³ç­–ç•¥ï¼šé€‰æ‹©æ—¶é—´æˆ³æœ€æ–°çš„
        entry1 = self.data_store.data_stores[node1_id].get(key)
        entry2 = self.data_store.data_stores[node2_id].get(key)
        
        if entry1 and entry2:
            if entry1.timestamp > entry2.timestamp:
                return entry1.value
            else:
                return entry2.value
        elif entry1:
            return entry1.value
        elif entry2:
            return entry2.value
        else:
            return None
            
    def sync_data(self, source_node: str, target_node: str) -> Dict[str, Any]:
        """åŒæ­¥æ•°æ®"""
        sync_result = {
            "synced_keys": 0,
            "conflicts": [],
            "errors": []
        }
        
        try:
            # æ£€æµ‹å†²çª
            conflicts = self.detect_conflicts(source_node, target_node)
            sync_result["conflicts"] = conflicts
            
            # åŒæ­¥æ•°æ®
            source_data = self.data_store.data_stores[source_node]
            target_data = self.data_store.data_stores[target_node]
            
            for key, entry in source_data.items():
                if key not in target_data or target_data[key].version < entry.version:
                    target_data[key] = entry
                    self.update_version_vector(target_node, key, entry.version)
                    sync_result["synced_keys"] += 1
                    
            # è§£å†³å†²çª
            for key in conflicts:
                resolved_value = self.resolve_conflict(key, source_node, target_node)
                if resolved_value is not None:
                    entry = DataEntry(
                        key=key,
                        value=resolved_value,
                        timestamp=time.time(),
                        version=max(
                            self.version_vectors[source_node][key],
                            self.version_vectors[target_node][key]
                        ) + 1
                    )
                    target_data[key] = entry
                    self.update_version_vector(target_node, key, entry.version)
                    
        except Exception as e:
            sync_result["errors"].append(str(e))
            
        return sync_result
```

### 2.3 æ•°æ®å¤‡ä»½å’Œæ¢å¤

```python
class DataBackupManager:
    """æ•°æ®å¤‡ä»½ç®¡ç†å™¨"""
    
    def __init__(self, data_store: DistributedDataStore):
        self.data_store = data_store
        self.backup_interval = 3600  # 1å°æ—¶
        self.backup_retention = 7  # ä¿ç•™7å¤©
        self.backups: List[Dict[str, Any]] = []
        
    def create_backup(self) -> str:
        """åˆ›å»ºå¤‡ä»½"""
        backup_id = str(uuid.uuid4())
        backup_data = {
            "backup_id": backup_id,
            "timestamp": time.time(),
            "data": {},
            "metadata": {
                "node_count": len(self.data_store.data_stores),
                "total_keys": sum(len(store) for store in self.data_store.data_stores.values())
            }
        }
        
        # å¤åˆ¶æ‰€æœ‰æ•°æ®
        for node_id, store in self.data_store.data_stores.items():
            backup_data["data"][node_id] = {
                key: {
                    "value": entry.value,
                    "timestamp": entry.timestamp,
                    "version": entry.version,
                    "metadata": entry.metadata
                }
                for key, entry in store.items()
            }
            
        self.backups.append(backup_data)
        
        # æ¸…ç†æ—§å¤‡ä»½
        self._cleanup_old_backups()
        
        return backup_id
        
    def restore_backup(self, backup_id: str) -> bool:
        """æ¢å¤å¤‡ä»½"""
        backup = None
        for b in self.backups:
            if b["backup_id"] == backup_id:
                backup = b
                break
                
        if not backup:
            return False
            
        try:
            # æ¸…ç©ºå½“å‰æ•°æ®
            self.data_store.data_stores.clear()
            
            # æ¢å¤æ•°æ®
            for node_id, node_data in backup["data"].items():
                self.data_store.data_stores[node_id] = {}
                for key, entry_data in node_data.items():
                    entry = DataEntry(
                        key=key,
                        value=entry_data["value"],
                        timestamp=entry_data["timestamp"],
                        version=entry_data["version"],
                        metadata=entry_data["metadata"]
                    )
                    self.data_store.data_stores[node_id][key] = entry
                    
            return True
        except Exception:
            return False
            
    def _cleanup_old_backups(self) -> None:
        """æ¸…ç†æ—§å¤‡ä»½"""
        current_time = time.time()
        cutoff_time = current_time - (self.backup_retention * 24 * 3600)
        
        self.backups = [
            backup for backup in self.backups
            if backup["timestamp"] > cutoff_time
        ]
        
    def get_backup_info(self) -> List[Dict[str, Any]]:
        """è·å–å¤‡ä»½ä¿¡æ¯"""
        return [
            {
                "backup_id": backup["backup_id"],
                "timestamp": backup["timestamp"],
                "size": len(str(backup["data"])),
                "metadata": backup["metadata"]
            }
            for backup in self.backups
        ]
```

## 3. ç†è®ºè¯æ˜

### 3.1 ä¸€è‡´æ€§å“ˆå¸Œæ€§è´¨

**å®šç† 3.1** (ä¸€è‡´æ€§å“ˆå¸Œå¹³è¡¡æ€§)
å¯¹äº $n$ ä¸ªèŠ‚ç‚¹å’Œ $m$ ä¸ªé”®ï¼Œä¸€è‡´æ€§å“ˆå¸Œçš„è´Ÿè½½åˆ†å¸ƒæ–¹å·®ä¸º $O(\frac{m}{n^2})$ã€‚

**è¯æ˜**:

1. æ¯ä¸ªè™šæ‹ŸèŠ‚ç‚¹æ˜ å°„åˆ°çœŸå®èŠ‚ç‚¹çš„æ¦‚ç‡ç›¸ç­‰
2. é”®çš„åˆ†å¸ƒæ˜¯å‡åŒ€çš„
3. è´Ÿè½½åˆ†å¸ƒçš„æœŸæœ›ä¸º $\frac{m}{n}$
4. æ–¹å·®ä¸º $O(\frac{m}{n^2})$

### 3.2 æ•°æ®å¤åˆ¶ç†è®º

**å®šç† 3.2** (å¤åˆ¶å¯ç”¨æ€§)
å¯¹äºå¤åˆ¶å› å­ä¸º $r$ çš„ç³»ç»Ÿï¼Œå¯ç”¨æ€§ä¸º $A = 1 - (1-p)^r$ï¼Œå…¶ä¸­ $p$ æ˜¯å•ä¸ªèŠ‚ç‚¹çš„å¯ç”¨æ€§ã€‚

**è¯æ˜**:

1. ç³»ç»Ÿä¸å¯ç”¨çš„æ¦‚ç‡ = æ‰€æœ‰å‰¯æœ¬éƒ½ä¸å¯ç”¨çš„æ¦‚ç‡
2. å•ä¸ªå‰¯æœ¬ä¸å¯ç”¨çš„æ¦‚ç‡ = $1-p$
3. æ‰€æœ‰å‰¯æœ¬éƒ½ä¸å¯ç”¨çš„æ¦‚ç‡ = $(1-p)^r$
4. å› æ­¤å¯ç”¨æ€§ = $1 - (1-p)^r$

## 4. æ€§èƒ½åˆ†æ

### 4.1 æ—¶é—´å¤æ‚åº¦

- æ•°æ®æŸ¥æ‰¾: $O(\log n)$ (ä¸€è‡´æ€§å“ˆå¸Œ)
- æ•°æ®å†™å…¥: $O(r)$ (rä¸ºå¤åˆ¶å› å­)
- æ•°æ®åŒæ­¥: $O(k)$ (kä¸ºé”®çš„æ•°é‡)

### 4.2 ç©ºé—´å¤æ‚åº¦

- æ•°æ®å­˜å‚¨: $O(k \times r)$
- å…ƒæ•°æ®: $O(n \times k)$
- ç‰ˆæœ¬å‘é‡: $O(n \times k)$

### 4.3 ç½‘ç»œå¼€é”€

- æ•°æ®å¤åˆ¶: $O(d \times r)$ (dä¸ºæ•°æ®é‡)
- åŒæ­¥æ¶ˆæ¯: $O(k)$
- å¿ƒè·³æ¶ˆæ¯: $O(n)$

## 5. å®é™…åº”ç”¨

### 5.1 åˆ†å¸ƒå¼ç¼“å­˜

```python
class DistributedCache:
    """åˆ†å¸ƒå¼ç¼“å­˜"""
    
    def __init__(self, ttl: int = 3600):
        self.data_store = DistributedDataStore(replication_factor=2)
        self.ttl = ttl
        self.expiry_times: Dict[str, float] = {}
        
    def set(self, key: str, value: Any, ttl: int = None) -> bool:
        """è®¾ç½®ç¼“å­˜"""
        if ttl is None:
            ttl = self.ttl
            
        expiry_time = time.time() + ttl
        self.expiry_times[key] = expiry_time
        
        return self.data_store.put(key, value)
        
    def get(self, key: str) -> Optional[Any]:
        """è·å–ç¼“å­˜"""
        # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
        if key in self.expiry_times and time.time() > self.expiry_times[key]:
            self.delete(key)
            return None
            
        return self.data_store.get(key)
        
    def delete(self, key: str) -> bool:
        """åˆ é™¤ç¼“å­˜"""
        if key in self.expiry_times:
            del self.expiry_times[key]
        return self.data_store.delete(key)
        
    def clear_expired(self) -> int:
        """æ¸…ç†è¿‡æœŸæ•°æ®"""
        current_time = time.time()
        expired_keys = [
            key for key, expiry_time in self.expiry_times.items()
            if current_time > expiry_time
        ]
        
        for key in expired_keys:
            self.delete(key)
            
        return len(expired_keys)
```

### 5.2 åˆ†å¸ƒå¼æ•°æ®åº“

```python
class DistributedDatabase:
    """åˆ†å¸ƒå¼æ•°æ®åº“"""
    
    def __init__(self, shard_count: int = 4):
        self.shard_count = shard_count
        self.shards: List[DistributedDataStore] = []
        self.backup_manager: Optional[DataBackupManager] = None
        
        # åˆå§‹åŒ–åˆ†ç‰‡
        for i in range(shard_count):
            shard = DistributedDataStore(replication_factor=3)
            self.shards.append(shard)
            
    def _get_shard(self, key: str) -> DistributedDataStore:
        """è·å–å¯¹åº”çš„åˆ†ç‰‡"""
        shard_index = hash(key) % self.shard_count
        return self.shards[shard_index]
        
    def insert(self, key: str, value: Any) -> bool:
        """æ’å…¥æ•°æ®"""
        shard = self._get_shard(key)
        return shard.put(key, value)
        
    def select(self, key: str) -> Optional[Any]:
        """æŸ¥è¯¢æ•°æ®"""
        shard = self._get_shard(key)
        return shard.get(key)
        
    def update(self, key: str, value: Any) -> bool:
        """æ›´æ–°æ•°æ®"""
        shard = self._get_shard(key)
        return shard.put(key, value)
        
    def delete(self, key: str) -> bool:
        """åˆ é™¤æ•°æ®"""
        shard = self._get_shard(key)
        return shard.delete(key)
        
    def create_backup(self) -> List[str]:
        """åˆ›å»ºå¤‡ä»½"""
        backup_ids = []
        for i, shard in enumerate(self.shards):
            if self.backup_manager is None:
                self.backup_manager = DataBackupManager(shard)
            backup_id = self.backup_manager.create_backup()
            backup_ids.append(backup_id)
        return backup_ids
```

## 6. æ€»ç»“

åˆ†å¸ƒå¼æ•°æ®ç®¡ç†æ˜¯æ„å»ºå¤§è§„æ¨¡åˆ†å¸ƒå¼ç³»ç»Ÿçš„æ ¸å¿ƒæŠ€æœ¯ã€‚é€šè¿‡å½¢å¼åŒ–å®šä¹‰ã€Pythonå®ç°å’Œç†è®ºè¯æ˜ï¼Œæˆ‘ä»¬å»ºç«‹äº†å®Œæ•´çš„æ•°æ®ç®¡ç†çŸ¥è¯†ä½“ç³»ã€‚

### å…³é”®è¦ç‚¹

1. **æ•°æ®æ¨¡å‹**: ä½¿ç”¨æ•°å­¦ç¬¦å·ä¸¥æ ¼å®šä¹‰åˆ†å¸ƒå¼æ•°æ®æ¨¡å‹
2. **ä¸€è‡´æ€§**: ç†è§£ä¸åŒä¸€è‡´æ€§çº§åˆ«çš„æƒè¡¡
3. **åˆ†ç‰‡ç­–ç•¥**: ä¸€è‡´æ€§å“ˆå¸Œç­‰ç®—æ³•å®ç°æ•°æ®åˆ†ç‰‡
4. **å¤åˆ¶æœºåˆ¶**: å¤šå‰¯æœ¬ä¿è¯æ•°æ®å¯ç”¨æ€§
5. **åŒæ­¥ç­–ç•¥**: ç‰ˆæœ¬å‘é‡ç­‰æœºåˆ¶å¤„ç†æ•°æ®åŒæ­¥

### åº”ç”¨åœºæ™¯

- åˆ†å¸ƒå¼ç¼“å­˜ç³»ç»Ÿ
- åˆ†å¸ƒå¼æ•°æ®åº“
- å¯¹è±¡å­˜å‚¨ç³»ç»Ÿ
- æ¶ˆæ¯é˜Ÿåˆ—ç³»ç»Ÿ

---

**ç›¸å…³æ–‡æ¡£**:

- [åˆ†å¸ƒå¼æ¶æ„åŸºç¡€](./05-02-01-åˆ†å¸ƒå¼æ¶æ„åŸºç¡€.md)
- [ä¸€è‡´æ€§åè®®](../03-å…·ä½“ç§‘å­¦/03-04-åˆ†å¸ƒå¼ç³»ç»Ÿ/03-04-02-ä¸€è‡´æ€§åè®®.md)
- [åˆ†å¸ƒå¼ç®—æ³•](../03-å…·ä½“ç§‘å­¦/03-04-åˆ†å¸ƒå¼ç³»ç»Ÿ/03-04-03-åˆ†å¸ƒå¼ç®—æ³•.md)
