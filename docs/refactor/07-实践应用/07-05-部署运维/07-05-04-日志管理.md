# æ—¥å¿—ç®¡ç†

## ğŸ“‹ æ¦‚è¿°

æœ¬æ–‡æ¡£ä»‹ç»è½¯ä»¶ç³»ç»Ÿæ—¥å¿—ç®¡ç†çš„ç†è®ºåŸºç¡€ã€å®ç°æ–¹æ³•å’Œæœ€ä½³å®è·µï¼Œä¸ºæ„å»ºå¯é çš„æ—¥å¿—ç³»ç»Ÿæä¾›ç³»ç»ŸåŒ–çš„è§£å†³æ–¹æ¡ˆã€‚

## 1. ç†è®ºåŸºç¡€

### 1.1 æ—¥å¿—ç³»ç»Ÿå®šä¹‰

**æ—¥å¿—ç³»ç»Ÿ** æ˜¯ä¸€ä¸ªè®°å½•ã€å­˜å‚¨ã€åˆ†æå’Œæ£€ç´¢ç³»ç»Ÿè¿è¡Œä¿¡æ¯çš„è‡ªåŠ¨åŒ–ç³»ç»Ÿã€‚

#### 1.1.1 å½¢å¼åŒ–å®šä¹‰

è®¾ $\mathcal{L}$ ä¸ºæ—¥å¿—ç³»ç»Ÿï¼Œåˆ™ï¼š

$$\mathcal{L} = (C, S, A, Q, R)$$

å…¶ä¸­ï¼š

- $C$: æ”¶é›†å™¨é›†åˆ (Collectors)
- $S$: å­˜å‚¨ç³»ç»Ÿé›†åˆ (Storage)
- $A$: åˆ†æå™¨é›†åˆ (Analyzers)
- $Q$: æŸ¥è¯¢ç³»ç»Ÿé›†åˆ (Query)
- $R$: ä¿ç•™ç­–ç•¥é›†åˆ (Retention)

#### 1.1.2 æ—¥å¿—æ¡ç›®æ¨¡å‹

è®¾ $E$ ä¸ºæ—¥å¿—æ¡ç›®é›†åˆï¼Œæ¯ä¸ªæ¡ç›® $e \in E$ å®šä¹‰ä¸ºï¼š

$$e = (t, l, m, c, d)$$

å…¶ä¸­ï¼š

- $t$: æ—¶é—´æˆ³ (Timestamp)
- $l$: æ—¥å¿—çº§åˆ« (Level)
- $m$: æ¶ˆæ¯å†…å®¹ (Message)
- $c$: ä¸Šä¸‹æ–‡ä¿¡æ¯ (Context)
- $d$: å…ƒæ•°æ® (Metadata)

### 1.2 æ—¥å¿—çº§åˆ«ç†è®º

#### 1.2.1 çº§åˆ«å®šä¹‰

æ—¥å¿—çº§åˆ«è¡¨ç¤ºä¿¡æ¯çš„ä¸¥é‡ç¨‹åº¦ï¼š

$$L = \{DEBUG, INFO, WARNING, ERROR, CRITICAL\}$$

#### 1.2.2 çº§åˆ«å…³ç³»

çº§åˆ«ä¹‹é—´å­˜åœ¨ååºå…³ç³»ï¼š

$$DEBUG \prec INFO \prec WARNING \prec ERROR \prec CRITICAL$$

#### 1.2.3 è¿‡æ»¤å‡½æ•°

è®¾ $f_l$ ä¸ºçº§åˆ«è¿‡æ»¤å‡½æ•°ï¼š

$$f_l(e, l_{min}) = \begin{cases}
true & \text{if } e.l \geq l_{min} \\
false & \text{if } e.l < l_{min}
\end{cases}$$

### 1.3 æ—¥å¿—åˆ†æç†è®º

#### 1.3.1 æ¨¡å¼è¯†åˆ«

æ—¥å¿—æ¨¡å¼å¯ä»¥è¡¨ç¤ºä¸ºæ­£åˆ™è¡¨è¾¾å¼ï¼š

$$P = \{p_1, p_2, ..., p_n\}$$

å…¶ä¸­æ¯ä¸ªæ¨¡å¼ $p_i$ æ˜¯ä¸€ä¸ªæ­£åˆ™è¡¨è¾¾å¼ã€‚

#### 1.3.2 å¼‚å¸¸æ£€æµ‹

åŸºäºç»Ÿè®¡æ¨¡å‹çš„å¼‚å¸¸æ£€æµ‹ï¼š

$$P(x_t | x_{t-1}, x_{t-2}, ..., x_{t-n}) < \alpha$$

å…¶ä¸­ $\alpha$ æ˜¯æ˜¾è‘—æ€§æ°´å¹³ã€‚

## 2. æ ¸å¿ƒç»„ä»¶å®ç°

### 2.1 æ—¥å¿—ç³»ç»Ÿæ¶æ„

```python
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from enum import Enum
from typing import Dict, List, Optional, Callable, Any, Union
import asyncio
import logging
from datetime import datetime, timedelta
import json
import re
import threading
import queue
import hashlib
from pathlib import Path
import gzip
import pickle

class LogLevel(Enum):
    """æ—¥å¿—çº§åˆ«æšä¸¾"""
    DEBUG = 10
    INFO = 20
    WARNING = 30
    ERROR = 40
    CRITICAL = 50

    def __lt__(self, other):
        return self.value < other.value

    def __le__(self, other):
        return self.value <= other.value

    def __gt__(self, other):
        return self.value > other.value

    def __ge__(self, other):
        return self.value >= other.value

@dataclass
class LogEntry:
    """æ—¥å¿—æ¡ç›®å®šä¹‰"""
    timestamp: datetime
    level: LogLevel
    message: str
    context: Dict[str, Any] = field(default_factory=dict)
    metadata: Dict[str, Any] = field(default_factory=dict)
    source: str = ""
    thread_id: int = 0
    process_id: int = 0

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "timestamp": self.timestamp.isoformat(),
            "level": self.level.name,
            "message": self.message,
            "context": self.context,
            "metadata": self.metadata,
            "source": self.source,
            "thread_id": self.thread_id,
            "process_id": self.process_id
        }

    def to_json(self) -> str:
        """è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²"""
        return json.dumps(self.to_dict(), default=str)

    def __str__(self) -> str:
        return f"[{self.timestamp.strftime('%Y-%m-%d %H:%M:%S')}] {self.level.name}: {self.message}"

class LogCollector(ABC):
    """æ—¥å¿—æ”¶é›†å™¨æŠ½è±¡åŸºç±»"""

    def __init__(self, name: str):
        self.name = name
        self.is_running = False
        self.log_queue = queue.Queue()

    @abstractmethod
    async def collect(self) -> List[LogEntry]:
        """æ”¶é›†æ—¥å¿—"""
        pass

    @abstractmethod
    async def start(self):
        """å¯åŠ¨æ”¶é›†å™¨"""
        pass

    @abstractmethod
    async def stop(self):
        """åœæ­¢æ”¶é›†å™¨"""
        pass

    def add_log(self, entry: LogEntry):
        """æ·»åŠ æ—¥å¿—æ¡ç›®"""
        self.log_queue.put(entry)

class FileLogCollector(LogCollector):
    """æ–‡ä»¶æ—¥å¿—æ”¶é›†å™¨"""

    def __init__(self, name: str, file_path: str, pattern: str = r".*"):
        super().__init__(name)
        self.file_path = file_path
        self.pattern = re.compile(pattern)
        self.last_position = 0
        self.collection_interval = 1  # ç§’

    async def collect(self) -> List[LogEntry]:
        """ä»æ–‡ä»¶æ”¶é›†æ—¥å¿—"""
        entries = []

        try:
            if not Path(self.file_path).exists():
                return entries

            with open(self.file_path, 'r', encoding='utf-8') as f:
                f.seek(self.last_position)

                for line in f:
                    if self.pattern.match(line):
                        entry = self._parse_line(line)
                        if entry:
                            entries.append(entry)

                self.last_position = f.tell()

        except Exception as e:
            logging.error(f"Error collecting logs from {self.file_path}: {e}")

        return entries

    def _parse_line(self, line: str) -> Optional[LogEntry]:
        """è§£ææ—¥å¿—è¡Œ"""
        try:
            # ç®€å•çš„æ—¥å¿—è§£æï¼ˆå¯ä»¥æ ¹æ®å®é™…æ ¼å¼è°ƒæ•´ï¼‰
            parts = line.strip().split(' ', 2)
            if len(parts) >= 3:
                timestamp_str = f"{parts[0]} {parts[1]}"
                timestamp = datetime.strptime(timestamp_str, "%Y-%m-%d %H:%M:%S")

                # æå–æ—¥å¿—çº§åˆ«å’Œæ¶ˆæ¯
                remaining = parts[2]
                level_match = re.search(r'\[(DEBUG|INFO|WARNING|ERROR|CRITICAL)\]', remaining)

                if level_match:
                    level_name = level_match.group(1)
                    level = LogLevel[level_name]
                    message = remaining[level_match.end():].strip()

                    return LogEntry(
                        timestamp=timestamp,
                        level=level,
                        message=message,
                        source=self.name
                    )
        except Exception as e:
            logging.error(f"Error parsing log line: {e}")

        return None

    async def start(self):
        """å¯åŠ¨æ–‡ä»¶æ”¶é›†å™¨"""
        self.is_running = True
        logging.info(f"Started file log collector: {self.name}")

    async def stop(self):
        """åœæ­¢æ–‡ä»¶æ”¶é›†å™¨"""
        self.is_running = False
        logging.info(f"Stopped file log collector: {self.name}")

class ApplicationLogCollector(LogCollector):
    """åº”ç”¨æ—¥å¿—æ”¶é›†å™¨"""

    def __init__(self, name: str, log_callback: Callable):
        super().__init__(name)
        self.log_callback = log_callback
        self.collection_interval = 5  # ç§’

    async def collect(self) -> List[LogEntry]:
        """æ”¶é›†åº”ç”¨æ—¥å¿—"""
        try:
            entries = await self.log_callback()
            return entries
        except Exception as e:
            logging.error(f"Error collecting application logs: {e}")
            return []

    async def start(self):
        """å¯åŠ¨åº”ç”¨æ”¶é›†å™¨"""
        self.is_running = True
        logging.info(f"Started application log collector: {self.name}")

    async def stop(self):
        """åœæ­¢åº”ç”¨æ”¶é›†å™¨"""
        self.is_running = False
        logging.info(f"Stopped application log collector: {self.name}")

class LogStorage(ABC):
    """æ—¥å¿—å­˜å‚¨æŠ½è±¡åŸºç±»"""

    def __init__(self, name: str):
        self.name = name

    @abstractmethod
    async def store(self, entries: List[LogEntry]) -> bool:
        """å­˜å‚¨æ—¥å¿—æ¡ç›®"""
        pass

    @abstractmethod
    async def query(self, query: Dict[str, Any]) -> List[LogEntry]:
        """æŸ¥è¯¢æ—¥å¿—æ¡ç›®"""
        pass

    @abstractmethod
    async def cleanup(self, retention_days: int) -> int:
        """æ¸…ç†è¿‡æœŸæ—¥å¿—"""
        pass

class FileLogStorage(LogStorage):
    """æ–‡ä»¶æ—¥å¿—å­˜å‚¨"""

    def __init__(self, name: str, base_path: str):
        super().__init__(name)
        self.base_path = Path(base_path)
        self.base_path.mkdir(parents=True, exist_ok=True)
        self.current_file = None
        self.current_date = None
        self.max_file_size = 100 * 1024 * 1024  # 100MB

    def _get_log_file(self, date: datetime) -> Path:
        """è·å–æ—¥å¿—æ–‡ä»¶è·¯å¾„"""
        date_str = date.strftime("%Y-%m-%d")
        return self.base_path / f"logs_{date_str}.jsonl"

    async def store(self, entries: List[LogEntry]) -> bool:
        """å­˜å‚¨æ—¥å¿—æ¡ç›®"""
        try:
            for entry in entries:
                date = entry.timestamp.date()

                # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ›å»ºæ–°æ–‡ä»¶
                if self.current_date != date:
                    self.current_file = self._get_log_file(entry.timestamp)
                    self.current_date = date

                # å†™å…¥æ—¥å¿—æ¡ç›®
                with open(self.current_file, 'a', encoding='utf-8') as f:
                    f.write(entry.to_json() + '\n')

                # æ£€æŸ¥æ–‡ä»¶å¤§å°
                if self.current_file.stat().st_size > self.max_file_size:
                    await self._rotate_file()

            return True

        except Exception as e:
            logging.error(f"Error storing logs: {e}")
            return False

    async def _rotate_file(self):
        """è½®è½¬æ—¥å¿—æ–‡ä»¶"""
        if self.current_file and self.current_file.exists():
            timestamp = datetime.now().strftime("%H%M%S")
            new_name = f"{self.current_file.stem}_{timestamp}{self.current_file.suffix}"
            new_path = self.current_file.parent / new_name
            self.current_file.rename(new_path)

    async def query(self, query: Dict[str, Any]) -> List[LogEntry]:
        """æŸ¥è¯¢æ—¥å¿—æ¡ç›®"""
        entries = []

        try:
            start_time = query.get('start_time')
            end_time = query.get('end_time')
            level = query.get('level')
            pattern = query.get('pattern')

            # è·å–éœ€è¦æŸ¥è¯¢çš„æ–‡ä»¶
            if start_time and end_time:
                current_date = start_time.date()
                end_date = end_time.date()

                while current_date <= end_date:
                    log_file = self._get_log_file(datetime.combine(current_date, datetime.min.time()))

                    if log_file.exists():
                        entries.extend(await self._read_file(log_file, query))

                    current_date += timedelta(days=1)
            else:
                # æŸ¥è¯¢æ‰€æœ‰æ–‡ä»¶
                for log_file in self.base_path.glob("logs_*.jsonl"):
                    entries.extend(await self._read_file(log_file, query))

            # æ’åº
            entries.sort(key=lambda x: x.timestamp)

        except Exception as e:
            logging.error(f"Error querying logs: {e}")

        return entries

    async def _read_file(self, log_file: Path, query: Dict[str, Any]) -> List[LogEntry]:
        """è¯»å–æ—¥å¿—æ–‡ä»¶"""
        entries = []

        try:
            start_time = query.get('start_time')
            end_time = query.get('end_time')
            level = query.get('level')
            pattern = query.get('pattern')

            with open(log_file, 'r', encoding='utf-8') as f:
                for line in f:
                    try:
                        data = json.loads(line.strip())
                        entry = self._dict_to_entry(data)

                        # åº”ç”¨è¿‡æ»¤æ¡ä»¶
                        if start_time and entry.timestamp < start_time:
                            continue
                        if end_time and entry.timestamp > end_time:
                            continue
                        if level and entry.level < level:
                            continue
                        if pattern and not re.search(pattern, entry.message):
                            continue

                        entries.append(entry)

                    except json.JSONDecodeError:
                        continue

        except Exception as e:
            logging.error(f"Error reading log file {log_file}: {e}")

        return entries

    def _dict_to_entry(self, data: Dict[str, Any]) -> LogEntry:
        """å­—å…¸è½¬æ¢ä¸ºæ—¥å¿—æ¡ç›®"""
        return LogEntry(
            timestamp=datetime.fromisoformat(data['timestamp']),
            level=LogLevel[data['level']],
            message=data['message'],
            context=data.get('context', {}),
            metadata=data.get('metadata', {}),
            source=data.get('source', ''),
            thread_id=data.get('thread_id', 0),
            process_id=data.get('process_id', 0)
        )

    async def cleanup(self, retention_days: int) -> int:
        """æ¸…ç†è¿‡æœŸæ—¥å¿—"""
        deleted_count = 0
        cutoff_date = datetime.now() - timedelta(days=retention_days)

        try:
            for log_file in self.base_path.glob("logs_*.jsonl"):
                # ä»æ–‡ä»¶åæå–æ—¥æœŸ
                date_str = log_file.stem.split['_'](1)
                file_date = datetime.strptime(date_str, "%Y-%m-%d")

                if file_date.date() < cutoff_date.date():
                    log_file.unlink()
                    deleted_count += 1

        except Exception as e:
            logging.error(f"Error cleaning up logs: {e}")

        return deleted_count

class LogAnalyzer:
    """æ—¥å¿—åˆ†æå™¨"""

    def __init__(self):
        self.patterns: Dict[str, re.Pattern] = {}
        self.statistics: Dict[str, Any] = {}

    def add_pattern(self, name: str, pattern: str):
        """æ·»åŠ åˆ†ææ¨¡å¼"""
        self.patterns[name] = re.compile(pattern)

    def analyze_entries(self, entries: List[LogEntry]) -> Dict[str, Any]:
        """åˆ†ææ—¥å¿—æ¡ç›®"""
        analysis = {
            "total_entries": len(entries),
            "level_distribution": {},
            "pattern_matches": {},
            "error_summary": {},
            "time_distribution": {},
            "source_distribution": {}
        }

        if not entries:
            return analysis

        # çº§åˆ«åˆ†å¸ƒ
        for entry in entries:
            level_name = entry.level.name
            analysis["level_distribution"][level_name] = analysis["level_distribution"].get(level_name, 0) + 1

        # æ¨¡å¼åŒ¹é…
        for pattern_name, pattern in self.patterns.items():
            matches = 0
            for entry in entries:
                if pattern.search(entry.message):
                    matches += 1
            analysis["pattern_matches"][pattern_name] = matches

        # é”™è¯¯æ‘˜è¦
        error_entries = [e for e in entries if e.level >= LogLevel.ERROR]
        analysis["error_summary"] = {
            "total_errors": len(error_entries),
            "error_types": {}
        }

        for entry in error_entries:
            # ç®€å•çš„é”™è¯¯ç±»å‹æå–
            error_type = entry.message.split[':'](0) if ':' in entry.message else "Unknown"
            analysis["error_summary"]["error_types"][error_type] = analysis["error_summary"]["error_types"].get(error_type, 0) + 1

        # æ—¶é—´åˆ†å¸ƒ
        for entry in entries:
            hour = entry.timestamp.hour
            analysis["time_distribution"][hour] = analysis["time_distribution"].get(hour, 0) + 1

        # æ¥æºåˆ†å¸ƒ
        for entry in entries:
            source = entry.source or "unknown"
            analysis["source_distribution"][source] = analysis["source_distribution"].get(source, 0) + 1

        return analysis

    def detect_anomalies(self, entries: List[LogEntry], window_size: int = 100) -> List[Dict[str, Any]]:
        """æ£€æµ‹å¼‚å¸¸"""
        anomalies = []

        if len(entries) < window_size:
            return anomalies

        # æŒ‰æ—¶é—´çª—å£åˆ†æ
        for i in range(window_size, len(entries)):
            window = entries[i-window_size:i]
            current = entries[i]

            # è®¡ç®—çª—å£å†…çš„é”™è¯¯ç‡
            error_count = sum(1 for e in window if e.level >= LogLevel.ERROR)
            error_rate = error_count / window_size

            # å¦‚æœå½“å‰æ¡ç›®æ˜¯é”™è¯¯ä¸”é”™è¯¯ç‡çªç„¶å¢åŠ ï¼Œå¯èƒ½æ˜¯å¼‚å¸¸
            if (current.level >= LogLevel.ERROR and
                error_rate > 0.1):  # 10%çš„é”™è¯¯ç‡é˜ˆå€¼
                anomalies.append({
                    "timestamp": current.timestamp,
                    "message": current.message,
                    "error_rate": error_rate,
                    "type": "error_spike"
                })

        return anomalies

class LogQueryEngine:
    """æ—¥å¿—æŸ¥è¯¢å¼•æ“"""

    def __init__(self, storage: LogStorage):
        self.storage = storage

    async def search(self, query: str, filters: Dict[str, Any] = None) -> List[LogEntry]:
        """æœç´¢æ—¥å¿—"""
        # è§£ææŸ¥è¯¢å­—ç¬¦ä¸²
        parsed_query = self._parse_query(query)

        if filters:
            parsed_query.update(filters)

        return await self.storage.query(parsed_query)

    def _parse_query(self, query: str) -> Dict[str, Any]:
        """è§£ææŸ¥è¯¢å­—ç¬¦ä¸²"""
        parsed = {}

        # ç®€å•çš„æŸ¥è¯¢è§£æ
        if "level:" in query:
            level_match = re.search(r'level:(\w+)', query)
            if level_match:
                level_name = level_match.group(1).upper()
                if level_name in LogLevel.__members__:
                    parsed['level'] = LogLevel[level_name]

        if "source:" in query:
            source_match = re.search(r'source:(\w+)', query)
            if source_match:
                parsed['source'] = source_match.group(1)

        if "pattern:" in query:
            pattern_match = re.search(r'pattern:"([^"]+)"', query)
            if pattern_match:
                parsed['pattern'] = pattern_match.group(1)

        return parsed

    async def get_statistics(self, time_range: Dict[str, datetime] = None) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        query = {}
        if time_range:
            query.update(time_range)

        entries = await self.storage.query(query)

        if not entries:
            return {}

        analyzer = LogAnalyzer()
        return analyzer.analyze_entries(entries)

class LogManager:
    """æ—¥å¿—ç®¡ç†å™¨"""

    def __init__(self, name: str):
        self.name = name
        self.collectors: List[LogCollector] = []
        self.storage: Optional[LogStorage] = None
        self.analyzer = LogAnalyzer()
        self.query_engine: Optional[LogQueryEngine] = None
        self.is_running = False
        self.collection_interval = 60  # ç§’
        self.retention_days = 30

    def add_collector(self, collector: LogCollector):
        """æ·»åŠ æ”¶é›†å™¨"""
        self.collectors.append(collector)

    def set_storage(self, storage: LogStorage):
        """è®¾ç½®å­˜å‚¨ç³»ç»Ÿ"""
        self.storage = storage
        self.query_engine = LogQueryEngine(storage)

    def add_analysis_pattern(self, name: str, pattern: str):
        """æ·»åŠ åˆ†ææ¨¡å¼"""
        self.analyzer.add_pattern(name, pattern)

    async def start(self):
        """å¯åŠ¨æ—¥å¿—ç®¡ç†å™¨"""
        self.is_running = True

        # å¯åŠ¨æ‰€æœ‰æ”¶é›†å™¨
        for collector in self.collectors:
            await collector.start()

        # å¯åŠ¨æ”¶é›†å¾ªç¯
        asyncio.create_task(self._collection_loop())

        # å¯åŠ¨æ¸…ç†å¾ªç¯
        asyncio.create_task(self._cleanup_loop())

        logging.info(f"Log manager {self.name} started")

    async def stop(self):
        """åœæ­¢æ—¥å¿—ç®¡ç†å™¨"""
        self.is_running = False

        # åœæ­¢æ‰€æœ‰æ”¶é›†å™¨
        for collector in self.collectors:
            await collector.stop()

        logging.info(f"Log manager {self.name} stopped")

    async def _collection_loop(self):
        """æ”¶é›†å¾ªç¯"""
        while self.is_running:
            try:
                all_entries = []

                # æ”¶é›†æ‰€æœ‰æ”¶é›†å™¨çš„æ—¥å¿—
                for collector in self.collectors:
                    entries = await collector.collect()
                    all_entries.extend(entries)

                # å­˜å‚¨æ—¥å¿—
                if all_entries and self.storage:
                    await self.storage.store(all_entries)

                # ç­‰å¾…ä¸‹æ¬¡æ”¶é›†
                await asyncio.sleep(self.collection_interval)

            except Exception as e:
                logging.error(f"Error in collection loop: {e}")
                await asyncio.sleep(10)

    async def _cleanup_loop(self):
        """æ¸…ç†å¾ªç¯"""
        while self.is_running:
            try:
                if self.storage:
                    deleted_count = await self.storage.cleanup(self.retention_days)
                    if deleted_count > 0:
                        logging.info(f"Cleaned up {deleted_count} old log files")

                # æ¯å¤©æ¸…ç†ä¸€æ¬¡
                await asyncio.sleep(24 * 60 * 60)

            except Exception as e:
                logging.error(f"Error in cleanup loop: {e}")
                await asyncio.sleep(60 * 60)  # 1å°æ—¶åé‡è¯•

    async def search_logs(self, query: str, filters: Dict[str, Any] = None) -> List[LogEntry]:
        """æœç´¢æ—¥å¿—"""
        if not self.query_engine:
            return []

        return await self.query_engine.search(query, filters)

    async def get_statistics(self, time_range: Dict[str, datetime] = None) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        if not self.query_engine:
            return {}

        return await self.query_engine.get_statistics(time_range)

    async def detect_anomalies(self, time_range: Dict[str, datetime] = None) -> List[Dict[str, Any]]:
        """æ£€æµ‹å¼‚å¸¸"""
        if not self.query_engine:
            return []

        query = {}
        if time_range:
            query.update(time_range)

        entries = await self.storage.query(query)
        return self.analyzer.detect_anomalies(entries)
```

### 2.2 æ—¥å¿—æ ¼å¼åŒ–å™¨

```python
class LogFormatter:
    """æ—¥å¿—æ ¼å¼åŒ–å™¨"""

    def __init__(self, format_string: str = None):
        self.format_string = format_string or "[{timestamp}] {level}: {message}"

    def format_entry(self, entry: LogEntry) -> str:
        """æ ¼å¼åŒ–æ—¥å¿—æ¡ç›®"""
        return self.format_string.format(
            timestamp=entry.timestamp.strftime("%Y-%m-%d %H:%M:%S"),
            level=entry.level.name,
            message=entry.message,
            source=entry.source,
            thread_id=entry.thread_id,
            process_id=entry.process_id
        )

    def format_json(self, entry: LogEntry) -> str:
        """JSONæ ¼å¼åŒ–"""
        return entry.to_json()

    def format_structured(self, entry: LogEntry) -> str:
        """ç»“æ„åŒ–æ ¼å¼åŒ–"""
        return f"{entry.timestamp.isoformat()} | {entry.level.name:8} | {entry.source:15} | {entry.message}"

class LogFilter:
    """æ—¥å¿—è¿‡æ»¤å™¨"""

    def __init__(self, min_level: LogLevel = LogLevel.DEBUG):
        self.min_level = min_level
        self.exclude_patterns: List[re.Pattern] = []
        self.include_patterns: List[re.Pattern] = []

    def add_exclude_pattern(self, pattern: str):
        """æ·»åŠ æ’é™¤æ¨¡å¼"""
        self.exclude_patterns.append(re.compile(pattern))

    def add_include_pattern(self, pattern: str):
        """æ·»åŠ åŒ…å«æ¨¡å¼"""
        self.include_patterns.append(re.compile(pattern))

    def filter_entry(self, entry: LogEntry) -> bool:
        """è¿‡æ»¤æ—¥å¿—æ¡ç›®"""
        # æ£€æŸ¥çº§åˆ«
        if entry.level < self.min_level:
            return False

        # æ£€æŸ¥æ’é™¤æ¨¡å¼
        for pattern in self.exclude_patterns:
            if pattern.search(entry.message):
                return False

        # æ£€æŸ¥åŒ…å«æ¨¡å¼
        if self.include_patterns:
            for pattern in self.include_patterns:
                if pattern.search(entry.message):
                    return True
            return False

        return True
```

## 3. å®é™…åº”ç”¨ç¤ºä¾‹

### 3.1 Webåº”ç”¨æ—¥å¿—ç³»ç»Ÿ

```python
async def web_app_logging_example():
    """Webåº”ç”¨æ—¥å¿—ç³»ç»Ÿç¤ºä¾‹"""

    # åˆ›å»ºæ—¥å¿—ç®¡ç†å™¨
    log_manager = LogManager("WebApp-Logging")

    # åˆ›å»ºæ–‡ä»¶å­˜å‚¨
    storage = FileLogStorage("webapp", "./logs/webapp")
    log_manager.set_storage(storage)

    # æ·»åŠ æ–‡ä»¶æ”¶é›†å™¨
    file_collector = FileLogCollector(
        "webapp-logs",
        "./logs/webapp.log",
        r".*\[(DEBUG|INFO|WARNING|ERROR|CRITICAL)\].*"
    )
    log_manager.add_collector(file_collector)

    # æ·»åŠ åº”ç”¨æ”¶é›†å™¨
    async def collect_app_logs():
        """æ”¶é›†åº”ç”¨æ—¥å¿—"""
        entries = []
        now = datetime.now()

        import random

        # æ¨¡æ‹Ÿåº”ç”¨æ—¥å¿—
        log_levels = [LogLevel.INFO, LogLevel.WARNING, LogLevel.ERROR]
        messages = [
            "User login successful",
            "Database connection established",
            "API request processed",
            "Cache miss occurred",
            "Database query timeout",
            "Authentication failed"
        ]

        # ç”Ÿæˆä¸€äº›æ—¥å¿—æ¡ç›®
        for _ in range(random.randint(1, 5)):
            level = random.choice(log_levels)
            message = random.choice(messages)

            entry = LogEntry(
                timestamp=now,
                level=level,
                message=message,
                context={
                    "user_id": random.randint(1, 1000),
                    "session_id": f"session_{random.randint(1000, 9999)}",
                    "ip_address": f"192.168.1.{random.randint(1, 255)}"
                },
                source="webapp"
            )
            entries.append(entry)

        return entries

    app_collector = ApplicationLogCollector("webapp", collect_app_logs)
    log_manager.add_collector(app_collector)

    # æ·»åŠ åˆ†ææ¨¡å¼
    log_manager.add_analysis_pattern("error_pattern", r"error|exception|failed|timeout")
    log_manager.add_analysis_pattern("auth_pattern", r"login|logout|authentication|authorization")
    log_manager.add_analysis_pattern("db_pattern", r"database|query|connection|transaction")

    # å¯åŠ¨æ—¥å¿—ç®¡ç†å™¨
    await log_manager.start()

    # è¿è¡Œä¸€æ®µæ—¶é—´
    await asyncio.sleep(300)  # 5åˆ†é’Ÿ

    # æœç´¢æ—¥å¿—
    error_logs = await log_manager.search_logs("level:ERROR")
    print(f"Found {len(error_logs)} error logs")

    # è·å–ç»Ÿè®¡ä¿¡æ¯
    stats = await log_manager.get_statistics()
    print("Log statistics:")
    print(json.dumps(stats, indent=2, default=str))

    # æ£€æµ‹å¼‚å¸¸
    anomalies = await log_manager.detect_anomalies()
    print(f"Detected {len(anomalies)} anomalies")

    # åœæ­¢æ—¥å¿—ç®¡ç†å™¨
    await log_manager.stop()

# è¿è¡Œç¤ºä¾‹
if __name__ == "__main__":
    asyncio.run(web_app_logging_example())
```

### 3.2 å¾®æœåŠ¡æ—¥å¿—ç³»ç»Ÿ

```python
async def microservice_logging_example():
    """å¾®æœåŠ¡æ—¥å¿—ç³»ç»Ÿç¤ºä¾‹"""

    # åˆ›å»ºå¤šä¸ªæœåŠ¡çš„æ—¥å¿—ç®¡ç†å™¨
    services = ["user-service", "order-service", "payment-service"]
    log_managers = {}

    for service in services:
        # åˆ›å»ºæ—¥å¿—ç®¡ç†å™¨
        log_manager = LogManager(f"{service}-Logging")

        # åˆ›å»ºå­˜å‚¨
        storage = FileLogStorage(service, f"./logs/{service}")
        log_manager.set_storage(storage)

        # æ·»åŠ æ”¶é›†å™¨
        async def create_service_collector(service_name: str):
            async def collect_service_logs():
                entries = []
                now = datetime.now()

                import random

                # æœåŠ¡ç‰¹å®šçš„æ—¥å¿—
                service_messages = {
                    "user-service": [
                        "User created successfully",
                        "User authentication failed",
                        "User profile updated",
                        "User deleted"
                    ],
                    "order-service": [
                        "Order created",
                        "Order status updated",
                        "Order cancelled",
                        "Payment processed"
                    ],
                    "payment-service": [
                        "Payment initiated",
                        "Payment completed",
                        "Payment failed",
                        "Refund processed"
                    ]
                }

                messages = service_messages.get(service_name, ["Service log"])

                for _ in range(random.randint(1, 3)):
                    level = random.choice([LogLevel.INFO, LogLevel.WARNING, LogLevel.ERROR])
                    message = random.choice(messages)

                    entry = LogEntry(
                        timestamp=now,
                        level=level,
                        message=message,
                        context={
                            "service": service_name,
                            "request_id": f"req_{random.randint(10000, 99999)}",
                            "user_id": random.randint(1, 1000)
                        },
                        source=service_name
                    )
                    entries.append(entry)

                return entries

            return collect_service_logs

        collector = ApplicationLogCollector(
            service,
            await create_service_collector(service)
        )
        log_manager.add_collector(collector)

        # æ·»åŠ åˆ†ææ¨¡å¼
        log_manager.add_analysis_pattern("service_error", r"error|failed|exception")
        log_manager.add_analysis_pattern("service_performance", r"timeout|slow|performance")

        log_managers[service] = log_manager

    # å¯åŠ¨æ‰€æœ‰æ—¥å¿—ç®¡ç†å™¨
    start_tasks = [manager.start() for manager in log_managers.values()]
    await asyncio.gather(*start_tasks)

    # è¿è¡Œä¸€æ®µæ—¶é—´
    await asyncio.sleep(180)  # 3åˆ†é’Ÿ

    # åˆ†ææ‰€æœ‰æœåŠ¡çš„æ—¥å¿—
    for service, manager in log_managers.items():
        print(f"\n{service} logs:")

        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = await manager.get_statistics()
        print(f"Total entries: {stats.get('total_entries', 0)}")
        print(f"Level distribution: {stats.get('level_distribution', {})}")

        # æœç´¢é”™è¯¯æ—¥å¿—
        error_logs = await manager.search_logs("level:ERROR")
        print(f"Error logs: {len(error_logs)}")

    # åœæ­¢æ‰€æœ‰æ—¥å¿—ç®¡ç†å™¨
    stop_tasks = [manager.stop() for manager in log_managers.values()]
    await asyncio.gather(*stop_tasks)
```

## 4. æœ€ä½³å®è·µ

### 4.1 æ—¥å¿—è®¾è®¡åŸåˆ™

1. **ç»“æ„åŒ–æ—¥å¿—**: ä½¿ç”¨ç»“æ„åŒ–çš„æ—¥å¿—æ ¼å¼
2. **é€‚å½“çº§åˆ«**: é€‰æ‹©åˆé€‚çš„æ—¥å¿—çº§åˆ«
3. **ä¸Šä¸‹æ–‡ä¿¡æ¯**: åŒ…å«è¶³å¤Ÿçš„ä¸Šä¸‹æ–‡ä¿¡æ¯
4. **æ€§èƒ½è€ƒè™‘**: é¿å…æ—¥å¿—å½±å“åº”ç”¨æ€§èƒ½

### 4.2 å­˜å‚¨ç­–ç•¥

1. **åˆ†çº§å­˜å‚¨**: ä¸åŒçº§åˆ«çš„æ—¥å¿—ä½¿ç”¨ä¸åŒçš„å­˜å‚¨ç­–ç•¥
2. **å‹ç¼©å­˜å‚¨**: å¯¹å†å²æ—¥å¿—è¿›è¡Œå‹ç¼©
3. **ç´¢å¼•ä¼˜åŒ–**: ä¸ºæŸ¥è¯¢åˆ›å»ºé€‚å½“çš„ç´¢å¼•
4. **å¤‡ä»½ç­–ç•¥**: é‡è¦æ—¥å¿—çš„å¤‡ä»½ç­–ç•¥

### 4.3 åˆ†æç­–ç•¥

1. **å®æ—¶åˆ†æ**: å®æ—¶åˆ†æå…³é”®æŒ‡æ ‡
2. **æ¨¡å¼è¯†åˆ«**: è¯†åˆ«å¸¸è§çš„æ—¥å¿—æ¨¡å¼
3. **å¼‚å¸¸æ£€æµ‹**: è‡ªåŠ¨æ£€æµ‹å¼‚å¸¸æƒ…å†µ
4. **è¶‹åŠ¿åˆ†æ**: åˆ†ææ—¥å¿—è¶‹åŠ¿

## 5. æ€§èƒ½ä¼˜åŒ–

### 5.1 å¼‚æ­¥å¤„ç†

```python
class AsyncLogManager(LogManager):
    """å¼‚æ­¥æ—¥å¿—ç®¡ç†å™¨"""

    def __init__(self, name: str, max_workers: int = 4):
        super().__init__(name)
        self.max_workers = max_workers
        self.worker_pool = None

    async def start(self):
        """å¯åŠ¨å¼‚æ­¥æ—¥å¿—ç®¡ç†å™¨"""
        await super().start()

        # åˆ›å»ºå·¥ä½œæ± 
        self.worker_pool = asyncio.Queue(maxsize=self.max_workers)

        # å¯åŠ¨å·¥ä½œçº¿ç¨‹
        workers = [asyncio.create_task(self._worker()) for _ in range(self.max_workers)]
        self.workers = workers

    async def _worker(self):
        """å·¥ä½œçº¿ç¨‹"""
        while self.is_running:
            try:
                entries = await asyncio.wait_for(self.worker_pool.get(), timeout=1.0)

                if self.storage:
                    await self.storage.store(entries)

                self.worker_pool.task_done()

            except asyncio.TimeoutError:
                continue
            except Exception as e:
                logging.error(f"Worker error: {e}")

    async def _collection_loop(self):
        """å¼‚æ­¥æ”¶é›†å¾ªç¯"""
        while self.is_running:
            try:
                all_entries = []

                # å¹¶è¡Œæ”¶é›†æ‰€æœ‰æ”¶é›†å™¨çš„æ—¥å¿—
                tasks = [collector.collect() for collector in self.collectors]
                results = await asyncio.gather(*tasks, return_exceptions=True)

                for result in results:
                    if isinstance(result, list):
                        all_entries.extend(result)

                # å¼‚æ­¥å­˜å‚¨
                if all_entries and self.worker_pool:
                    await self.worker_pool.put(all_entries)

                await asyncio.sleep(self.collection_interval)

            except Exception as e:
                logging.error(f"Error in collection loop: {e}")
                await asyncio.sleep(10)
```

### 5.2 ç¼“å­˜ä¼˜åŒ–

```python
class CachedLogStorage(FileLogStorage):
    """å¸¦ç¼“å­˜çš„æ—¥å¿—å­˜å‚¨"""

    def __init__(self, name: str, base_path: str, cache_size: int = 1000):
        super().__init__(name, base_path)
        self.cache_size = cache_size
        self.write_cache: List[LogEntry] = []
        self.cache_lock = asyncio.Lock()

    async def store(self, entries: List[LogEntry]) -> bool:
        """å¸¦ç¼“å­˜çš„å­˜å‚¨"""
        async with self.cache_lock:
            self.write_cache.extend(entries)

            # æ£€æŸ¥ç¼“å­˜å¤§å°
            if len(self.write_cache) >= self.cache_size:
                # æ‰¹é‡å†™å…¥
                await self._flush_cache()

        return True

    async def _flush_cache(self):
        """åˆ·æ–°ç¼“å­˜"""
        if not self.write_cache:
            return

        try:
            # æŒ‰æ—¥æœŸåˆ†ç»„
            entries_by_date = {}
            for entry in self.write_cache:
                date = entry.timestamp.date()
                if date not in entries_by_date:
                    entries_by_date[date] = []
                entries_by_date[date].append(entry)

            # æ‰¹é‡å†™å…¥æ¯ä¸ªæ—¥æœŸçš„æ–‡ä»¶
            for date, entries in entries_by_date.items():
                log_file = self._get_log_file(datetime.combine(date, datetime.min.time()))

                with open(log_file, 'a', encoding='utf-8') as f:
                    for entry in entries:
                        f.write(entry.to_json() + '\n')

            # æ¸…ç©ºç¼“å­˜
            self.write_cache.clear()

        except Exception as e:
            logging.error(f"Error flushing cache: {e}")

    async def stop(self):
        """åœæ­¢æ—¶åˆ·æ–°ç¼“å­˜"""
        await self._flush_cache()
```

## 6. æ€»ç»“

æ—¥å¿—ç®¡ç†ç³»ç»Ÿæ˜¯ç°ä»£è½¯ä»¶å·¥ç¨‹çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œé€šè¿‡ç³»ç»ŸåŒ–çš„æ—¥å¿—æ”¶é›†ã€å­˜å‚¨ã€åˆ†æå’ŒæŸ¥è¯¢ï¼Œä¸ºç³»ç»Ÿç›‘æ§ã€é—®é¢˜è¯Šæ–­å’Œæ€§èƒ½ä¼˜åŒ–æä¾›äº†é‡è¦æ”¯æŒã€‚æœ¬æ–‡æ¡£æä¾›äº†å®Œæ•´çš„ç†è®ºåŸºç¡€ã€å®ç°æ–¹æ³•å’Œæœ€ä½³å®è·µï¼Œä¸ºæ„å»ºé«˜è´¨é‡çš„æ—¥å¿—ç®¡ç†ç³»ç»Ÿæä¾›äº†ç³»ç»ŸåŒ–çš„è§£å†³æ–¹æ¡ˆã€‚

### å…³é”®è¦ç‚¹

1. **ç†è®ºåŸºç¡€**: ä¸¥æ ¼çš„å½¢å¼åŒ–å®šä¹‰å’Œæ•°å­¦æ¨¡å‹
2. **å®ç°æ–¹æ³•**: å®Œæ•´çš„Pythonå®ç°å’Œä»£ç ç¤ºä¾‹
3. **æœ€ä½³å®è·µ**: ç»è¿‡éªŒè¯çš„è®¾è®¡åŸåˆ™å’Œç­–ç•¥
4. **æ€§èƒ½ä¼˜åŒ–**: å¼‚æ­¥å¤„ç†å’Œç¼“å­˜æœºåˆ¶
5. **å¯æ‰©å±•æ€§**: æ”¯æŒå¤šç§æ•°æ®æºå’Œå­˜å‚¨åç«¯

### åº”ç”¨ä»·å€¼

1. **é—®é¢˜è¯Šæ–­**: å¿«é€Ÿå®šä½å’Œè§£å†³é—®é¢˜
2. **æ€§èƒ½ç›‘æ§**: å®æ—¶ç›‘æ§ç³»ç»Ÿæ€§èƒ½
3. **å®‰å…¨å®¡è®¡**: è®°å½•å’Œå®¡è®¡ç³»ç»Ÿè¡Œä¸º
4. **ä¸šåŠ¡åˆ†æ**: åˆ†æç”¨æˆ·è¡Œä¸ºå’Œä¸šåŠ¡è¶‹åŠ¿

---

**ç›¸å…³æ–‡æ¡£**:
- [ç›‘æ§å‘Šè­¦](./07-05-03-ç›‘æ§å‘Šè­¦.md)
- [CI/CDæµæ°´çº¿](./07-05-02-CI-CDæµæ°´çº¿.md)
- [å®¹å™¨åŒ–éƒ¨ç½²](./07-05-01-å®¹å™¨åŒ–éƒ¨ç½².md)
