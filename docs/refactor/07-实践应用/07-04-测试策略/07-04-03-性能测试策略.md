# æ€§èƒ½æµ‹è¯•ç­–ç•¥

## ğŸ“‹ æ¦‚è¿°

æ€§èƒ½æµ‹è¯•æ˜¯éªŒè¯ç³»ç»Ÿåœ¨ç‰¹å®šè´Ÿè½½ä¸‹çš„æ€§èƒ½è¡¨ç°ï¼ŒåŒ…æ‹¬å“åº”æ—¶é—´ã€ååé‡ã€èµ„æºä½¿ç”¨ç‡ç­‰å…³é”®æŒ‡æ ‡ã€‚

## 1. ç†è®ºåŸºç¡€

### 1.1 æ€§èƒ½æŒ‡æ ‡å®šä¹‰

**å®šä¹‰ 1.1** (å“åº”æ—¶é—´)
å“åº”æ—¶é—´æ˜¯è¯·æ±‚ä»å‘é€åˆ°æ¥æ”¶å“åº”çš„æ—¶é—´ï¼š
$$T_{response} = T_{network} + T_{processing} + T_{database}$$

**å®šä¹‰ 1.2** (ååé‡)
ååé‡æ˜¯å•ä½æ—¶é—´å†…å¤„ç†çš„è¯·æ±‚æ•°é‡ï¼š
$$\text{Throughput} = \frac{\text{Request Count}}{\text{Time Period}}$$

**å®šä¹‰ 1.3** (å¹¶å‘åº¦)
å¹¶å‘åº¦æ˜¯åŒæ—¶å¤„ç†çš„è¯·æ±‚æ•°é‡ï¼š
$$\text{Concurrency} = \text{Active Requests}$$

## 2. Pythonå®ç°

### 2.1 æ€§èƒ½æµ‹è¯•æ¡†æ¶

```python
from typing import Dict, List, Optional, Tuple, Any, Callable
from dataclasses import dataclass
from abc import ABC, abstractmethod
import time
import threading
import asyncio
import statistics
import psutil
import requests
from concurrent.futures import ThreadPoolExecutor
from enum import Enum
import logging
import json
import matplotlib.pyplot as plt

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class TestType(Enum):
    """æµ‹è¯•ç±»å‹æšä¸¾"""
    LOAD_TEST = "load_test"
    STRESS_TEST = "stress_test"
    SPIKE_TEST = "spike_test"
    ENDURANCE_TEST = "endurance_test"

@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡"""
    response_time: float = 0.0
    throughput: float = 0.0
    error_rate: float = 0.0
    cpu_usage: float = 0.0
    memory_usage: float = 0.0
    
    def __post_init__(self):
        self.start_time = time.time()
        self.end_time = None
    
    @property
    def duration(self) -> float:
        """æµ‹è¯•æŒç»­æ—¶é—´"""
        if self.end_time:
            return self.end_time - self.start_time
        return time.time() - self.start_time

class PerformanceTester:
    """æ€§èƒ½æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.results: List[PerformanceMetrics] = []
        self.lock = threading.Lock()
    
    def load_test(self, target_url: str, concurrent_users: int, 
                  duration: int) -> PerformanceMetrics:
        """è´Ÿè½½æµ‹è¯•"""
        logger.info(f"å¼€å§‹è´Ÿè½½æµ‹è¯•: {concurrent_users} å¹¶å‘ç”¨æˆ·, {duration}ç§’")
        
        metrics = PerformanceMetrics()
        start_time = time.time()
        
        def worker():
            while time.time() - start_time < duration:
                try:
                    request_start = time.time()
                    response = requests.get(target_url, timeout=10)
                    request_time = time.time() - request_start
                    
                    with self.lock:
                        metrics.response_time += request_time
                        if response.status_code != 200:
                            metrics.error_rate += 1
                except Exception as e:
                    with self.lock:
                        metrics.error_rate += 1
        
        # åˆ›å»ºå¹¶å‘çº¿ç¨‹
        threads = []
        for _ in range(concurrent_users):
            thread = threading.Thread(target=worker)
            threads.append(thread)
            thread.start()
        
        # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
        for thread in threads:
            thread.join()
        
        metrics.end_time = time.time()
        metrics.throughput = concurrent_users / metrics.duration
        metrics.error_rate /= concurrent_users * duration
        
        return metrics
    
    def stress_test(self, target_url: str, max_users: int, 
                   step_size: int) -> List[PerformanceMetrics]:
        """å‹åŠ›æµ‹è¯•"""
        logger.info(f"å¼€å§‹å‹åŠ›æµ‹è¯•: æœ€å¤§ {max_users} ç”¨æˆ·")
        
        results = []
        current_users = step_size
        
        while current_users <= max_users:
            metrics = self.load_test(target_url, current_users, 30)
            results.append(metrics)
            
            if metrics.error_rate > 0.1:  # é”™è¯¯ç‡è¶…è¿‡10%
                break
            
            current_users += step_size
        
        return results
    
    def spike_test(self, target_url: str, normal_users: int, 
                  spike_users: int, duration: int) -> PerformanceMetrics:
        """å°–å³°æµ‹è¯•"""
        logger.info(f"å¼€å§‹å°–å³°æµ‹è¯•: æ­£å¸¸ {normal_users} ç”¨æˆ·, å°–å³° {spike_users} ç”¨æˆ·")
        
        # æ­£å¸¸è´Ÿè½½é˜¶æ®µ
        normal_metrics = self.load_test(target_url, normal_users, duration // 2)
        
        # å°–å³°è´Ÿè½½é˜¶æ®µ
        spike_metrics = self.load_test(target_url, spike_users, duration // 2)
        
        # åˆå¹¶ç»“æœ
        combined_metrics = PerformanceMetrics()
        combined_metrics.response_time = (normal_metrics.response_time + spike_metrics.response_time) / 2
        combined_metrics.throughput = (normal_metrics.throughput + spike_metrics.throughput) / 2
        combined_metrics.error_rate = (normal_metrics.error_rate + spike_metrics.error_rate) / 2
        
        return combined_metrics

class SystemMonitor:
    """ç³»ç»Ÿç›‘æ§å™¨"""
    
    def __init__(self):
        self.metrics_history: List[Dict[str, float]] = []
    
    def start_monitoring(self) -> None:
        """å¼€å§‹ç›‘æ§"""
        self.monitoring = True
        self.monitor_thread = threading.Thread(target=self._monitor_loop)
        self.monitor_thread.start()
    
    def stop_monitoring(self) -> None:
        """åœæ­¢ç›‘æ§"""
        self.monitoring = False
        if hasattr(self, 'monitor_thread'):
            self.monitor_thread.join()
    
    def _monitor_loop(self) -> None:
        """ç›‘æ§å¾ªç¯"""
        while self.monitoring:
            metrics = {
                'timestamp': time.time(),
                'cpu_percent': psutil.cpu_percent(),
                'memory_percent': psutil.virtual_memory().percent,
                'disk_usage': psutil.disk_usage('/').percent
            }
            self.metrics_history.append(metrics)
            time.sleep(1)
    
    def get_average_metrics(self) -> Dict[str, float]:
        """è·å–å¹³å‡æŒ‡æ ‡"""
        if not self.metrics_history:
            return {}
        
        cpu_values = [m['cpu_percent'] for m in self.metrics_history]
        memory_values = [m['memory_percent'] for m in self.metrics_history]
        disk_values = [m['disk_usage'] for m in self.metrics_history]
        
        return {
            'avg_cpu': statistics.mean(cpu_values),
            'avg_memory': statistics.mean(memory_values),
            'avg_disk': statistics.mean(disk_values),
            'max_cpu': max(cpu_values),
            'max_memory': max(memory_values),
            'max_disk': max(disk_values)
        }

class PerformanceAnalyzer:
    """æ€§èƒ½åˆ†æå™¨"""
    
    def __init__(self):
        self.test_results: List[PerformanceMetrics] = []
    
    def add_result(self, result: PerformanceMetrics) -> None:
        """æ·»åŠ æµ‹è¯•ç»“æœ"""
        self.test_results.append(result)
    
    def analyze_performance(self) -> Dict[str, Any]:
        """åˆ†ææ€§èƒ½"""
        if not self.test_results:
            return {}
        
        response_times = [r.response_time for r in self.test_results]
        throughputs = [r.throughput for r in self.test_results]
        error_rates = [r.error_rate for r in self.test_results]
        
        return {
            'avg_response_time': statistics.mean(response_times),
            'max_response_time': max(response_times),
            'min_response_time': min(response_times),
            'avg_throughput': statistics.mean(throughputs),
            'max_throughput': max(throughputs),
            'avg_error_rate': statistics.mean(error_rates),
            'total_tests': len(self.test_results)
        }
    
    def generate_report(self) -> str:
        """ç”ŸæˆæŠ¥å‘Š"""
        analysis = self.analyze_performance()
        
        report = f"""
æ€§èƒ½æµ‹è¯•æŠ¥å‘Š
============

æµ‹è¯•ç»Ÿè®¡
--------
æ€»æµ‹è¯•æ•°: {analysis['total_tests']}
å¹³å‡å“åº”æ—¶é—´: {analysis['avg_response_time']:.3f}s
æœ€å¤§å“åº”æ—¶é—´: {analysis['max_response_time']:.3f}s
æœ€å°å“åº”æ—¶é—´: {analysis['min_response_time']:.3f}s
å¹³å‡ååé‡: {analysis['avg_throughput']:.2f} req/s
æœ€å¤§ååé‡: {analysis['max_throughput']:.2f} req/s
å¹³å‡é”™è¯¯ç‡: {analysis['avg_error_rate']:.2%}

æ€§èƒ½å»ºè®®
--------
"""
        
        if analysis['avg_response_time'] > 1.0:
            report += "- å“åº”æ—¶é—´è¿‡é•¿ï¼Œå»ºè®®ä¼˜åŒ–ä»£ç æˆ–å¢åŠ èµ„æº\n"
        
        if analysis['avg_error_rate'] > 0.05:
            report += "- é”™è¯¯ç‡è¿‡é«˜ï¼Œå»ºè®®æ£€æŸ¥ç³»ç»Ÿç¨³å®šæ€§\n"
        
        if analysis['avg_throughput'] < 100:
            report += "- ååé‡è¾ƒä½ï¼Œå»ºè®®ä¼˜åŒ–å¹¶å‘å¤„ç†\n"
        
        return report
    
    def plot_results(self, filename: str = "performance_results.png") -> None:
        """ç»˜åˆ¶ç»“æœå›¾è¡¨"""
        if len(self.test_results) < 2:
            return
        
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))
        
        # å“åº”æ—¶é—´å›¾
        response_times = [r.response_time for r in self.test_results]
        ax1.plot(response_times, marker='o')
        ax1.set_title('å“åº”æ—¶é—´å˜åŒ–')
        ax1.set_ylabel('å“åº”æ—¶é—´ (ç§’)')
        ax1.grid(True)
        
        # ååé‡å›¾
        throughputs = [r.throughput for r in self.test_results]
        ax2.plot(throughputs, marker='s', color='orange')
        ax2.set_title('ååé‡å˜åŒ–')
        ax2.set_ylabel('ååé‡ (req/s)')
        ax2.set_xlabel('æµ‹è¯•åºå·')
        ax2.grid(True)
        
        plt.tight_layout()
        plt.savefig(filename)
        plt.close()

# å®é™…åº”ç”¨ç¤ºä¾‹
def performance_test_example():
    """æ€§èƒ½æµ‹è¯•ç¤ºä¾‹"""
    print("=== æ€§èƒ½æµ‹è¯•ç¤ºä¾‹ ===")
    
    tester = PerformanceTester()
    monitor = SystemMonitor()
    analyzer = PerformanceAnalyzer()
    
    # æ¨¡æ‹Ÿç›®æ ‡URL
    target_url = "http://httpbin.org/delay/1"
    
    # 1. è´Ÿè½½æµ‹è¯•
    print("\n1. è´Ÿè½½æµ‹è¯•")
    load_result = tester.load_test(target_url, 10, 30)
    analyzer.add_result(load_result)
    print(f"  å“åº”æ—¶é—´: {load_result.response_time:.3f}s")
    print(f"  ååé‡: {load_result.throughput:.2f} req/s")
    print(f"  é”™è¯¯ç‡: {load_result.error_rate:.2%}")
    
    # 2. å‹åŠ›æµ‹è¯•
    print("\n2. å‹åŠ›æµ‹è¯•")
    stress_results = tester.stress_test(target_url, 50, 10)
    for i, result in enumerate(stress_results):
        analyzer.add_result(result)
        print(f"  {i+1}. ç”¨æˆ·æ•°: {(i+1)*10}, å“åº”æ—¶é—´: {result.response_time:.3f}s")
    
    # 3. å°–å³°æµ‹è¯•
    print("\n3. å°–å³°æµ‹è¯•")
    spike_result = tester.spike_test(target_url, 5, 20, 60)
    analyzer.add_result(spike_result)
    print(f"  å“åº”æ—¶é—´: {spike_result.response_time:.3f}s")
    print(f"  ååé‡: {spike_result.throughput:.2f} req/s")
    
    # 4. ç”ŸæˆæŠ¥å‘Š
    print("\n4. æ€§èƒ½åˆ†ææŠ¥å‘Š")
    report = analyzer.generate_report()
    print(report)
    
    # 5. ç»˜åˆ¶å›¾è¡¨
    analyzer.plot_results()

if __name__ == "__main__":
    performance_test_example()
```

## 3. æµ‹è¯•ç­–ç•¥

### 3.1 æµ‹è¯•ç±»å‹

1. **è´Ÿè½½æµ‹è¯•**: éªŒè¯ç³»ç»Ÿåœ¨æ­£å¸¸è´Ÿè½½ä¸‹çš„æ€§èƒ½
2. **å‹åŠ›æµ‹è¯•**: éªŒè¯ç³»ç»Ÿåœ¨æé™è´Ÿè½½ä¸‹çš„è¡¨ç°
3. **å°–å³°æµ‹è¯•**: éªŒè¯ç³»ç»Ÿå¯¹çªå‘è´Ÿè½½çš„å“åº”
4. **è€ä¹…æµ‹è¯•**: éªŒè¯ç³»ç»Ÿé•¿æœŸè¿è¡Œçš„ç¨³å®šæ€§

### 3.2 æ€§èƒ½åŸºå‡†

1. **å“åº”æ—¶é—´**: é€šå¸¸è¦æ±‚ < 1ç§’
2. **ååé‡**: æ ¹æ®ä¸šåŠ¡éœ€æ±‚è®¾å®š
3. **é”™è¯¯ç‡**: é€šå¸¸è¦æ±‚ < 1%
4. **èµ„æºä½¿ç”¨**: CPU < 80%, å†…å­˜ < 90%

## 4. æœ€ä½³å®è·µ

### 4.1 æµ‹è¯•ç¯å¢ƒ

1. **ç¯å¢ƒéš”ç¦»**: ä½¿ç”¨ç‹¬ç«‹çš„æµ‹è¯•ç¯å¢ƒ
2. **æ•°æ®å‡†å¤‡**: å‡†å¤‡è¶³å¤Ÿçš„æµ‹è¯•æ•°æ®
3. **ç›‘æ§å·¥å…·**: ä½¿ç”¨ä¸“ä¸šçš„ç›‘æ§å·¥å…·

### 4.2 æµ‹è¯•æ‰§è¡Œ

1. **æ¸è¿›å¼æµ‹è¯•**: ä»ä½è´Ÿè½½å¼€å§‹é€æ­¥å¢åŠ 
2. **æŒç»­ç›‘æ§**: å®æ—¶ç›‘æ§ç³»ç»Ÿèµ„æº
3. **ç»“æœåˆ†æ**: æ·±å…¥åˆ†ææ€§èƒ½ç“¶é¢ˆ

## 5. æ€»ç»“

æ€§èƒ½æµ‹è¯•æ˜¯ç¡®ä¿ç³»ç»Ÿè´¨é‡çš„é‡è¦ç¯èŠ‚ï¼Œé€šè¿‡ç³»ç»Ÿæ€§çš„æµ‹è¯•ç­–ç•¥å’Œå·¥å…·ï¼Œå¯ä»¥æœ‰æ•ˆéªŒè¯å’Œä¼˜åŒ–ç³»ç»Ÿæ€§èƒ½ã€‚

---

**ç›¸å…³æ–‡æ¡£**:

- [å•å…ƒæµ‹è¯•ç­–ç•¥](./07-04-01-å•å…ƒæµ‹è¯•ç­–ç•¥.md)
- [é›†æˆæµ‹è¯•ç­–ç•¥](./07-04-02-é›†æˆæµ‹è¯•ç­–ç•¥.md)
- [æ€§èƒ½ä¼˜åŒ–æœ€ä½³å®è·µ](../07-02-æœ€ä½³å®è·µ/07-02-04-æ€§èƒ½ä¼˜åŒ–æœ€ä½³å®è·µ.md)
