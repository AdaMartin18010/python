# 性能测试策略

## 📋 概述

性能测试是验证系统在特定负载下的性能表现，包括响应时间、吞吐量、资源使用率等关键指标。

## 1. 理论基础

### 1.1 性能指标定义

**定义 1.1** (响应时间)
响应时间是请求从发送到接收响应的时间：
$$T_{response} = T_{network} + T_{processing} + T_{database}$$

**定义 1.2** (吞吐量)
吞吐量是单位时间内处理的请求数量：
$$\text{Throughput} = \frac{\text{Request Count}}{\text{Time Period}}$$

**定义 1.3** (并发度)
并发度是同时处理的请求数量：
$$\text{Concurrency} = \text{Active Requests}$$

## 2. Python实现

### 2.1 性能测试框架

```python
from typing import Dict, List, Optional, Tuple, Any, Callable
from dataclasses import dataclass
from abc import ABC, abstractmethod
import time
import threading
import asyncio
import statistics
import psutil
import requests
from concurrent.futures import ThreadPoolExecutor
from enum import Enum
import logging
import json
import matplotlib.pyplot as plt

# 配置日志
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class TestType(Enum):
    """测试类型枚举"""
    LOAD_TEST = "load_test"
    STRESS_TEST = "stress_test"
    SPIKE_TEST = "spike_test"
    ENDURANCE_TEST = "endurance_test"

@dataclass
class PerformanceMetrics:
    """性能指标"""
    response_time: float = 0.0
    throughput: float = 0.0
    error_rate: float = 0.0
    cpu_usage: float = 0.0
    memory_usage: float = 0.0
    
    def __post_init__(self):
        self.start_time = time.time()
        self.end_time = None
    
    @property
    def duration(self) -> float:
        """测试持续时间"""
        if self.end_time:
            return self.end_time - self.start_time
        return time.time() - self.start_time

class PerformanceTester:
    """性能测试器"""
    
    def __init__(self):
        self.results: List[PerformanceMetrics] = []
        self.lock = threading.Lock()
    
    def load_test(self, target_url: str, concurrent_users: int, 
                  duration: int) -> PerformanceMetrics:
        """负载测试"""
        logger.info(f"开始负载测试: {concurrent_users} 并发用户, {duration}秒")
        
        metrics = PerformanceMetrics()
        start_time = time.time()
        
        def worker():
            while time.time() - start_time < duration:
                try:
                    request_start = time.time()
                    response = requests.get(target_url, timeout=10)
                    request_time = time.time() - request_start
                    
                    with self.lock:
                        metrics.response_time += request_time
                        if response.status_code != 200:
                            metrics.error_rate += 1
                except Exception as e:
                    with self.lock:
                        metrics.error_rate += 1
        
        # 创建并发线程
        threads = []
        for _ in range(concurrent_users):
            thread = threading.Thread(target=worker)
            threads.append(thread)
            thread.start()
        
        # 等待所有线程完成
        for thread in threads:
            thread.join()
        
        metrics.end_time = time.time()
        metrics.throughput = concurrent_users / metrics.duration
        metrics.error_rate /= concurrent_users * duration
        
        return metrics
    
    def stress_test(self, target_url: str, max_users: int, 
                   step_size: int) -> List[PerformanceMetrics]:
        """压力测试"""
        logger.info(f"开始压力测试: 最大 {max_users} 用户")
        
        results = []
        current_users = step_size
        
        while current_users <= max_users:
            metrics = self.load_test(target_url, current_users, 30)
            results.append(metrics)
            
            if metrics.error_rate > 0.1:  # 错误率超过10%
                break
            
            current_users += step_size
        
        return results
    
    def spike_test(self, target_url: str, normal_users: int, 
                  spike_users: int, duration: int) -> PerformanceMetrics:
        """尖峰测试"""
        logger.info(f"开始尖峰测试: 正常 {normal_users} 用户, 尖峰 {spike_users} 用户")
        
        # 正常负载阶段
        normal_metrics = self.load_test(target_url, normal_users, duration // 2)
        
        # 尖峰负载阶段
        spike_metrics = self.load_test(target_url, spike_users, duration // 2)
        
        # 合并结果
        combined_metrics = PerformanceMetrics()
        combined_metrics.response_time = (normal_metrics.response_time + spike_metrics.response_time) / 2
        combined_metrics.throughput = (normal_metrics.throughput + spike_metrics.throughput) / 2
        combined_metrics.error_rate = (normal_metrics.error_rate + spike_metrics.error_rate) / 2
        
        return combined_metrics

class SystemMonitor:
    """系统监控器"""
    
    def __init__(self):
        self.metrics_history: List[Dict[str, float]] = []
    
    def start_monitoring(self) -> None:
        """开始监控"""
        self.monitoring = True
        self.monitor_thread = threading.Thread(target=self._monitor_loop)
        self.monitor_thread.start()
    
    def stop_monitoring(self) -> None:
        """停止监控"""
        self.monitoring = False
        if hasattr(self, 'monitor_thread'):
            self.monitor_thread.join()
    
    def _monitor_loop(self) -> None:
        """监控循环"""
        while self.monitoring:
            metrics = {
                'timestamp': time.time(),
                'cpu_percent': psutil.cpu_percent(),
                'memory_percent': psutil.virtual_memory().percent,
                'disk_usage': psutil.disk_usage('/').percent
            }
            self.metrics_history.append(metrics)
            time.sleep(1)
    
    def get_average_metrics(self) -> Dict[str, float]:
        """获取平均指标"""
        if not self.metrics_history:
            return {}
        
        cpu_values = [m['cpu_percent'] for m in self.metrics_history]
        memory_values = [m['memory_percent'] for m in self.metrics_history]
        disk_values = [m['disk_usage'] for m in self.metrics_history]
        
        return {
            'avg_cpu': statistics.mean(cpu_values),
            'avg_memory': statistics.mean(memory_values),
            'avg_disk': statistics.mean(disk_values),
            'max_cpu': max(cpu_values),
            'max_memory': max(memory_values),
            'max_disk': max(disk_values)
        }

class PerformanceAnalyzer:
    """性能分析器"""
    
    def __init__(self):
        self.test_results: List[PerformanceMetrics] = []
    
    def add_result(self, result: PerformanceMetrics) -> None:
        """添加测试结果"""
        self.test_results.append(result)
    
    def analyze_performance(self) -> Dict[str, Any]:
        """分析性能"""
        if not self.test_results:
            return {}
        
        response_times = [r.response_time for r in self.test_results]
        throughputs = [r.throughput for r in self.test_results]
        error_rates = [r.error_rate for r in self.test_results]
        
        return {
            'avg_response_time': statistics.mean(response_times),
            'max_response_time': max(response_times),
            'min_response_time': min(response_times),
            'avg_throughput': statistics.mean(throughputs),
            'max_throughput': max(throughputs),
            'avg_error_rate': statistics.mean(error_rates),
            'total_tests': len(self.test_results)
        }
    
    def generate_report(self) -> str:
        """生成报告"""
        analysis = self.analyze_performance()
        
        report = f"""
性能测试报告
============

测试统计
--------
总测试数: {analysis['total_tests']}
平均响应时间: {analysis['avg_response_time']:.3f}s
最大响应时间: {analysis['max_response_time']:.3f}s
最小响应时间: {analysis['min_response_time']:.3f}s
平均吞吐量: {analysis['avg_throughput']:.2f} req/s
最大吞吐量: {analysis['max_throughput']:.2f} req/s
平均错误率: {analysis['avg_error_rate']:.2%}

性能建议
--------
"""
        
        if analysis['avg_response_time'] > 1.0:
            report += "- 响应时间过长，建议优化代码或增加资源\n"
        
        if analysis['avg_error_rate'] > 0.05:
            report += "- 错误率过高，建议检查系统稳定性\n"
        
        if analysis['avg_throughput'] < 100:
            report += "- 吞吐量较低，建议优化并发处理\n"
        
        return report
    
    def plot_results(self, filename: str = "performance_results.png") -> None:
        """绘制结果图表"""
        if len(self.test_results) < 2:
            return
        
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))
        
        # 响应时间图
        response_times = [r.response_time for r in self.test_results]
        ax1.plot(response_times, marker='o')
        ax1.set_title('响应时间变化')
        ax1.set_ylabel('响应时间 (秒)')
        ax1.grid(True)
        
        # 吞吐量图
        throughputs = [r.throughput for r in self.test_results]
        ax2.plot(throughputs, marker='s', color='orange')
        ax2.set_title('吞吐量变化')
        ax2.set_ylabel('吞吐量 (req/s)')
        ax2.set_xlabel('测试序号')
        ax2.grid(True)
        
        plt.tight_layout()
        plt.savefig(filename)
        plt.close()

# 实际应用示例
def performance_test_example():
    """性能测试示例"""
    print("=== 性能测试示例 ===")
    
    tester = PerformanceTester()
    monitor = SystemMonitor()
    analyzer = PerformanceAnalyzer()
    
    # 模拟目标URL
    target_url = "http://httpbin.org/delay/1"
    
    # 1. 负载测试
    print("\n1. 负载测试")
    load_result = tester.load_test(target_url, 10, 30)
    analyzer.add_result(load_result)
    print(f"  响应时间: {load_result.response_time:.3f}s")
    print(f"  吞吐量: {load_result.throughput:.2f} req/s")
    print(f"  错误率: {load_result.error_rate:.2%}")
    
    # 2. 压力测试
    print("\n2. 压力测试")
    stress_results = tester.stress_test(target_url, 50, 10)
    for i, result in enumerate(stress_results):
        analyzer.add_result(result)
        print(f"  {i+1}. 用户数: {(i+1)*10}, 响应时间: {result.response_time:.3f}s")
    
    # 3. 尖峰测试
    print("\n3. 尖峰测试")
    spike_result = tester.spike_test(target_url, 5, 20, 60)
    analyzer.add_result(spike_result)
    print(f"  响应时间: {spike_result.response_time:.3f}s")
    print(f"  吞吐量: {spike_result.throughput:.2f} req/s")
    
    # 4. 生成报告
    print("\n4. 性能分析报告")
    report = analyzer.generate_report()
    print(report)
    
    # 5. 绘制图表
    analyzer.plot_results()

if __name__ == "__main__":
    performance_test_example()
```

## 3. 测试策略

### 3.1 测试类型

1. **负载测试**: 验证系统在正常负载下的性能
2. **压力测试**: 验证系统在极限负载下的表现
3. **尖峰测试**: 验证系统对突发负载的响应
4. **耐久测试**: 验证系统长期运行的稳定性

### 3.2 性能基准

1. **响应时间**: 通常要求 < 1秒
2. **吞吐量**: 根据业务需求设定
3. **错误率**: 通常要求 < 1%
4. **资源使用**: CPU < 80%, 内存 < 90%

## 4. 最佳实践

### 4.1 测试环境

1. **环境隔离**: 使用独立的测试环境
2. **数据准备**: 准备足够的测试数据
3. **监控工具**: 使用专业的监控工具

### 4.2 测试执行

1. **渐进式测试**: 从低负载开始逐步增加
2. **持续监控**: 实时监控系统资源
3. **结果分析**: 深入分析性能瓶颈

## 5. 总结

性能测试是确保系统质量的重要环节，通过系统性的测试策略和工具，可以有效验证和优化系统性能。

---

**相关文档**:

- [单元测试策略](./07-04-01-单元测试策略.md)
- [集成测试策略](./07-04-02-集成测试策略.md)
- [性能优化最佳实践](../07-02-最佳实践/07-02-04-性能优化最佳实践.md)
