# 自然语言处理实战

## 7.5 自然语言处理实战

### 概念与流程

自然语言处理(NLP)是人工智能的重要分支，致力于让计算机理解、生成和处理人类语言。

#### 典型NLP流程

1. 文本预处理
2. 特征提取
3. 模型训练
4. 结果评估
5. 应用部署

### 文本预处理

#### 1. 基础文本清理

```python
import re
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer

# 下载必要的NLTK数据
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

def text_preprocessing(text):
    """基础文本预处理"""
    # 转换为小写
    text = text.lower()
    
    # 移除特殊字符
    text = re.sub(r'[^\w\s]', '', text)
    
    # 移除多余空格
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text

def tokenization(text):
    """分词"""
    # 句子分词
    sentences = sent_tokenize(text)
    
    # 单词分词
    words = word_tokenize(text)
    
    return sentences, words

def remove_stopwords(words):
    """移除停用词"""
    stop_words = set(stopwords.words('english'))
    filtered_words = [word for word in words if word not in stop_words]
    return filtered_words

def stemming(words):
    """词干提取"""
    stemmer = PorterStemmer()
    stemmed_words = [stemmer.stem(word) for word in words]
    return stemmed_words

def lemmatization(words):
    """词形还原"""
    lemmatizer = WordNetLemmatizer()
    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]
    return lemmatized_words

def complete_preprocessing(text):
    """完整的文本预处理流程"""
    # 基础清理
    text = text_preprocessing(text)
    
    # 分词
    sentences, words = tokenization(text)
    
    # 移除停用词
    words = remove_stopwords(words)
    
    # 词形还原
    words = lemmatization(words)
    
    return sentences, words
```

#### 2. 高级文本处理

```python
import spacy
from textblob import TextBlob

# 加载spaCy模型
nlp = spacy.load("en_core_web_sm")

def advanced_preprocessing(text):
    """使用spaCy进行高级文本处理"""
    doc = nlp(text)
    
    # 命名实体识别
    entities = [(ent.text, ent.label_) for ent in doc.ents]
    
    # 词性标注
    pos_tags = [(token.text, token.pos_) for token in doc]
    
    # 依存句法分析
    dependencies = [(token.text, token.dep_, token.head.text) for token in doc]
    
    return entities, pos_tags, dependencies

def sentiment_analysis_basic(text):
    """基础情感分析"""
    blob = TextBlob(text)
    return blob.sentiment.polarity, blob.sentiment.subjectivity
```

### 词向量与表示学习

#### 1. Word2Vec实现

```python
import numpy as np
from gensim.models import Word2Vec
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

def train_word2vec(sentences, vector_size=100, window=5, min_count=1):
    """训练Word2Vec模型"""
    model = Word2Vec(sentences, 
                     vector_size=vector_size, 
                     window=window, 
                     min_count=min_count, 
                     workers=4)
    return model

def word_similarity(model, word1, word2):
    """计算词相似度"""
    try:
        similarity = model.wv.similarity(word1, word2)
        return similarity
    except KeyError:
        return None

def find_similar_words(model, word, topn=10):
    """查找相似词"""
    try:
        similar_words = model.wv.most_similar(word, topn=topn)
        return similar_words
    except KeyError:
        return []

def visualize_word_vectors(model, words):
    """可视化词向量"""
    # 获取词向量
    word_vectors = []
    valid_words = []
    
    for word in words:
        try:
            vector = model.wv[word]
            word_vectors.append(vector)
            valid_words.append(word)
        except KeyError:
            continue
    
    if not word_vectors:
        return
    
    # 降维到2D
    pca = PCA(n_components=2)
    vectors_2d = pca.fit_transform(word_vectors)
    
    # 可视化
    plt.figure(figsize=(10, 8))
    plt.scatter(vectors_2d[:, 0], vectors_2d[:, 1])
    
    for i, word in enumerate(valid_words):
        plt.annotate(word, (vectors_2d[i, 0], vectors_2d[i, 1]))
    
    plt.title('Word Vectors Visualization')
    plt.show()
```

#### 2. TF-IDF向量化

```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

def tfidf_vectorization(documents):
    """TF-IDF向量化"""
    vectorizer = TfidfVectorizer(
        max_features=1000,
        stop_words='english',
        ngram_range=(1, 2)
    )
    
    tfidf_matrix = vectorizer.fit_transform(documents)
    feature_names = vectorizer.get_feature_names_out()
    
    return tfidf_matrix, feature_names, vectorizer

def document_similarity(tfidf_matrix, doc1_idx, doc2_idx):
    """计算文档相似度"""
    similarity = cosine_similarity(
        tfidf_matrix[doc1_idx:doc1_idx+1], 
        tfidf_matrix[doc2_idx:doc2_idx+1]
    )[0][0]
    return similarity
```

### 文本分类

#### 1. 朴素贝叶斯分类器

```python
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import pandas as pd

def naive_bayes_classification(X_train, X_test, y_train, y_test):
    """朴素贝叶斯文本分类"""
    # 训练模型
    nb_classifier = MultinomialNB()
    nb_classifier.fit(X_train, y_train)
    
    # 预测
    y_pred = nb_classifier.predict(X_test)
    
    # 评估
    print("分类报告:")
    print(classification_report(y_test, y_pred))
    
    print("混淆矩阵:")
    print(confusion_matrix(y_test, y_pred))
    
    return nb_classifier

# 示例：新闻分类
def news_classification_example():
    """新闻分类示例"""
    # 假设数据
    news_data = [
        ("Apple releases new iPhone", "technology"),
        ("Stock market reaches new high", "business"),
        ("Scientists discover new species", "science"),
        ("Team wins championship", "sports"),
        ("New restaurant opens downtown", "lifestyle")
    ]
    
    texts = [item[0] for item in news_data]
    labels = [item[1] for item in news_data]
    
    # TF-IDF向量化
    tfidf_matrix, feature_names, vectorizer = tfidf_vectorization(texts)
    
    # 划分训练测试集
    X_train, X_test, y_train, y_test = train_test_split(
        tfidf_matrix, labels, test_size=0.2, random_state=42
    )
    
    # 训练分类器
    classifier = naive_bayes_classification(X_train, X_test, y_train, y_test)
    
    return classifier, vectorizer
```

#### 2. 支持向量机分类器

```python
from sklearn.svm import SVC
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import CountVectorizer

def svm_text_classification(X_train, X_test, y_train, y_test):
    """SVM文本分类"""
    # 创建管道
    pipeline = Pipeline([
        ('vectorizer', CountVectorizer(max_features=1000)),
        ('classifier', SVC(kernel='linear', random_state=42))
    ])
    
    # 训练模型
    pipeline.fit(X_train, y_train)
    
    # 预测
    y_pred = pipeline.predict(X_test)
    
    # 评估
    print("SVM分类报告:")
    print(classification_report(y_test, y_pred))
    
    return pipeline
```

### 情感分析

#### 1. 基于词典的情感分析

```python
def lexicon_based_sentiment(text):
    """基于词典的情感分析"""
    # 情感词典（简化版）
    positive_words = {'good', 'great', 'excellent', 'amazing', 'wonderful', 'love', 'like'}
    negative_words = {'bad', 'terrible', 'awful', 'hate', 'dislike', 'horrible', 'worst'}
    
    words = text.lower().split()
    
    positive_count = sum(1 for word in words if word in positive_words)
    negative_count = sum(1 for word in words if word in negative_words)
    
    if positive_count > negative_count:
        return 'positive'
    elif negative_count > positive_count:
        return 'negative'
    else:
        return 'neutral'

def vader_sentiment_analysis(text):
    """使用VADER进行情感分析"""
    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
    
    analyzer = SentimentIntensityAnalyzer()
    scores = analyzer.polarity_scores(text)
    
    return scores
```

#### 2. 机器学习情感分析

```python
def ml_sentiment_analysis():
    """机器学习情感分析"""
    # 示例数据
    reviews = [
        ("This product is amazing!", "positive"),
        ("I love this movie!", "positive"),
        ("Terrible service, very disappointed.", "negative"),
        ("The food was okay, nothing special.", "neutral"),
        ("Best purchase ever!", "positive"),
        ("Waste of money, don't buy.", "negative")
    ]
    
    texts = [review[0] for review in reviews]
    labels = [review[1] for review in reviews]
    
    # TF-IDF向量化
    tfidf_matrix, feature_names, vectorizer = tfidf_vectorization(texts)
    
    # 训练分类器
    classifier = MultinomialNB()
    classifier.fit(tfidf_matrix, labels)
    
    return classifier, vectorizer

def predict_sentiment(classifier, vectorizer, text):
    """预测文本情感"""
    # 预处理文本
    processed_text = text_preprocessing(text)
    
    # 向量化
    text_vector = vectorizer.transform([processed_text])
    
    # 预测
    prediction = classifier.predict(text_vector)[0]
    probability = classifier.predict_proba(text_vector)[0]
    
    return prediction, probability
```

### 命名实体识别

```python
def named_entity_recognition(text):
    """命名实体识别"""
    doc = nlp(text)
    
    entities = []
    for ent in doc.ents:
        entities.append({
            'text': ent.text,
            'label': ent.label_,
            'start': ent.start_char,
            'end': ent.end_char
        })
    
    return entities

def extract_person_names(text):
    """提取人名"""
    doc = nlp(text)
    person_names = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']
    return person_names

def extract_organizations(text):
    """提取组织名"""
    doc = nlp(text)
    organizations = [ent.text for ent in doc.ents if ent.label_ == 'ORG']
    return organizations
```

### 文本摘要

#### 1. 抽取式摘要

```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

def extractive_summarization(text, num_sentences=3):
    """抽取式文本摘要"""
    # 分句
    sentences = sent_tokenize(text)
    
    if len(sentences) <= num_sentences:
        return sentences
    
    # TF-IDF向量化
    vectorizer = TfidfVectorizer(stop_words='english')
    tfidf_matrix = vectorizer.fit_transform(sentences)
    
    # 计算句子相似度
    similarity_matrix = cosine_similarity(tfidf_matrix)
    
    # 计算句子重要性分数
    sentence_scores = similarity_matrix.sum(axis=1)
    
    # 选择得分最高的句子
    top_indices = sentence_scores.argsort()[-num_sentences:][::-1]
    top_indices = sorted(top_indices)
    
    summary = [sentences[i] for i in top_indices]
    return summary
```

#### 2. 关键词提取

```python
from sklearn.feature_extraction.text import TfidfVectorizer

def extract_keywords(text, num_keywords=10):
    """提取关键词"""
    # 预处理
    processed_text = text_preprocessing(text)
    
    # TF-IDF向量化
    vectorizer = TfidfVectorizer(
        max_features=100,
        stop_words='english',
        ngram_range=(1, 2)
    )
    
    tfidf_matrix = vectorizer.fit_transform([processed_text])
    feature_names = vectorizer.get_feature_names_out()
    
    # 获取TF-IDF分数
    scores = tfidf_matrix.toarray()[0]
    
    # 排序并返回关键词
    keyword_scores = list(zip(feature_names, scores))
    keyword_scores.sort(key=lambda x: x[1], reverse=True)
    
    return keyword_scores[:num_keywords]
```

### 机器翻译基础

```python
def simple_translation_memory(source_text, target_text, new_source):
    """简单的翻译记忆"""
    # 构建翻译词典
    translation_dict = {}
    
    # 分词
    source_words = word_tokenize(source_text.lower())
    target_words = word_tokenize(target_text.lower())
    
    # 简单的词对齐（假设长度相同）
    if len(source_words) == len(target_words):
        for src_word, tgt_word in zip(source_words, target_words):
            translation_dict[src_word] = tgt_word
    
    # 翻译新文本
    new_words = word_tokenize(new_source.lower())
    translated_words = []
    
    for word in new_words:
        if word in translation_dict:
            translated_words.append(translation_dict[word])
        else:
            translated_words.append(word)  # 保持原词
    
    return ' '.join(translated_words)

# 使用transformers进行翻译
def transformer_translation(text, source_lang='en', target_lang='zh'):
    """使用预训练模型进行翻译"""
    try:
        from transformers import MarianMTModel, MarianTokenizer
        
        model_name = f'Helsinki-NLP/opus-mt-{source_lang}-{target_lang}'
        tokenizer = MarianTokenizer.from_pretrained(model_name)
        model = MarianMTModel.from_pretrained(model_name)
        
        # 编码
        inputs = tokenizer(text, return_tensors="pt", padding=True)
        
        # 翻译
        translated = model.generate(**inputs)
        
        # 解码
        translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)
        
        return translated_text
    except Exception as e:
        return f"翻译失败: {str(e)}"
```

### 实战案例：新闻分类系统

```python
def news_classification_system():
    """完整的新闻分类系统"""
    # 示例新闻数据
    news_data = [
        ("Apple launches new iPhone with advanced features", "technology"),
        ("Stock market hits record high as economy recovers", "business"),
        ("Scientists discover new species in Amazon rainforest", "science"),
        ("Local team wins championship after dramatic final", "sports"),
        ("New restaurant opens with innovative menu", "lifestyle"),
        ("Tech company reports record quarterly profits", "business"),
        ("Research shows benefits of exercise on mental health", "science"),
        ("Famous actor announces retirement from acting", "entertainment")
    ]
    
    # 准备数据
    texts = [item[0] for item in news_data]
    labels = [item[1] for item in news_data]
    
    # 文本预处理
    processed_texts = []
    for text in texts:
        processed_text = text_preprocessing(text)
        processed_texts.append(processed_text)
    
    # TF-IDF向量化
    tfidf_matrix, feature_names, vectorizer = tfidf_vectorization(processed_texts)
    
    # 训练分类器
    classifier = MultinomialNB()
    classifier.fit(tfidf_matrix, labels)
    
    # 测试新文本
    test_text = "New smartphone features breakthrough camera technology"
    processed_test = text_preprocessing(test_text)
    test_vector = vectorizer.transform([processed_test])
    
    prediction = classifier.predict(test_vector)[0]
    probability = classifier.predict_proba(test_vector)[0]
    
    print(f"测试文本: {test_text}")
    print(f"预测类别: {prediction}")
    print(f"置信度: {max(probability):.3f}")
    
    return classifier, vectorizer

# 运行示例
if __name__ == "__main__":
    print("=== 新闻分类系统 ===")
    classifier, vectorizer = news_classification_system()
    
    print("\n=== 情感分析示例 ===")
    sentiment_text = "I absolutely love this product! It's amazing!"
    sentiment = vader_sentiment_analysis(sentiment_text)
    print(f"文本: {sentiment_text}")
    print(f"情感分析结果: {sentiment}")
    
    print("\n=== 文本摘要示例 ===")
    long_text = """
    Natural language processing (NLP) is a subfield of linguistics, computer science, 
    and artificial intelligence concerned with the interactions between computers and 
    human language, in particular how to program computers to process and analyze large 
    amounts of natural language data. Challenges in natural language processing frequently 
    involve speech recognition, natural language understanding, and natural language generation.
    """
    summary = extractive_summarization(long_text, num_sentences=2)
    print("原文摘要:")
    for i, sentence in enumerate(summary, 1):
        print(f"{i}. {sentence}")
```

### 理论总结

自然语言处理是人工智能的重要应用领域，掌握文本预处理、特征提取、模型训练等核心技术，能够构建各种实用的NLP应用系统。
