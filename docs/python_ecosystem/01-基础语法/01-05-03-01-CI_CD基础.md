# CI/CDåŸºç¡€

## ğŸ“‹ æ¦‚è¿°

CI/CDï¼ˆæŒç»­é›†æˆ/æŒç»­éƒ¨ç½²ï¼‰æ˜¯ç°ä»£è½¯ä»¶å¼€å‘çš„æ ¸å¿ƒç†å¿µï¼Œé€šè¿‡è‡ªåŠ¨åŒ–æµç¨‹å®ç°ä»£ç çš„å¿«é€Ÿã€å¯é ã€é¢‘ç¹çš„é›†æˆå’Œéƒ¨ç½²ã€‚

## ğŸ¯ æ ¸å¿ƒæ¦‚å¿µ

### æŒç»­é›†æˆ (Continuous Integration)

**å®šä¹‰**ï¼šæŒç»­é›†æˆæ˜¯ä¸€ç§å¼€å‘å®è·µï¼Œè¦æ±‚å¼€å‘äººå‘˜é¢‘ç¹åœ°å°†ä»£ç é›†æˆåˆ°ä¸»å¹²åˆ†æ”¯ï¼Œæ¯æ¬¡é›†æˆéƒ½é€šè¿‡è‡ªåŠ¨åŒ–æ„å»ºå’Œæµ‹è¯•éªŒè¯ã€‚

**æ•°å­¦è¡¨ç¤º**ï¼š
$$CI(S, T, B) = \{s \in S | \forall t \in T, B(s, t) = \text{success}\}$$

å…¶ä¸­ï¼š

- $S$ æ˜¯ä»£ç æäº¤é›†åˆ
- $T$ æ˜¯æµ‹è¯•é›†åˆ
- $B$ æ˜¯æ„å»ºå‡½æ•°

### æŒç»­éƒ¨ç½² (Continuous Deployment)

**å®šä¹‰**ï¼šæŒç»­éƒ¨ç½²æ˜¯æŒç»­é›†æˆçš„å»¶ä¼¸ï¼Œå°†é€šè¿‡æ‰€æœ‰æµ‹è¯•çš„ä»£ç è‡ªåŠ¨éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒã€‚

**æ•°å­¦è¡¨ç¤º**ï¼š
$$CD(CI\_Result, E) = \begin{cases}
\text{deploy} & \text{if } CI\_Result = \text{success} \land E = \text{ready} \\
\text{hold} & \text{otherwise}
\end{cases}$$

å…¶ä¸­ $E$ æ˜¯ç¯å¢ƒå°±ç»ªçŠ¶æ€ã€‚

## ğŸ”§ Pythonå®ç°

### CI/CDæµæ°´çº¿æ¡†æ¶

```python
from abc import ABC, abstractmethod
from typing import Dict, List, Any, Optional, Callable
from dataclasses import dataclass, field
from enum import Enum
import asyncio
import json
import logging
from datetime import datetime
import subprocess
import os
import yaml

# æµæ°´çº¿çŠ¶æ€
class PipelineStatus(Enum):
    PENDING = "pending"
    RUNNING = "running"
    SUCCESS = "success"
    FAILED = "failed"
    CANCELLED = "cancelled"

# é˜¶æ®µçŠ¶æ€
class StageStatus(Enum):
    PENDING = "pending"
    RUNNING = "running"
    SUCCESS = "success"
    FAILED = "failed"
    SKIPPED = "skipped"

# æµæ°´çº¿é…ç½®
@dataclass
class PipelineConfig:
    name: str
    version: str
    stages: List[str]
    triggers: List[str] = field(default_factory=list)
    timeout: int = 3600  # 1å°æ—¶
    parallel_stages: List[str] = field(default_factory=list)
    environment_variables: Dict[str, str] = field(default_factory=dict)

# æµæ°´çº¿æ‰§è¡Œä¸Šä¸‹æ–‡
@dataclass
class PipelineContext:
    pipeline_id: str
    commit_hash: str
    branch: str
    author: str
    timestamp: datetime
    variables: Dict[str, Any] = field(default_factory=dict)
    artifacts: Dict[str, str] = field(default_factory=dict)

# æµæ°´çº¿é˜¶æ®µ
class PipelineStage(ABC):
    """æµæ°´çº¿é˜¶æ®µæŠ½è±¡åŸºç±»"""

    def __init__(self, name: str, config: Dict[str, Any]):
        self.name = name
        self.config = config
        self.status = StageStatus.PENDING
        self.start_time: Optional[datetime] = None
        self.end_time: Optional[datetime] = None
        self.logger = logging.getLogger(f"pipeline.stage.{name}")

    @abstractmethod
    async def execute(self, context: PipelineContext) -> bool:
        """æ‰§è¡Œé˜¶æ®µ"""
        pass

    def get_duration(self) -> Optional[float]:
        """è·å–æ‰§è¡Œæ—¶é•¿"""
        if self.start_time and self.end_time:
            return (self.end_time - self.start_time).total_seconds()
        return None

# ä»£ç æ£€å‡ºé˜¶æ®µ
class CheckoutStage(PipelineStage):
    """ä»£ç æ£€å‡ºé˜¶æ®µ"""

    async def execute(self, context: PipelineContext) -> bool:
        """æ‰§è¡Œä»£ç æ£€å‡º"""
        self.status = StageStatus.RUNNING
        self.start_time = datetime.now()

        try:
            repo_url = self.config.get("repository_url")
            branch = context.branch

            # æ‰§è¡Œgit clone
            cmd = f"git clone -b {branch} {repo_url} ."
            result = subprocess.run(cmd, shell=True, capture_output=True, text=True)

            if result.returncode == 0:
                self.status = StageStatus.SUCCESS
                self.logger.info(f"Code checkout successful: {branch}")
                return True
            else:
                self.status = StageStatus.FAILED
                self.logger.error(f"Code checkout failed: {result.stderr}")
                return False

        except Exception as e:
            self.status = StageStatus.FAILED
            self.logger.error(f"Code checkout error: {e}")
            return False
        finally:
            self.end_time = datetime.now()

# æ„å»ºé˜¶æ®µ
class BuildStage(PipelineStage):
    """æ„å»ºé˜¶æ®µ"""

    async def execute(self, context: PipelineContext) -> bool:
        """æ‰§è¡Œæ„å»º"""
        self.status = StageStatus.RUNNING
        self.start_time = datetime.now()

        try:
            build_command = self.config.get("build_command", "make build")
            build_args = self.config.get("build_args", [])

            # æ‰§è¡Œæ„å»ºå‘½ä»¤
            cmd = [build_command] + build_args
            result = subprocess.run(cmd, capture_output=True, text=True)

            if result.returncode == 0:
                self.status = StageStatus.SUCCESS
                self.logger.info("Build successful")

                # ä¿å­˜æ„å»ºäº§ç‰©
                artifact_path = self.config.get("artifact_path")
                if artifact_path and os.path.exists(artifact_path):
                    context.artifacts["build"] = artifact_path

                return True
            else:
                self.status = StageStatus.FAILED
                self.logger.error(f"Build failed: {result.stderr}")
                return False

        except Exception as e:
            self.status = StageStatus.FAILED
            self.logger.error(f"Build error: {e}")
            return False
        finally:
            self.end_time = datetime.now()

# æµ‹è¯•é˜¶æ®µ
class TestStage(PipelineStage):
    """æµ‹è¯•é˜¶æ®µ"""

    async def execute(self, context: PipelineContext) -> bool:
        """æ‰§è¡Œæµ‹è¯•"""
        self.status = StageStatus.RUNNING
        self.start_time = datetime.now()

        try:
            test_command = self.config.get("test_command", "pytest")
            test_args = self.config.get("test_args", [])
            coverage_threshold = self.config.get("coverage_threshold", 80.0)

            # æ‰§è¡Œæµ‹è¯•å‘½ä»¤
            cmd = [test_command] + test_args
            result = subprocess.run(cmd, capture_output=True, text=True)

            if result.returncode == 0:
                # æ£€æŸ¥è¦†ç›–ç‡
                coverage_result = self._check_coverage(coverage_threshold)
                if coverage_result:
                    self.status = StageStatus.SUCCESS
                    self.logger.info("Tests passed with coverage threshold")
                    return True
                else:
                    self.status = StageStatus.FAILED
                    self.logger.error("Coverage threshold not met")
                    return False
            else:
                self.status = StageStatus.FAILED
                self.logger.error(f"Tests failed: {result.stderr}")
                return False

        except Exception as e:
            self.status = StageStatus.FAILED
            self.logger.error(f"Test error: {e}")
            return False
        finally:
            self.end_time = datetime.now()

    def _check_coverage(self, threshold: float) -> bool:
        """æ£€æŸ¥æµ‹è¯•è¦†ç›–ç‡"""
        try:
            # è¿™é‡Œåº”è¯¥å®ç°å…·ä½“çš„è¦†ç›–ç‡æ£€æŸ¥é€»è¾‘
            # ä¾‹å¦‚è§£æcoverage.xmlæ–‡ä»¶
            return True  # ç®€åŒ–å®ç°
        except Exception as e:
            self.logger.error(f"Coverage check error: {e}")
            return False

# ä»£ç è´¨é‡æ£€æŸ¥é˜¶æ®µ
class CodeQualityStage(PipelineStage):
    """ä»£ç è´¨é‡æ£€æŸ¥é˜¶æ®µ"""

    async def execute(self, context: PipelineContext) -> bool:
        """æ‰§è¡Œä»£ç è´¨é‡æ£€æŸ¥"""
        self.status = StageStatus.RUNNING
        self.start_time = datetime.now()

        try:
            quality_tools = self.config.get("tools", ["flake8", "pylint"])
            max_score = self.config.get("max_score", 8.0)

            all_passed = True

            for tool in quality_tools:
                if tool == "flake8":
                    result = subprocess.run(["flake8", "."], capture_output=True, text=True)
                    if result.returncode != 0:
                        self.logger.warning(f"Flake8 issues found: {result.stdout}")
                        all_passed = False

                elif tool == "pylint":
                    result = subprocess.run(["pylint", "--score=y", "."], capture_output=True, text=True)
                    if result.returncode != 0:
                        # è§£æpylintåˆ†æ•°
                        score_line = [line for line in result.stdout.split('\n') if 'Your code has been rated at' in line]
                        if score_line:
                            score = float(score_line[0].split()[-2])
                            if score < max_score:
                                self.logger.warning(f"Pylint score {score} below threshold {max_score}")
                                all_passed = False

            if all_passed:
                self.status = StageStatus.SUCCESS
                self.logger.info("Code quality checks passed")
                return True
            else:
                self.status = StageStatus.FAILED
                self.logger.error("Code quality checks failed")
                return False

        except Exception as e:
            self.status = StageStatus.FAILED
            self.logger.error(f"Code quality check error: {e}")
            return False
        finally:
            self.end_time = datetime.now()

# éƒ¨ç½²é˜¶æ®µ
class DeployStage(PipelineStage):
    """éƒ¨ç½²é˜¶æ®µ"""

    async def execute(self, context: PipelineContext) -> bool:
        """æ‰§è¡Œéƒ¨ç½²"""
        self.status = StageStatus.RUNNING
        self.start_time = datetime.now()

        try:
            environment = self.config.get("environment", "staging")
            deployment_strategy = self.config.get("strategy", "rolling")

            if deployment_strategy == "rolling":
                success = await self._rolling_deploy(environment, context)
            elif deployment_strategy == "blue_green":
                success = await self._blue_green_deploy(environment, context)
            else:
                success = await self._simple_deploy(environment, context)

            if success:
                self.status = StageStatus.SUCCESS
                self.logger.info(f"Deployment to {environment} successful")
                return True
            else:
                self.status = StageStatus.FAILED
                self.logger.error(f"Deployment to {environment} failed")
                return False

        except Exception as e:
            self.status = StageStatus.FAILED
            self.logger.error(f"Deployment error: {e}")
            return False
        finally:
            self.end_time = datetime.now()

    async def _rolling_deploy(self, environment: str, context: PipelineContext) -> bool:
        """æ»šåŠ¨éƒ¨ç½²"""
        # å®ç°æ»šåŠ¨éƒ¨ç½²é€»è¾‘
        self.logger.info(f"Performing rolling deployment to {environment}")
        await asyncio.sleep(5)  # æ¨¡æ‹Ÿéƒ¨ç½²æ—¶é—´
        return True

    async def _blue_green_deploy(self, environment: str, context: PipelineContext) -> bool:
        """è“ç»¿éƒ¨ç½²"""
        # å®ç°è“ç»¿éƒ¨ç½²é€»è¾‘
        self.logger.info(f"Performing blue-green deployment to {environment}")
        await asyncio.sleep(10)  # æ¨¡æ‹Ÿéƒ¨ç½²æ—¶é—´
        return True

    async def _simple_deploy(self, environment: str, context: PipelineContext) -> bool:
        """ç®€å•éƒ¨ç½²"""
        # å®ç°ç®€å•éƒ¨ç½²é€»è¾‘
        self.logger.info(f"Performing simple deployment to {environment}")
        await asyncio.sleep(3)  # æ¨¡æ‹Ÿéƒ¨ç½²æ—¶é—´
        return True

# æµæ°´çº¿æ‰§è¡Œå™¨
class PipelineExecutor:
    """æµæ°´çº¿æ‰§è¡Œå™¨"""

    def __init__(self):
        self.stages: Dict[str, PipelineStage] = {}
        self.logger = logging.getLogger("pipeline_executor")

    def register_stage(self, stage: PipelineStage) -> None:
        """æ³¨å†Œæµæ°´çº¿é˜¶æ®µ"""
        self.stages[stage.name] = stage
        self.logger.info(f"Stage registered: {stage.name}")

    async def execute_pipeline(self, config: PipelineConfig, context: PipelineContext) -> bool:
        """æ‰§è¡Œå®Œæ•´æµæ°´çº¿"""
        self.logger.info(f"Starting pipeline: {config.name}")

        # åˆ›å»ºé˜¶æ®µå®ä¾‹
        stage_instances = self._create_stage_instances(config)

        # æ‰§è¡Œé˜¶æ®µ
        for stage_name in config.stages:
            if stage_name not in stage_instances:
                self.logger.error(f"Stage not found: {stage_name}")
                return False

            stage = stage_instances[stage_name]
            self.logger.info(f"Executing stage: {stage_name}")

            success = await stage.execute(context)
            if not success:
                self.logger.error(f"Stage failed: {stage_name}")
                return False

        self.logger.info(f"Pipeline completed successfully: {config.name}")
        return True

    def _create_stage_instances(self, config: PipelineConfig) -> Dict[str, PipelineStage]:
        """åˆ›å»ºé˜¶æ®µå®ä¾‹"""
        instances = {}

        # è¿™é‡Œåº”è¯¥æ ¹æ®é…ç½®åˆ›å»ºå…·ä½“çš„é˜¶æ®µå®ä¾‹
        # ç®€åŒ–å®ç°ï¼Œå®é™…åº”è¯¥ä»é…ç½®æ–‡ä»¶æˆ–æ•°æ®åº“åŠ è½½
        stage_configs = {
            "checkout": {"repository_url": "https://github.com/example/repo.git"},
            "build": {"build_command": "make", "build_args": ["build"]},
            "test": {"test_command": "pytest", "coverage_threshold": 80.0},
            "quality": {"tools": ["flake8", "pylint"], "max_score": 8.0},
            "deploy": {"environment": "staging", "strategy": "rolling"}
        }

        for stage_name in config.stages:
            if stage_name == "checkout":
                instances[stage_name] = CheckoutStage(stage_name, stage_configs.get(stage_name, {}))
            elif stage_name == "build":
                instances[stage_name] = BuildStage(stage_name, stage_configs.get(stage_name, {}))
            elif stage_name == "test":
                instances[stage_name] = TestStage(stage_name, stage_configs.get(stage_name, {}))
            elif stage_name == "quality":
                instances[stage_name] = CodeQualityStage(stage_name, stage_configs.get(stage_name, {}))
            elif stage_name == "deploy":
                instances[stage_name] = DeployStage(stage_name, stage_configs.get(stage_name, {}))

        return instances
```

### æµæ°´çº¿é…ç½®ç®¡ç†

```python
import yaml
from typing import Dict, Any, List
from pathlib import Path

class PipelineConfigManager:
    """æµæ°´çº¿é…ç½®ç®¡ç†å™¨"""

    def __init__(self, config_dir: str = "pipelines"):
        self.config_dir = Path(config_dir)
        self.config_dir.mkdir(exist_ok=True)

    def load_pipeline_config(self, pipeline_name: str) -> PipelineConfig:
        """åŠ è½½æµæ°´çº¿é…ç½®"""
        config_file = self.config_dir / f"{pipeline_name}.yml"

        if not config_file.exists():
            raise FileNotFoundError(f"Pipeline config not found: {config_file}")

        with open(config_file, 'r') as f:
            config_data = yaml.safe_load(f)

        return PipelineConfig(
            name=config_data["name"],
            version=config_data["version"],
            stages=config_data["stages"],
            triggers=config_data.get("triggers", []),
            timeout=config_data.get("timeout", 3600),
            parallel_stages=config_data.get("parallel_stages", []),
            environment_variables=config_data.get("environment_variables", {})
        )

    def save_pipeline_config(self, config: PipelineConfig) -> None:
        """ä¿å­˜æµæ°´çº¿é…ç½®"""
        config_file = self.config_dir / f"{config.name}.yml"

        config_data = {
            "name": config.name,
            "version": config.version,
            "stages": config.stages,
            "triggers": config.triggers,
            "timeout": config.timeout,
            "parallel_stages": config.parallel_stages,
            "environment_variables": config.environment_variables
        }

        with open(config_file, 'w') as f:
            yaml.dump(config_data, f, default_flow_style=False)

    def list_pipelines(self) -> List[str]:
        """åˆ—å‡ºæ‰€æœ‰æµæ°´çº¿"""
        return [f.stem for f in self.config_dir.glob("*.yml")]

# ç¤ºä¾‹é…ç½®æ–‡ä»¶
SAMPLE_PIPELINE_CONFIG = """
name: python-app-pipeline
version: "1.0"
stages:
  - checkout
  - build
  - test
  - quality
  - deploy
triggers:
  - push
  - pull_request
timeout: 3600
parallel_stages:
  - test
  - quality
environment_variables:
  PYTHON_VERSION: "3.11"
  NODE_VERSION: "18"
"""

# åˆ›å»ºç¤ºä¾‹é…ç½®
def create_sample_pipeline():
    """åˆ›å»ºç¤ºä¾‹æµæ°´çº¿é…ç½®"""
    config_manager = PipelineConfigManager()

    config = PipelineConfig(
        name="python-app-pipeline",
        version="1.0",
        stages=["checkout", "build", "test", "quality", "deploy"],
        triggers=["push", "pull_request"],
        timeout=3600,
        parallel_stages=["test", "quality"],
        environment_variables={
            "PYTHON_VERSION": "3.11",
            "NODE_VERSION": "18"
        }
    )

    config_manager.save_pipeline_config(config)
    print("Sample pipeline config created")
```

### æµæ°´çº¿ç›‘æ§å’ŒæŠ¥å‘Š

```python
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from datetime import datetime, timedelta
import json

# æµæ°´çº¿æ‰§è¡Œè®°å½•
@dataclass
class PipelineExecution:
    pipeline_id: str
    pipeline_name: str
    status: PipelineStatus
    start_time: datetime
    end_time: Optional[datetime] = None
    duration: Optional[float] = None
    stages: List[Dict[str, Any]] = None
    variables: Dict[str, Any] = None

    def __post_init__(self):
        if self.stages is None:
            self.stages = []
        if self.variables is None:
            self.variables = {}

# æµæ°´çº¿ç›‘æ§å™¨
class PipelineMonitor:
    """æµæ°´çº¿ç›‘æ§å™¨"""

    def __init__(self):
        self.executions: List[PipelineExecution] = []
        self.logger = logging.getLogger("pipeline_monitor")

    def record_execution(self, execution: PipelineExecution) -> None:
        """è®°å½•æµæ°´çº¿æ‰§è¡Œ"""
        self.executions.append(execution)
        self.logger.info(f"Pipeline execution recorded: {execution.pipeline_id}")

    def get_execution(self, pipeline_id: str) -> Optional[PipelineExecution]:
        """è·å–æ‰§è¡Œè®°å½•"""
        for execution in self.executions:
            if execution.pipeline_id == pipeline_id:
                return execution
        return None

    def get_recent_executions(self, hours: int = 24) -> List[PipelineExecution]:
        """è·å–æœ€è¿‘çš„æ‰§è¡Œè®°å½•"""
        cutoff_time = datetime.now() - timedelta(hours=hours)
        return [
            execution for execution in self.executions
            if execution.start_time >= cutoff_time
        ]

    def get_success_rate(self, pipeline_name: Optional[str] = None, hours: int = 24) -> float:
        """è·å–æˆåŠŸç‡"""
        recent_executions = self.get_recent_executions(hours)

        if pipeline_name:
            recent_executions = [
                e for e in recent_executions
                if e.pipeline_name == pipeline_name
            ]

        if not recent_executions:
            return 0.0

        successful = sum(1 for e in recent_executions if e.status == PipelineStatus.SUCCESS)
        return successful / len(recent_executions)

    def get_average_duration(self, pipeline_name: Optional[str] = None, hours: int = 24) -> float:
        """è·å–å¹³å‡æ‰§è¡Œæ—¶é•¿"""
        recent_executions = self.get_recent_executions(hours)

        if pipeline_name:
            recent_executions = [
                e for e in recent_executions
                if e.pipeline_name == pipeline_name
            ]

        completed_executions = [
            e for e in recent_executions
            if e.duration is not None
        ]

        if not completed_executions:
            return 0.0

        return sum(e.duration for e in completed_executions) / len(completed_executions)

# æµæ°´çº¿æŠ¥å‘Šç”Ÿæˆå™¨
class PipelineReporter:
    """æµæ°´çº¿æŠ¥å‘Šç”Ÿæˆå™¨"""

    def __init__(self, monitor: PipelineMonitor):
        self.monitor = monitor

    def generate_execution_report(self, pipeline_id: str) -> Dict[str, Any]:
        """ç”Ÿæˆæ‰§è¡ŒæŠ¥å‘Š"""
        execution = self.monitor.get_execution(pipeline_id)
        if not execution:
            return {"error": "Execution not found"}

        return {
            "pipeline_id": execution.pipeline_id,
            "pipeline_name": execution.pipeline_name,
            "status": execution.status.value,
            "start_time": execution.start_time.isoformat(),
            "end_time": execution.end_time.isoformat() if execution.end_time else None,
            "duration": execution.duration,
            "stages": execution.stages,
            "variables": execution.variables
        }

    def generate_summary_report(self, hours: int = 24) -> Dict[str, Any]:
        """ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š"""
        recent_executions = self.monitor.get_recent_executions(hours)

        if not recent_executions:
            return {"message": "No executions found"}

        # æŒ‰çŠ¶æ€ç»Ÿè®¡
        status_counts = {}
        for execution in recent_executions:
            status = execution.status.value
            status_counts[status] = status_counts.get(status, 0) + 1

        # æŒ‰æµæ°´çº¿ç»Ÿè®¡
        pipeline_stats = {}
        for execution in recent_executions:
            name = execution.pipeline_name
            if name not in pipeline_stats:
                pipeline_stats[name] = {
                    "total": 0,
                    "successful": 0,
                    "failed": 0,
                    "total_duration": 0
                }

            pipeline_stats[name]["total"] += 1
            if execution.status == PipelineStatus.SUCCESS:
                pipeline_stats[name]["successful"] += 1
            elif execution.status == PipelineStatus.FAILED:
                pipeline_stats[name]["failed"] += 1

            if execution.duration:
                pipeline_stats[name]["total_duration"] += execution.duration

        # è®¡ç®—å¹³å‡æ—¶é•¿
        for name, stats in pipeline_stats.items():
            if stats["total"] > 0:
                stats["avg_duration"] = stats["total_duration"] / stats["total"]
                stats["success_rate"] = stats["successful"] / stats["total"]

        return {
            "period_hours": hours,
            "total_executions": len(recent_executions),
            "status_distribution": status_counts,
            "pipeline_statistics": pipeline_stats,
            "overall_success_rate": self.monitor.get_success_rate(hours=hours),
            "overall_avg_duration": self.monitor.get_average_duration(hours=hours)
        }
```

## ğŸ“Š æ€§èƒ½æŒ‡æ ‡

### æµæ°´çº¿æ•ˆç‡

**æ„å»ºæ—¶é—´**ï¼š$BuildTime = \sum_{i=1}^{n} t_i$ï¼Œå…¶ä¸­ $t_i$ æ˜¯ç¬¬ $i$ ä¸ªé˜¶æ®µçš„æ‰§è¡Œæ—¶é—´ã€‚

**æˆåŠŸç‡**ï¼š$SuccessRate = \frac{Successful\_Builds}{Total\_Builds}$

**å¹³å‡æ¢å¤æ—¶é—´**ï¼š$MTTR = \frac{\sum_{i=1}^{n} RecoveryTime_i}{n}$

### éƒ¨ç½²é¢‘ç‡

**éƒ¨ç½²é¢‘ç‡**ï¼š$DeploymentFrequency = \frac{Deployments}{TimePeriod}$

**å˜æ›´å‰ç½®æ—¶é—´**ï¼š$LeadTime = CommitTime - DeploymentTime$

## ğŸ›¡ï¸ å®‰å…¨è€ƒè™‘

### å¯†é’¥ç®¡ç†

```python
import os
from cryptography.fernet import Fernet
import base64

class SecretManager:
    """å¯†é’¥ç®¡ç†å™¨"""

    def __init__(self, key_file: str = ".ci_secrets"):
        self.key_file = key_file
        self.cipher_suite = self._load_or_create_key()

    def _load_or_create_key(self) -> Fernet:
        """åŠ è½½æˆ–åˆ›å»ºåŠ å¯†å¯†é’¥"""
        if os.path.exists(self.key_file):
            with open(self.key_file, 'rb') as f:
                key = f.read()
        else:
            key = Fernet.generate_key()
            with open(self.key_file, 'wb') as f:
                f.write(key)
        return Fernet(key)

    def encrypt_secret(self, secret: str) -> str:
        """åŠ å¯†å¯†é’¥"""
        return self.cipher_suite.encrypt(secret.encode()).decode()

    def decrypt_secret(self, encrypted_secret: str) -> str:
        """è§£å¯†å¯†é’¥"""
        return self.cipher_suite.decrypt(encrypted_secret.encode()).decode()

    def get_environment_secret(self, secret_name: str) -> str:
        """è·å–ç¯å¢ƒå˜é‡ä¸­çš„å¯†é’¥"""
        encrypted_value = os.environ.get(secret_name)
        if encrypted_value:
            return self.decrypt_secret(encrypted_value)
        return os.environ.get(secret_name, "")
```

### è®¿é—®æ§åˆ¶

```python
class AccessControl:
    """è®¿é—®æ§åˆ¶"""

    def __init__(self):
        self.permissions = {
            "admin": ["read", "write", "execute", "delete"],
            "developer": ["read", "write", "execute"],
            "viewer": ["read"]
        }

    def check_permission(self, user_role: str, action: str) -> bool:
        """æ£€æŸ¥æƒé™"""
        if user_role not in self.permissions:
            return False
        return action in self.permissions[user_role]

    def can_deploy(self, user_role: str, environment: str) -> bool:
        """æ£€æŸ¥éƒ¨ç½²æƒé™"""
        if environment == "production":
            return user_role == "admin"
        return self.check_permission(user_role, "execute")
```

## ğŸ“‹ æœ€ä½³å®è·µ

### 1. æµæ°´çº¿è®¾è®¡åŸåˆ™

- **å¿«é€Ÿåé¦ˆ**ï¼šç¡®ä¿æ„å»ºå’Œæµ‹è¯•å¿«é€Ÿå®Œæˆ
- **å¤±è´¥å¿«é€Ÿ**ï¼šåœ¨æ—©æœŸé˜¶æ®µå‘ç°å¹¶æŠ¥å‘Šé—®é¢˜
- **å¯é‡å¤æ€§**ï¼šç¡®ä¿æµæ°´çº¿åœ¨ä¸åŒç¯å¢ƒä¸­å¯é‡å¤æ‰§è¡Œ
- **å¯è§‚æµ‹æ€§**ï¼šæä¾›è¯¦ç»†çš„æ—¥å¿—å’Œç›‘æ§ä¿¡æ¯

### 2. å®‰å…¨æœ€ä½³å®è·µ

- **æœ€å°æƒé™åŸåˆ™**ï¼šåªæˆäºˆå¿…è¦çš„æƒé™
- **å¯†é’¥è½®æ¢**ï¼šå®šæœŸæ›´æ–°å¯†é’¥å’Œè¯ä¹¦
- **å®¡è®¡æ—¥å¿—**ï¼šè®°å½•æ‰€æœ‰æ“ä½œå’Œè®¿é—®
- **ç¯å¢ƒéš”ç¦»**ï¼šä¸¥æ ¼åˆ†ç¦»ä¸åŒç¯å¢ƒçš„èµ„æº

### 3. æ€§èƒ½ä¼˜åŒ–

```python
class PipelineOptimizer:
    """æµæ°´çº¿ä¼˜åŒ–å™¨"""

    def __init__(self):
        self.cache_dir = ".pipeline_cache"
        os.makedirs(self.cache_dir, exist_ok=True)

    def should_skip_stage(self, stage_name: str, context: PipelineContext) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥è·³è¿‡é˜¶æ®µ"""
        # æ£€æŸ¥ç¼“å­˜
        cache_key = f"{stage_name}_{context.commit_hash}"
        cache_file = os.path.join(self.cache_dir, cache_key)

        if os.path.exists(cache_file):
            # æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ
            cache_time = os.path.getmtime(cache_file)
            if datetime.now().timestamp() - cache_time < 3600:  # 1å°æ—¶ç¼“å­˜
                return True
        return False

    def cache_stage_result(self, stage_name: str, context: PipelineContext, result: Any) -> None:
        """ç¼“å­˜é˜¶æ®µç»“æœ"""
        cache_key = f"{stage_name}_{context.commit_hash}"
        cache_file = os.path.join(self.cache_dir, cache_key)

        with open(cache_file, 'w') as f:
            json.dump(result, f)
```

## ğŸ”— ç›¸å…³é“¾æ¥

- [05-æ¶æ„é¢†åŸŸ/05-02-å¾®æœåŠ¡æ¶æ„/05-02-01-å¾®æœåŠ¡åŸºç¡€.md](../05-02-å¾®æœåŠ¡æ¶æ„/05-02-01-å¾®æœåŠ¡åŸºç¡€.md) - å¾®æœåŠ¡æ¶æ„åŸºç¡€
- [02-ç†è®ºåŸºç¡€/02-01-ç®—æ³•ç†è®º/02-01-01-ç®—æ³•åŸºç¡€.md](../../02-ç†è®ºåŸºç¡€/02-01-ç®—æ³•ç†è®º/02-01-01-ç®—æ³•åŸºç¡€.md) - ç®—æ³•ç†è®ºåŸºç¡€
- [03-å…·ä½“ç§‘å­¦/03-01-è®¾è®¡æ¨¡å¼ç§‘å­¦.md](../../03-å…·ä½“ç§‘å­¦/03-01-è®¾è®¡æ¨¡å¼ç§‘å­¦.md) - è®¾è®¡æ¨¡å¼ç§‘å­¦

---

*æœ¬æ–‡æ¡£æä¾›äº†CI/CDçš„å®Œæ•´ç†è®ºåŸºç¡€å’ŒPythonå®ç°ï¼ŒåŒ…æ‹¬æµæ°´çº¿æ¡†æ¶ã€é…ç½®ç®¡ç†ã€ç›‘æ§æŠ¥å‘Šç­‰æ ¸å¿ƒç»„ä»¶ã€‚*
