# 04-05-01 大数据基础

## 📋 概述

大数据分析是处理和分析海量、高速、多样化的数据以发现有价值信息的技术。本文档详细介绍大数据的基础理论、处理框架、分析算法和Python实现，包括分布式计算、数据流处理、机器学习等核心概念。

## 🎯 学习目标

1. **理解大数据原理**：掌握大数据的特征、处理模式和架构设计
2. **掌握分布式计算**：理解MapReduce、Spark等分布式计算框架
3. **实现数据处理**：用Python实现大数据处理和分析算法
4. **应用实践**：掌握大数据在实际项目中的应用

## 📚 目录

- [04-05-01 大数据基础](#04-05-01-大数据基础)
  - [📋 概述](#-概述)
  - [🎯 学习目标](#-学习目标)
  - [📚 目录](#-目录)
  - [1. 大数据基础概念](#1-大数据基础概念)
    - [1.1 大数据定义](#11-大数据定义)
      - [1.1.1 数学定义](#111-数学定义)
    - [1.2 大数据特征](#12-大数据特征)
      - [1.2.1 数学定义](#121-数学定义)
    - [1.3 大数据处理模式](#13-大数据处理模式)
      - [1.3.1 数学定义](#131-数学定义)
  - [2. 分布式计算理论](#2-分布式计算理论)
    - [2.1 MapReduce模型](#21-mapreduce模型)
      - [2.1.1 数学定义](#211-数学定义)
    - [2.2 分布式系统理论](#22-分布式系统理论)
      - [2.2.1 数学定义](#221-数学定义)
    - [2.3 容错理论](#23-容错理论)
      - [2.3.1 数学定义](#231-数学定义)
  - [3. 数据处理框架](#3-数据处理框架)
    - [3.1 Hadoop生态系统](#31-hadoop生态系统)
      - [3.1.1 数学定义](#311-数学定义)
    - [3.2 Spark框架](#32-spark框架)
      - [3.2.1 数学定义](#321-数学定义)
    - [3.3 流处理框架](#33-流处理框架)
      - [3.3.1 数学定义](#331-数学定义)
  - [4. 数据分析算法](#4-数据分析算法)
    - [4.1 数据预处理](#41-数据预处理)
      - [4.1.1 数学定义](#411-数学定义)
    - [4.2 聚类算法](#42-聚类算法)
      - [4.2.1 数学定义](#421-数学定义)
    - [4.3 关联规则挖掘](#43-关联规则挖掘)
      - [4.3.1 数学定义](#431-数学定义)
  - [5. Python实现](#5-python实现)
    - [5.1 基础框架](#51-基础框架)
    - [5.2 MapReduce实现](#52-mapreduce实现)
    - [5.3 流处理实现](#53-流处理实现)
    - [5.4 数据分析算法实现](#54-数据分析算法实现)
  - [6. 实践应用](#6-实践应用)
    - [6.1 大数据处理演示](#61-大数据处理演示)
  - [7. 总结](#7-总结)
    - [7.1 核心要点](#71-核心要点)
    - [7.2 关键公式](#72-关键公式)
    - [7.3 应用场景](#73-应用场景)
    - [7.4 技术挑战](#74-技术挑战)
    - [7.5 发展趋势](#75-发展趋势)
    - [7.6 最佳实践](#76-最佳实践)

---

## 1. 大数据基础概念

### 1.1 大数据定义

#### 1.1.1 数学定义

**定义 1.1** (大数据)
大数据是具有4V特征的数据集合：

$$BigData = \{Volume, Velocity, Variety, Veracity\}$$

其中：

- $Volume$：数据量，通常以TB、PB、EB为单位
- $Velocity$：数据速度，实时或近实时处理
- $Variety$：数据多样性，结构化、半结构化、非结构化
- $Veracity$：数据真实性，数据质量和可信度

**定义 1.2** (数据量级)
数据量级定义：

$$
DataScale = \begin{cases}
KB & \text{if } |Data| < 2^{10} \\
MB & \text{if } 2^{10} \leq |Data| < 2^{20} \\
GB & \text{if } 2^{20} \leq |Data| < 2^{30} \\
TB & \text{if } 2^{30} \leq |Data| < 2^{40} \\
PB & \text{if } 2^{40} \leq |Data| < 2^{50} \\
EB & \text{if } 2^{50} \leq |Data| < 2^{60}
\end{cases}
$$

### 1.2 大数据特征

#### 1.2.1 数学定义

**定义 1.3** (数据量特征)
数据量特征满足：

$$Volume(t) = \int_{0}^{t} DataRate(\tau) d\tau$$

其中 $DataRate(t)$ 是时刻 $t$ 的数据产生速率。

**定义 1.4** (数据速度特征)
数据速度特征：

$$Velocity = \frac{\Delta Data}{\Delta Time}$$

**定义 1.5** (数据多样性)
数据多样性包括：

$$Variety = \{Structured, SemiStructured, Unstructured\}$$

其中：

- $Structured = \{Relational, Tabular, Matrix\}$
- $SemiStructured = \{JSON, XML, Log\}$
- $Unstructured = \{Text, Image, Video, Audio\}$

### 1.3 大数据处理模式

#### 1.3.1 数学定义

**定义 1.6** (批处理)
批处理模式：

$$BatchProcessing = \{(Data_i, Process_i) \mid i = 1, 2, \ldots, n\}$$

其中 $Data_i$ 是数据批次，$Process_i$ 是处理函数。

**定义 1.7** (流处理)
流处理模式：

$$StreamProcessing = \{(Data_t, Process_t) \mid t \in \mathbb{R}^+\}$$

其中 $Data_t$ 是时刻 $t$ 的数据流。

**定义 1.8** (交互式处理)
交互式处理模式：

$$InteractiveProcessing = \{(Query_i, Response_i) \mid i = 1, 2, \ldots\}$$

## 2. 分布式计算理论

### 2.1 MapReduce模型

#### 2.1.1 数学定义

**定义 2.1** (Map函数)
Map函数将输入数据转换为键值对：

$$Map: (k_1, v_1) \rightarrow [(k_2, v_2)]$$

**定义 2.2** (Reduce函数)
Reduce函数聚合相同键的值：

$$Reduce: (k_2, [v_2]) \rightarrow [(k_3, v_3)]$$

**定义 2.3** (MapReduce作业)
MapReduce作业定义为：

$$MapReduce = Map \circ Shuffle \circ Reduce$$

其中 $Shuffle$ 是数据重分布过程。

### 2.2 分布式系统理论

#### 2.2.1 数学定义

**定义 2.4** (分布式系统)
分布式系统是一个节点集合：

$$DistributedSystem = \{Node_1, Node_2, \ldots, Node_n\}$$

其中每个节点 $Node_i$ 具有：

- 计算能力 $CPU_i$
- 存储能力 $Storage_i$
- 网络连接 $Network_i$

**定义 2.5** (系统可用性)
系统可用性定义为：

$$Availability = \frac{MTTF}{MTTF + MTTR}$$

其中：

- $MTTF$：平均故障时间
- $MTTR$：平均修复时间

**定义 2.6** (系统吞吐量)
系统吞吐量：

$$Throughput = \frac{TotalWork}{TotalTime}$$

### 2.3 容错理论

#### 2.3.1 数学定义

**定义 2.7** (容错能力)
容错能力定义为系统在故障节点数量下的可用性：

$$FaultTolerance(f) = P(SystemAvailable \mid f \text{ nodes failed})$$

**定义 2.8** (数据复制)
数据复制策略：

$$Replication = \{Primary, Secondary_1, Secondary_2, \ldots, Secondary_n\}$$

**定理 2.1** (CAP定理)
分布式系统最多只能同时满足以下三个特性中的两个：

- Consistency（一致性）
- Availability（可用性）
- Partition tolerance（分区容错性）

## 3. 数据处理框架

### 3.1 Hadoop生态系统

#### 3.1.1 数学定义

**定义 3.1** (HDFS)
Hadoop分布式文件系统：

$$HDFS = \{NameNode, DataNodes, Blocks\}$$

其中：

- $NameNode$：元数据管理节点
- $DataNodes = \{DN_1, DN_2, \ldots, DN_n\}$：数据节点
- $Blocks = \{Block_1, Block_2, \ldots, Block_m\}$：数据块

**定义 3.2** (数据块分布)
数据块分布策略：

$$BlockDistribution(Block_i) = \{DN_j \mid j \in \{1, 2, \ldots, n\}\}$$

**定义 3.3** (副本因子)
副本因子定义为：

$$ReplicationFactor = \frac{TotalCopies}{OriginalBlocks}$$

### 3.2 Spark框架

#### 3.2.1 数学定义

**定义 3.4** (RDD)
弹性分布式数据集：

$$RDD = \{Partition_1, Partition_2, \ldots, Partition_k\}$$

其中每个分区 $Partition_i$ 包含数据子集。

**定义 3.5** (转换操作)
RDD转换操作：

$$Transform: RDD \rightarrow RDD$$

**定义 3.6** (行动操作)
RDD行动操作：

$$Action: RDD \rightarrow Result$$

### 3.3 流处理框架

#### 3.3.1 数学定义

**定义 3.7** (数据流)
数据流定义为时间序列：

$$DataStream = \{(t_i, data_i) \mid i = 1, 2, \ldots\}$$

**定义 3.8** (窗口操作)
窗口操作：

$$Window(t, w) = \{data_i \mid t - w \leq t_i \leq t\}$$

其中 $w$ 是窗口大小。

**定义 3.9** (滑动窗口)
滑动窗口：

$$SlidingWindow(t, w, s) = \{data_i \mid t - w + k \cdot s \leq t_i \leq t + k \cdot s\}$$

其中 $s$ 是滑动步长。

## 4. 数据分析算法

### 4.1 数据预处理

#### 4.1.1 数学定义

**定义 4.1** (数据清洗)
数据清洗函数：

$$Clean: RawData \rightarrow CleanData$$

**定义 4.2** (数据标准化)
数据标准化：

$$Standardize(x) = \frac{x - \mu}{\sigma}$$

其中 $\mu$ 是均值，$\sigma$ 是标准差。

**定义 4.3** (数据归一化)
数据归一化：

$$Normalize(x) = \frac{x - x_{min}}{x_{max} - x_{min}}$$

### 4.2 聚类算法

#### 4.2.1 数学定义

**定义 4.4** (K-means聚类)
K-means目标函数：

$$J = \sum_{i=1}^{k} \sum_{x \in C_i} \|x - \mu_i\|^2$$

其中 $C_i$ 是第 $i$ 个聚类，$\mu_i$ 是聚类中心。

**定义 4.5** (层次聚类)
层次聚类距离：

$$d(C_i, C_j) = \min_{x \in C_i, y \in C_j} d(x, y)$$

### 4.3 关联规则挖掘

#### 4.3.1 数学定义

**定义 4.6** (支持度)
支持度定义为：

$$Support(X) = \frac{|T_X|}{|T|}$$

其中 $T_X$ 是包含项集 $X$ 的事务集合。

**定义 4.7** (置信度)
置信度定义为：

$$Confidence(X \rightarrow Y) = \frac{Support(X \cup Y)}{Support(X)}$$

**定义 4.8** (提升度)
提升度定义为：

$$Lift(X \rightarrow Y) = \frac{Confidence(X \rightarrow Y)}{Support(Y)}$$

## 5. Python实现

### 5.1 基础框架

```python
"""
大数据基础实现
作者：AI助手
日期：2024年
版本：1.0
"""

from __future__ import annotations
from abc import ABC, abstractmethod
from typing import Any, Dict, List, Optional, Tuple, Union, Iterator
import numpy as np
import pandas as pd
from dataclasses import dataclass, field
from enum import Enum
import time
import json
import hashlib
from collections import defaultdict, Counter
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
import warnings
warnings.filterwarnings('ignore')

@dataclass
class DataRecord:
    """数据记录"""
    id: str
    timestamp: float
    data: Dict[str, Any]

    def __post_init__(self):
        if not self.id:
            self.id = hashlib.md5(f"{self.timestamp}{str(self.data)}".encode()).hexdigest()[:8]

@dataclass
class DataPartition:
    """数据分区"""
    id: str
    records: List[DataRecord]
    size: int = 0

    def __post_init__(self):
        self.size = len(self.records)

class DataNode:
    """数据节点"""

    def __init__(self, node_id: str, capacity: int = 1000):
        self.node_id = node_id
        self.capacity = capacity
        self.partitions: Dict[str, DataPartition] = {}
        self.available_space = capacity

    def add_partition(self, partition: DataPartition) -> bool:
        """添加分区"""
        if partition.size <= self.available_space:
            self.partitions[partition.id] = partition
            self.available_space -= partition.size
            return True
        return False

    def remove_partition(self, partition_id: str) -> Optional[DataPartition]:
        """移除分区"""
        if partition_id in self.partitions:
            partition = self.partitions.pop(partition_id)
            self.available_space += partition.size
            return partition
        return None

    def get_partition(self, partition_id: str) -> Optional[DataPartition]:
        """获取分区"""
        return self.partitions.get(partition_id)

class DistributedSystem:
    """分布式系统"""

    def __init__(self):
        self.nodes: Dict[str, DataNode] = {}
        self.partitions: Dict[str, DataPartition] = {}

    def add_node(self, node_id: str, capacity: int = 1000) -> DataNode:
        """添加节点"""
        node = DataNode(node_id, capacity)
        self.nodes[node_id] = node
        return node

    def create_partition(self, records: List[DataRecord]) -> DataPartition:
        """创建分区"""
        partition_id = hashlib.md5(f"{time.time()}{len(records)}".encode()).hexdigest()[:8]
        partition = DataPartition(partition_id, records)
        self.partitions[partition_id] = partition
        return partition

    def distribute_partition(self, partition: DataPartition, replication_factor: int = 3) -> bool:
        """分发分区"""
        available_nodes = [node for node in self.nodes.values() if node.available_space >= partition.size]

        if len(available_nodes) < replication_factor:
            return False

        # 选择节点进行复制
        selected_nodes = available_nodes[:replication_factor]

        for node in selected_nodes:
            node.add_partition(partition)

        return True

    def get_partition_locations(self, partition_id: str) -> List[str]:
        """获取分区位置"""
        locations = []
        for node_id, node in self.nodes.items():
            if partition_id in node.partitions:
                locations.append(node_id)
        return locations
```

### 5.2 MapReduce实现

```python
class MapReduce:
    """MapReduce框架"""

    def __init__(self, distributed_system: DistributedSystem):
        self.system = distributed_system
        self.map_results: Dict[str, List[Tuple[str, Any]]] = {}
        self.reduce_results: Dict[str, Any] = {}

    def map_function(self, record: DataRecord) -> List[Tuple[str, Any]]:
        """Map函数（可重写）"""
        # 默认实现：提取所有字段作为键值对
        results = []
        for key, value in record.data.items():
            results.append((key, value))
        return results

    def reduce_function(self, key: str, values: List[Any]) -> Any:
        """Reduce函数（可重写）"""
        # 默认实现：计算平均值
        if isinstance(values[0], (int, float)):
            return sum(values) / len(values)
        else:
            return values

    def execute(self, partition_id: str) -> Dict[str, Any]:
        """执行MapReduce作业"""
        # 获取分区
        partition = self.system.partitions.get(partition_id)
        if not partition:
            raise ValueError(f"Partition {partition_id} not found")

        # Map阶段
        map_results = []
        for record in partition.records:
            map_output = self.map_function(record)
            map_results.extend(map_output)

        # Shuffle阶段
        shuffled_data = defaultdict(list)
        for key, value in map_results:
            shuffled_data[key].append(value)

        # Reduce阶段
        reduce_results = {}
        for key, values in shuffled_data.items():
            reduce_results[key] = self.reduce_function(key, values)

        return reduce_results

class WordCountMapReduce(MapReduce):
    """词频统计MapReduce"""

    def map_function(self, record: DataRecord) -> List[Tuple[str, int]]:
        """Map函数：提取单词"""
        text = str(record.data.get('text', ''))
        words = text.lower().split()
        return [(word, 1) for word in words]

    def reduce_function(self, key: str, values: List[int]) -> int:
        """Reduce函数：统计词频"""
        return sum(values)

class AverageMapReduce(MapReduce):
    """平均值计算MapReduce"""

    def map_function(self, record: DataRecord) -> List[Tuple[str, float]]:
        """Map函数：提取数值"""
        value = record.data.get('value', 0)
        category = record.data.get('category', 'default')
        return [(category, float(value))]

    def reduce_function(self, key: str, values: List[float]) -> float:
        """Reduce函数：计算平均值"""
        return sum(values) / len(values)
```

### 5.3 流处理实现

```python
class DataStream:
    """数据流"""

    def __init__(self):
        self.records: List[DataRecord] = []
        self.max_size = 10000

    def add_record(self, record: DataRecord) -> None:
        """添加记录"""
        self.records.append(record)

        # 保持流大小
        if len(self.records) > self.max_size:
            self.records.pop(0)

    def get_window(self, window_size: int) -> List[DataRecord]:
        """获取窗口数据"""
        return self.records[-window_size:] if len(self.records) >= window_size else self.records

    def get_sliding_window(self, window_size: int, step: int = 1) -> Iterator[List[DataRecord]]:
        """获取滑动窗口"""
        for i in range(0, len(self.records) - window_size + 1, step):
            yield self.records[i:i + window_size]

class StreamProcessor:
    """流处理器"""

    def __init__(self, stream: DataStream):
        self.stream = stream
        self.processors: List[StreamProcessor] = []

    def add_processor(self, processor: 'StreamProcessor') -> None:
        """添加处理器"""
        self.processors.append(processor)

    def process(self, window_size: int = 100) -> Any:
        """处理数据流"""
        window = self.stream.get_window(window_size)
        return self.process_window(window)

    def process_window(self, window: List[DataRecord]) -> Any:
        """处理窗口数据（可重写）"""
        return len(window)

class MovingAverageProcessor(StreamProcessor):
    """移动平均处理器"""

    def __init__(self, stream: DataStream, field: str):
        super().__init__(stream)
        self.field = field

    def process_window(self, window: List[DataRecord]) -> float:
        """计算移动平均"""
        values = []
        for record in window:
            value = record.data.get(self.field)
            if isinstance(value, (int, float)):
                values.append(value)

        return sum(values) / len(values) if values else 0.0

class AnomalyDetector(StreamProcessor):
    """异常检测器"""

    def __init__(self, stream: DataStream, field: str, threshold: float = 2.0):
        super().__init__(stream)
        self.field = field
        self.threshold = threshold
        self.mean = 0.0
        self.std = 1.0
        self.count = 0

    def process_window(self, window: List[DataRecord]) -> List[DataRecord]:
        """检测异常"""
        values = []
        for record in window:
            value = record.data.get(self.field)
            if isinstance(value, (int, float)):
                values.append(value)

        if not values:
            return []

        # 更新统计信息
        self.update_statistics(values)

        # 检测异常
        anomalies = []
        for record in window:
            value = record.data.get(self.field)
            if isinstance(value, (int, float)):
                z_score = abs((value - self.mean) / self.std)
                if z_score > self.threshold:
                    anomalies.append(record)

        return anomalies

    def update_statistics(self, values: List[float]) -> None:
        """更新统计信息"""
        if not values:
            return

        # 在线更新均值和标准差
        for value in values:
            self.count += 1
            delta = value - self.mean
            self.mean += delta / self.count
            delta2 = value - self.mean
            self.std = np.sqrt((self.std ** 2 * (self.count - 1) + delta * delta2) / self.count)
```

### 5.4 数据分析算法实现

```python
class DataAnalyzer:
    """数据分析器"""

    def __init__(self):
        self.statistics: Dict[str, Dict[str, float]] = {}

    def analyze_partition(self, partition: DataPartition) -> Dict[str, Any]:
        """分析分区数据"""
        if not partition.records:
            return {}

        # 提取数值字段
        numeric_fields = self.extract_numeric_fields(partition.records)

        # 计算统计信息
        stats = {}
        for field in numeric_fields:
            values = [record.data[field] for record in partition.records if field in record.data]
            if values:
                stats[field] = {
                    'count': len(values),
                    'mean': np.mean(values),
                    'std': np.std(values),
                    'min': np.min(values),
                    'max': np.max(values),
                    'median': np.median(values)
                }

        return stats

    def extract_numeric_fields(self, records: List[DataRecord]) -> List[str]:
        """提取数值字段"""
        numeric_fields = set()
        for record in records:
            for key, value in record.data.items():
                if isinstance(value, (int, float)):
                    numeric_fields.add(key)
        return list(numeric_fields)

class ClusteringAnalyzer:
    """聚类分析器"""

    def __init__(self, k: int = 3):
        self.k = k
        self.centroids: Optional[np.ndarray] = None
        self.labels: Optional[np.ndarray] = None

    def kmeans_clustering(self, data: np.ndarray, max_iterations: int = 100) -> Tuple[np.ndarray, np.ndarray]:
        """K-means聚类"""
        n_samples, n_features = data.shape

        # 随机初始化聚类中心
        centroids = data[np.random.choice(n_samples, self.k, replace=False)]

        for iteration in range(max_iterations):
            # 分配样本到最近的聚类中心
            distances = np.sqrt(((data[:, np.newaxis, :] - centroids[np.newaxis, :, :]) ** 2).sum(axis=2))
            labels = np.argmin(distances, axis=1)

            # 更新聚类中心
            new_centroids = np.zeros_like(centroids)
            for k in range(self.k):
                if np.sum(labels == k) > 0:
                    new_centroids[k] = np.mean(data[labels == k], axis=0)
                else:
                    new_centroids[k] = centroids[k]

            # 检查收敛
            if np.allclose(centroids, new_centroids):
                break

            centroids = new_centroids

        self.centroids = centroids
        self.labels = labels

        return centroids, labels

    def analyze_clusters(self, data: np.ndarray, labels: np.ndarray) -> Dict[str, Any]:
        """分析聚类结果"""
        analysis = {
            'n_clusters': self.k,
            'cluster_sizes': [],
            'cluster_centers': self.centroids.tolist() if self.centroids is not None else [],
            'inertia': 0.0
        }

        if self.centroids is not None:
            # 计算聚类大小
            for k in range(self.k):
                cluster_size = np.sum(labels == k)
                analysis['cluster_sizes'].append(int(cluster_size))

            # 计算惯性（簇内平方和）
            inertia = 0
            for k in range(self.k):
                cluster_points = data[labels == k]
                if len(cluster_points) > 0:
                    inertia += np.sum((cluster_points - self.centroids[k]) ** 2)
            analysis['inertia'] = float(inertia)

        return analysis

class AssociationRuleMiner:
    """关联规则挖掘器"""

    def __init__(self, min_support: float = 0.1, min_confidence: float = 0.5):
        self.min_support = min_support
        self.min_confidence = min_confidence
        self.frequent_itemsets: Dict[frozenset, int] = {}
        self.association_rules: List[Dict[str, Any]] = []

    def find_frequent_itemsets(self, transactions: List[List[str]]) -> Dict[frozenset, int]:
        """查找频繁项集"""
        # 计算项的支持度
        item_counts = Counter()
        for transaction in transactions:
            for item in transaction:
                item_counts[item] += 1

        # 过滤满足最小支持度的项
        n_transactions = len(transactions)
        frequent_items = {item for item, count in item_counts.items()
                         if count / n_transactions >= self.min_support}

        # 生成频繁项集
        self.frequent_itemsets = {}
        for k in range(1, len(frequent_items) + 1):
            for itemset in self.generate_k_itemsets(frequent_items, k):
                support = self.calculate_support(itemset, transactions)
                if support >= self.min_support:
                    self.frequent_itemsets[itemset] = support

        return self.frequent_itemsets

    def generate_k_itemsets(self, items: set, k: int) -> Iterator[frozenset]:
        """生成k项集"""
        if k == 1:
            for item in items:
                yield frozenset([item])
        else:
            items_list = list(items)
            for i in range(len(items_list)):
                for j in range(i + 1, len(items_list)):
                    yield frozenset([items_list[i], items_list[j]])

    def calculate_support(self, itemset: frozenset, transactions: List[List[str]]) -> float:
        """计算支持度"""
        count = 0
        for transaction in transactions:
            if itemset.issubset(transaction):
                count += 1
        return count / len(transactions)

    def generate_rules(self, transactions: List[List[str]]) -> List[Dict[str, Any]]:
        """生成关联规则"""
        if not self.frequent_itemsets:
            self.find_frequent_itemsets(transactions)

        self.association_rules = []

        for itemset, support in self.frequent_itemsets.items():
            if len(itemset) < 2:
                continue

            # 生成所有可能的规则
            items_list = list(itemset)
            for i in range(1, len(items_list)):
                for antecedent in self.generate_subsets(items_list, i):
                    consequent = itemset - antecedent
                    confidence = support / self.frequent_itemsets.get(antecedent, 0)

                    if confidence >= self.min_confidence:
                        rule = {
                            'antecedent': list(antecedent),
                            'consequent': list(consequent),
                            'support': support,
                            'confidence': confidence,
                            'lift': confidence / self.frequent_itemsets.get(consequent, 0)
                        }
                        self.association_rules.append(rule)

        return self.association_rules

    def generate_subsets(self, items: List[str], k: int) -> Iterator[frozenset]:
        """生成子集"""
        if k == 0:
            yield frozenset()
        elif k == 1:
            for item in items:
                yield frozenset([item])
        else:
            for i in range(len(items) - k + 1):
                for subset in self.generate_subsets(items[i+1:], k-1):
                    yield frozenset([items[i]]) | subset
```

## 6. 实践应用

### 6.1 大数据处理演示

```python
def big_data_processing_demo():
    """大数据处理演示"""
    print("=== 大数据处理演示 ===\n")

    # 创建分布式系统
    system = DistributedSystem()

    # 添加数据节点
    for i in range(5):
        system.add_node(f"node-{i}", capacity=1000)

    print("1. 分布式系统设置")
    print(f"节点数量: {len(system.nodes)}")
    for node_id, node in system.nodes.items():
        print(f"  节点 {node_id}: 容量 {node.capacity}, 可用空间 {node.available_space}")

    # 生成模拟数据
    print("\n2. 数据生成")
    records = []
    for i in range(1000):
        record = DataRecord(
            id=f"record-{i}",
            timestamp=time.time() + i,
            data={
                'user_id': f"user-{i % 100}",
                'product_id': f"product-{i % 50}",
                'price': np.random.uniform(10, 1000),
                'quantity': np.random.randint(1, 10),
                'category': f"cat-{i % 10}",
                'rating': np.random.uniform(1, 5)
            }
        )
        records.append(record)

    print(f"生成记录数: {len(records)}")

    # 创建分区
    print("\n3. 数据分区")
    partition_size = 200
    partitions = []

    for i in range(0, len(records), partition_size):
        partition_records = records[i:i + partition_size]
        partition = system.create_partition(partition_records)
        partitions.append(partition)
        print(f"创建分区 {partition.id}: {len(partition_records)} 条记录")

    # 分发分区
    print("\n4. 分区分发")
    for partition in partitions:
        success = system.distribute_partition(partition, replication_factor=3)
        if success:
            locations = system.get_partition_locations(partition.id)
            print(f"分区 {partition.id} 分发到节点: {locations}")
        else:
            print(f"分区 {partition.id} 分发失败")

    # MapReduce处理
    print("\n5. MapReduce处理")

    # 词频统计
    word_count_mr = WordCountMapReduce(system)
    text_records = [
        DataRecord("1", time.time(), {"text": "hello world hello python"}),
        DataRecord("2", time.time(), {"text": "python programming is fun"}),
        DataRecord("3", time.time(), {"text": "hello python world"})
    ]

    text_partition = system.create_partition(text_records)
    word_count_results = word_count_mr.execute(text_partition.id)
    print("词频统计结果:")
    for word, count in word_count_results.items():
        print(f"  {word}: {count}")

    # 平均值计算
    avg_mr = AverageMapReduce(system)
    avg_results = avg_mr.execute(partitions[0].id)
    print("\n平均值计算结果:")
    for category, avg_value in avg_results.items():
        print(f"  {category}: {avg_value:.2f}")

    return system, partitions

def stream_processing_demo():
    """流处理演示"""
    print("\n=== 流处理演示 ===\n")

    # 创建数据流
    stream = DataStream()

    # 生成流数据
    print("1. 生成流数据")
    for i in range(100):
        record = DataRecord(
            id=f"stream-{i}",
            timestamp=time.time() + i,
            data={
                'sensor_id': f"sensor-{i % 5}",
                'temperature': np.random.normal(25, 5),
                'humidity': np.random.normal(60, 10),
                'pressure': np.random.normal(1013, 20)
            }
        )
        stream.add_record(record)

    print(f"流数据记录数: {len(stream.records)}")

    # 移动平均处理
    print("\n2. 移动平均处理")
    ma_processor = MovingAverageProcessor(stream, 'temperature')

    window_sizes = [10, 20, 50]
    for window_size in window_sizes:
        avg_temp = ma_processor.process(window_size)
        print(f"窗口大小 {window_size}: 平均温度 {avg_temp:.2f}°C")

    # 异常检测
    print("\n3. 异常检测")
    anomaly_detector = AnomalyDetector(stream, 'temperature', threshold=2.0)

    window = stream.get_window(50)
    anomalies = anomaly_detector.process_window(window)

    print(f"检测到 {len(anomalies)} 个异常:")
    for anomaly in anomalies[:5]:  # 只显示前5个
        temp = anomaly.data['temperature']
        print(f"  时间 {anomaly.timestamp}: 温度 {temp:.2f}°C")

    return stream, ma_processor, anomaly_detector

def data_analysis_demo():
    """数据分析演示"""
    print("\n=== 数据分析演示 ===\n")

    # 创建分析器
    analyzer = DataAnalyzer()
    clustering_analyzer = ClusteringAnalyzer(k=3)

    # 生成分析数据
    print("1. 生成分析数据")
    records = []
    for i in range(300):
        # 生成3个聚类的数据
        if i < 100:
            cluster = 0
            x = np.random.normal(0, 1)
            y = np.random.normal(0, 1)
        elif i < 200:
            cluster = 1
            x = np.random.normal(5, 1)
            y = np.random.normal(5, 1)
        else:
            cluster = 2
            x = np.random.normal(10, 1)
            y = np.random.normal(0, 1)

        record = DataRecord(
            id=f"analysis-{i}",
            timestamp=time.time() + i,
            data={
                'x': x,
                'y': y,
                'cluster': cluster,
                'value': np.random.uniform(0, 100)
            }
        )
        records.append(record)

    partition = DataPartition("analysis-partition", records)

    # 统计分析
    print("\n2. 统计分析")
    stats = analyzer.analyze_partition(partition)

    for field, field_stats in stats.items():
        print(f"\n{field} 统计:")
        for stat_name, stat_value in field_stats.items():
            print(f"  {stat_name}: {stat_value:.4f}")

    # 聚类分析
    print("\n3. 聚类分析")
    data = np.array([[record.data['x'], record.data['y']] for record in records])
    centroids, labels = clustering_analyzer.kmeans_clustering(data)

    cluster_analysis = clustering_analyzer.analyze_clusters(data, labels)
    print(f"聚类数量: {cluster_analysis['n_clusters']}")
    print(f"聚类大小: {cluster_analysis['cluster_sizes']}")
    print(f"惯性: {cluster_analysis['inertia']:.4f}")

    # 关联规则挖掘
    print("\n4. 关联规则挖掘")
    transactions = [
        ['milk', 'bread', 'butter'],
        ['milk', 'bread', 'eggs'],
        ['milk', 'bread', 'cheese'],
        ['bread', 'butter', 'eggs'],
        ['milk', 'cheese', 'yogurt'],
        ['bread', 'cheese', 'ham'],
        ['milk', 'bread', 'ham'],
        ['bread', 'butter', 'cheese'],
        ['milk', 'yogurt', 'fruits'],
        ['bread', 'milk', 'fruits']
    ]

    rule_miner = AssociationRuleMiner(min_support=0.3, min_confidence=0.6)
    rules = rule_miner.generate_rules(transactions)

    print(f"发现 {len(rules)} 条关联规则:")
    for i, rule in enumerate(rules[:5]):  # 只显示前5条
        print(f"  规则 {i+1}: {rule['antecedent']} -> {rule['consequent']}")
        print(f"    支持度: {rule['support']:.3f}, 置信度: {rule['confidence']:.3f}, 提升度: {rule['lift']:.3f}")

    return analyzer, clustering_analyzer, rule_miner

def performance_analysis():
    """性能分析"""
    print("\n=== 性能分析 ===\n")

    # 测试不同数据规模的处理时间
    data_sizes = [1000, 5000, 10000, 50000]
    processing_times = []

    for size in data_sizes:
        print(f"测试数据规模: {size}")

        # 生成数据
        start_time = time.time()
        records = []
        for i in range(size):
            record = DataRecord(
                id=f"perf-{i}",
                timestamp=time.time() + i,
                data={
                    'value': np.random.uniform(0, 100),
                    'category': f"cat-{i % 10}",
                    'score': np.random.uniform(0, 1)
                }
            )
            records.append(record)

        # 创建分区
        partition = DataPartition(f"perf-{size}", records)

        # 统计分析
        analyzer = DataAnalyzer()
        stats = analyzer.analyze_partition(partition)

        end_time = time.time()
        processing_time = end_time - start_time
        processing_times.append(processing_time)

        print(f"  处理时间: {processing_time:.4f}秒")
        print(f"  记录数: {len(records)}")
        print(f"  统计字段数: {len(stats)}")

    # 可视化性能结果
    import matplotlib.pyplot as plt

    plt.figure(figsize=(10, 6))
    plt.plot(data_sizes, processing_times, 'bo-')
    plt.xlabel('数据规模')
    plt.ylabel('处理时间 (秒)')
    plt.title('大数据处理性能分析')
    plt.grid(True)
    plt.show()

    print(f"\n性能总结:")
    for i, size in enumerate(data_sizes):
        throughput = size / processing_times[i]
        print(f"  数据规模 {size}: 吞吐量 {throughput:.0f} 记录/秒")

if __name__ == "__main__":
    # 运行大数据处理演示
    system, partitions = big_data_processing_demo()

    # 运行流处理演示
    stream, ma_processor, anomaly_detector = stream_processing_demo()

    # 运行数据分析演示
    analyzer, clustering_analyzer, rule_miner = data_analysis_demo()

    # 运行性能分析
    performance_analysis()
```

## 7. 总结

### 7.1 核心要点

1. **大数据特征**：Volume（数据量）、Velocity（速度）、Variety（多样性）、Veracity（真实性）
2. **分布式计算**：MapReduce、容错机制、CAP定理
3. **处理框架**：Hadoop、Spark、流处理
4. **分析算法**：聚类、关联规则、统计分析
5. **应用场景**：商业智能、推荐系统、风险控制

### 7.2 关键公式

- **数据量级**：$DataScale = \begin{cases} KB & \text{if } |Data| < 2^{10} \\ MB & \text{if } 2^{10} \leq |Data| < 2^{20} \\ \ldots \end{cases}$
- **MapReduce**：$MapReduce = Map \circ Shuffle \circ Reduce$
- **系统可用性**：$Availability = \frac{MTTF}{MTTF + MTTR}$
- **K-means目标**：$J = \sum_{i=1}^{k} \sum_{x \in C_i} \|x - \mu_i\|^2$
- **支持度**：$Support(X) = \frac{|T_X|}{|T|}$

### 7.3 应用场景

1. **商业智能**：销售分析、客户行为分析、市场趋势
2. **推荐系统**：商品推荐、内容推荐、个性化服务
3. **风险控制**：欺诈检测、信用评估、异常监控
4. **物联网**：传感器数据分析、设备监控、预测维护
5. **金融科技**：交易分析、风险评估、投资决策

### 7.4 技术挑战

1. **数据质量**：数据清洗、一致性、完整性
2. **性能优化**：并行处理、内存管理、I/O优化
3. **可扩展性**：水平扩展、负载均衡、资源调度
4. **实时性**：流处理、低延迟、实时分析
5. **安全性**：数据隐私、访问控制、加密传输

### 7.5 发展趋势

1. **实时处理**：流计算、事件驱动、实时分析
2. **AI集成**：机器学习、深度学习、智能分析
3. **云原生**：容器化、微服务、弹性伸缩
4. **边缘计算**：本地处理、边缘分析、分布式智能
5. **数据湖**：统一存储、多格式支持、元数据管理

### 7.6 最佳实践

1. **架构设计**：分层架构、模块化设计、松耦合
2. **数据处理**：增量处理、分区策略、缓存优化
3. **性能调优**：并行度调优、内存配置、网络优化
4. **监控告警**：性能监控、异常检测、容量规划
5. **数据治理**：数据质量、元数据管理、生命周期管理

---

**相关文档**：

- [04-05-02-分布式计算](./04-05-02-分布式计算.md)
- [04-05-03-实时流处理](./04-05-03-实时流处理.md)
- [06-组件算法/06-01-基础算法/06-01-01-排序算法](../06-组件算法/06-01-基础算法/06-01-01-排序算法.md)

**返回上级**：[04-行业领域](../README.md)
