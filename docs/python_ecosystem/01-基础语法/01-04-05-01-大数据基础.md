# 04-05-01 å¤§æ•°æ®åŸºç¡€

## ğŸ“‹ æ¦‚è¿°

å¤§æ•°æ®åˆ†ææ˜¯å¤„ç†å’Œåˆ†ææµ·é‡ã€é«˜é€Ÿã€å¤šæ ·åŒ–çš„æ•°æ®ä»¥å‘ç°æœ‰ä»·å€¼ä¿¡æ¯çš„æŠ€æœ¯ã€‚æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç»å¤§æ•°æ®çš„åŸºç¡€ç†è®ºã€å¤„ç†æ¡†æ¶ã€åˆ†æç®—æ³•å’ŒPythonå®ç°ï¼ŒåŒ…æ‹¬åˆ†å¸ƒå¼è®¡ç®—ã€æ•°æ®æµå¤„ç†ã€æœºå™¨å­¦ä¹ ç­‰æ ¸å¿ƒæ¦‚å¿µã€‚

## ğŸ¯ å­¦ä¹ ç›®æ ‡

1. **ç†è§£å¤§æ•°æ®åŸç†**ï¼šæŒæ¡å¤§æ•°æ®çš„ç‰¹å¾ã€å¤„ç†æ¨¡å¼å’Œæ¶æ„è®¾è®¡
2. **æŒæ¡åˆ†å¸ƒå¼è®¡ç®—**ï¼šç†è§£MapReduceã€Sparkç­‰åˆ†å¸ƒå¼è®¡ç®—æ¡†æ¶
3. **å®ç°æ•°æ®å¤„ç†**ï¼šç”¨Pythonå®ç°å¤§æ•°æ®å¤„ç†å’Œåˆ†æç®—æ³•
4. **åº”ç”¨å®è·µ**ï¼šæŒæ¡å¤§æ•°æ®åœ¨å®é™…é¡¹ç›®ä¸­çš„åº”ç”¨

## ğŸ“š ç›®å½•

- [04-05-01 å¤§æ•°æ®åŸºç¡€](#04-05-01-å¤§æ•°æ®åŸºç¡€)
  - [ğŸ“‹ æ¦‚è¿°](#-æ¦‚è¿°)
  - [ğŸ¯ å­¦ä¹ ç›®æ ‡](#-å­¦ä¹ ç›®æ ‡)
  - [ğŸ“š ç›®å½•](#-ç›®å½•)
  - [1. å¤§æ•°æ®åŸºç¡€æ¦‚å¿µ](#1-å¤§æ•°æ®åŸºç¡€æ¦‚å¿µ)
    - [1.1 å¤§æ•°æ®å®šä¹‰](#11-å¤§æ•°æ®å®šä¹‰)
      - [1.1.1 æ•°å­¦å®šä¹‰](#111-æ•°å­¦å®šä¹‰)
    - [1.2 å¤§æ•°æ®ç‰¹å¾](#12-å¤§æ•°æ®ç‰¹å¾)
      - [1.2.1 æ•°å­¦å®šä¹‰](#121-æ•°å­¦å®šä¹‰)
    - [1.3 å¤§æ•°æ®å¤„ç†æ¨¡å¼](#13-å¤§æ•°æ®å¤„ç†æ¨¡å¼)
      - [1.3.1 æ•°å­¦å®šä¹‰](#131-æ•°å­¦å®šä¹‰)
  - [2. åˆ†å¸ƒå¼è®¡ç®—ç†è®º](#2-åˆ†å¸ƒå¼è®¡ç®—ç†è®º)
    - [2.1 MapReduceæ¨¡å‹](#21-mapreduceæ¨¡å‹)
      - [2.1.1 æ•°å­¦å®šä¹‰](#211-æ•°å­¦å®šä¹‰)
    - [2.2 åˆ†å¸ƒå¼ç³»ç»Ÿç†è®º](#22-åˆ†å¸ƒå¼ç³»ç»Ÿç†è®º)
      - [2.2.1 æ•°å­¦å®šä¹‰](#221-æ•°å­¦å®šä¹‰)
    - [2.3 å®¹é”™ç†è®º](#23-å®¹é”™ç†è®º)
      - [2.3.1 æ•°å­¦å®šä¹‰](#231-æ•°å­¦å®šä¹‰)
  - [3. æ•°æ®å¤„ç†æ¡†æ¶](#3-æ•°æ®å¤„ç†æ¡†æ¶)
    - [3.1 Hadoopç”Ÿæ€ç³»ç»Ÿ](#31-hadoopç”Ÿæ€ç³»ç»Ÿ)
      - [3.1.1 æ•°å­¦å®šä¹‰](#311-æ•°å­¦å®šä¹‰)
    - [3.2 Sparkæ¡†æ¶](#32-sparkæ¡†æ¶)
      - [3.2.1 æ•°å­¦å®šä¹‰](#321-æ•°å­¦å®šä¹‰)
    - [3.3 æµå¤„ç†æ¡†æ¶](#33-æµå¤„ç†æ¡†æ¶)
      - [3.3.1 æ•°å­¦å®šä¹‰](#331-æ•°å­¦å®šä¹‰)
  - [4. æ•°æ®åˆ†æç®—æ³•](#4-æ•°æ®åˆ†æç®—æ³•)
    - [4.1 æ•°æ®é¢„å¤„ç†](#41-æ•°æ®é¢„å¤„ç†)
      - [4.1.1 æ•°å­¦å®šä¹‰](#411-æ•°å­¦å®šä¹‰)
    - [4.2 èšç±»ç®—æ³•](#42-èšç±»ç®—æ³•)
      - [4.2.1 æ•°å­¦å®šä¹‰](#421-æ•°å­¦å®šä¹‰)
    - [4.3 å…³è”è§„åˆ™æŒ–æ˜](#43-å…³è”è§„åˆ™æŒ–æ˜)
      - [4.3.1 æ•°å­¦å®šä¹‰](#431-æ•°å­¦å®šä¹‰)
  - [5. Pythonå®ç°](#5-pythonå®ç°)
    - [5.1 åŸºç¡€æ¡†æ¶](#51-åŸºç¡€æ¡†æ¶)
    - [5.2 MapReduceå®ç°](#52-mapreduceå®ç°)
    - [5.3 æµå¤„ç†å®ç°](#53-æµå¤„ç†å®ç°)
    - [5.4 æ•°æ®åˆ†æç®—æ³•å®ç°](#54-æ•°æ®åˆ†æç®—æ³•å®ç°)
  - [6. å®è·µåº”ç”¨](#6-å®è·µåº”ç”¨)
    - [6.1 å¤§æ•°æ®å¤„ç†æ¼”ç¤º](#61-å¤§æ•°æ®å¤„ç†æ¼”ç¤º)
  - [7. æ€»ç»“](#7-æ€»ç»“)
    - [7.1 æ ¸å¿ƒè¦ç‚¹](#71-æ ¸å¿ƒè¦ç‚¹)
    - [7.2 å…³é”®å…¬å¼](#72-å…³é”®å…¬å¼)
    - [7.3 åº”ç”¨åœºæ™¯](#73-åº”ç”¨åœºæ™¯)
    - [7.4 æŠ€æœ¯æŒ‘æˆ˜](#74-æŠ€æœ¯æŒ‘æˆ˜)
    - [7.5 å‘å±•è¶‹åŠ¿](#75-å‘å±•è¶‹åŠ¿)
    - [7.6 æœ€ä½³å®è·µ](#76-æœ€ä½³å®è·µ)

---

## 1. å¤§æ•°æ®åŸºç¡€æ¦‚å¿µ

### 1.1 å¤§æ•°æ®å®šä¹‰

#### 1.1.1 æ•°å­¦å®šä¹‰

**å®šä¹‰ 1.1** (å¤§æ•°æ®)
å¤§æ•°æ®æ˜¯å…·æœ‰4Vç‰¹å¾çš„æ•°æ®é›†åˆï¼š

$$BigData = \{Volume, Velocity, Variety, Veracity\}$$

å…¶ä¸­ï¼š

- $Volume$ï¼šæ•°æ®é‡ï¼Œé€šå¸¸ä»¥TBã€PBã€EBä¸ºå•ä½
- $Velocity$ï¼šæ•°æ®é€Ÿåº¦ï¼Œå®æ—¶æˆ–è¿‘å®æ—¶å¤„ç†
- $Variety$ï¼šæ•°æ®å¤šæ ·æ€§ï¼Œç»“æ„åŒ–ã€åŠç»“æ„åŒ–ã€éç»“æ„åŒ–
- $Veracity$ï¼šæ•°æ®çœŸå®æ€§ï¼Œæ•°æ®è´¨é‡å’Œå¯ä¿¡åº¦

**å®šä¹‰ 1.2** (æ•°æ®é‡çº§)
æ•°æ®é‡çº§å®šä¹‰ï¼š

$$
DataScale = \begin{cases}
KB & \text{if } |Data| < 2^{10} \\
MB & \text{if } 2^{10} \leq |Data| < 2^{20} \\
GB & \text{if } 2^{20} \leq |Data| < 2^{30} \\
TB & \text{if } 2^{30} \leq |Data| < 2^{40} \\
PB & \text{if } 2^{40} \leq |Data| < 2^{50} \\
EB & \text{if } 2^{50} \leq |Data| < 2^{60}
\end{cases}
$$

### 1.2 å¤§æ•°æ®ç‰¹å¾

#### 1.2.1 æ•°å­¦å®šä¹‰

**å®šä¹‰ 1.3** (æ•°æ®é‡ç‰¹å¾)
æ•°æ®é‡ç‰¹å¾æ»¡è¶³ï¼š

$$Volume(t) = \int_{0}^{t} DataRate(\tau) d\tau$$

å…¶ä¸­ $DataRate(t)$ æ˜¯æ—¶åˆ» $t$ çš„æ•°æ®äº§ç”Ÿé€Ÿç‡ã€‚

**å®šä¹‰ 1.4** (æ•°æ®é€Ÿåº¦ç‰¹å¾)
æ•°æ®é€Ÿåº¦ç‰¹å¾ï¼š

$$Velocity = \frac{\Delta Data}{\Delta Time}$$

**å®šä¹‰ 1.5** (æ•°æ®å¤šæ ·æ€§)
æ•°æ®å¤šæ ·æ€§åŒ…æ‹¬ï¼š

$$Variety = \{Structured, SemiStructured, Unstructured\}$$

å…¶ä¸­ï¼š

- $Structured = \{Relational, Tabular, Matrix\}$
- $SemiStructured = \{JSON, XML, Log\}$
- $Unstructured = \{Text, Image, Video, Audio\}$

### 1.3 å¤§æ•°æ®å¤„ç†æ¨¡å¼

#### 1.3.1 æ•°å­¦å®šä¹‰

**å®šä¹‰ 1.6** (æ‰¹å¤„ç†)
æ‰¹å¤„ç†æ¨¡å¼ï¼š

$$BatchProcessing = \{(Data_i, Process_i) \mid i = 1, 2, \ldots, n\}$$

å…¶ä¸­ $Data_i$ æ˜¯æ•°æ®æ‰¹æ¬¡ï¼Œ$Process_i$ æ˜¯å¤„ç†å‡½æ•°ã€‚

**å®šä¹‰ 1.7** (æµå¤„ç†)
æµå¤„ç†æ¨¡å¼ï¼š

$$StreamProcessing = \{(Data_t, Process_t) \mid t \in \mathbb{R}^+\}$$

å…¶ä¸­ $Data_t$ æ˜¯æ—¶åˆ» $t$ çš„æ•°æ®æµã€‚

**å®šä¹‰ 1.8** (äº¤äº’å¼å¤„ç†)
äº¤äº’å¼å¤„ç†æ¨¡å¼ï¼š

$$InteractiveProcessing = \{(Query_i, Response_i) \mid i = 1, 2, \ldots\}$$

## 2. åˆ†å¸ƒå¼è®¡ç®—ç†è®º

### 2.1 MapReduceæ¨¡å‹

#### 2.1.1 æ•°å­¦å®šä¹‰

**å®šä¹‰ 2.1** (Mapå‡½æ•°)
Mapå‡½æ•°å°†è¾“å…¥æ•°æ®è½¬æ¢ä¸ºé”®å€¼å¯¹ï¼š

$$Map: (k_1, v_1) \rightarrow [(k_2, v_2)]$$

**å®šä¹‰ 2.2** (Reduceå‡½æ•°)
Reduceå‡½æ•°èšåˆç›¸åŒé”®çš„å€¼ï¼š

$$Reduce: (k_2, [v_2]) \rightarrow [(k_3, v_3)]$$

**å®šä¹‰ 2.3** (MapReduceä½œä¸š)
MapReduceä½œä¸šå®šä¹‰ä¸ºï¼š

$$MapReduce = Map \circ Shuffle \circ Reduce$$

å…¶ä¸­ $Shuffle$ æ˜¯æ•°æ®é‡åˆ†å¸ƒè¿‡ç¨‹ã€‚

### 2.2 åˆ†å¸ƒå¼ç³»ç»Ÿç†è®º

#### 2.2.1 æ•°å­¦å®šä¹‰

**å®šä¹‰ 2.4** (åˆ†å¸ƒå¼ç³»ç»Ÿ)
åˆ†å¸ƒå¼ç³»ç»Ÿæ˜¯ä¸€ä¸ªèŠ‚ç‚¹é›†åˆï¼š

$$DistributedSystem = \{Node_1, Node_2, \ldots, Node_n\}$$

å…¶ä¸­æ¯ä¸ªèŠ‚ç‚¹ $Node_i$ å…·æœ‰ï¼š

- è®¡ç®—èƒ½åŠ› $CPU_i$
- å­˜å‚¨èƒ½åŠ› $Storage_i$
- ç½‘ç»œè¿æ¥ $Network_i$

**å®šä¹‰ 2.5** (ç³»ç»Ÿå¯ç”¨æ€§)
ç³»ç»Ÿå¯ç”¨æ€§å®šä¹‰ä¸ºï¼š

$$Availability = \frac{MTTF}{MTTF + MTTR}$$

å…¶ä¸­ï¼š

- $MTTF$ï¼šå¹³å‡æ•…éšœæ—¶é—´
- $MTTR$ï¼šå¹³å‡ä¿®å¤æ—¶é—´

**å®šä¹‰ 2.6** (ç³»ç»Ÿååé‡)
ç³»ç»Ÿååé‡ï¼š

$$Throughput = \frac{TotalWork}{TotalTime}$$

### 2.3 å®¹é”™ç†è®º

#### 2.3.1 æ•°å­¦å®šä¹‰

**å®šä¹‰ 2.7** (å®¹é”™èƒ½åŠ›)
å®¹é”™èƒ½åŠ›å®šä¹‰ä¸ºç³»ç»Ÿåœ¨æ•…éšœèŠ‚ç‚¹æ•°é‡ä¸‹çš„å¯ç”¨æ€§ï¼š

$$FaultTolerance(f) = P(SystemAvailable \mid f \text{ nodes failed})$$

**å®šä¹‰ 2.8** (æ•°æ®å¤åˆ¶)
æ•°æ®å¤åˆ¶ç­–ç•¥ï¼š

$$Replication = \{Primary, Secondary_1, Secondary_2, \ldots, Secondary_n\}$$

**å®šç† 2.1** (CAPå®šç†)
åˆ†å¸ƒå¼ç³»ç»Ÿæœ€å¤šåªèƒ½åŒæ—¶æ»¡è¶³ä»¥ä¸‹ä¸‰ä¸ªç‰¹æ€§ä¸­çš„ä¸¤ä¸ªï¼š

- Consistencyï¼ˆä¸€è‡´æ€§ï¼‰
- Availabilityï¼ˆå¯ç”¨æ€§ï¼‰
- Partition toleranceï¼ˆåˆ†åŒºå®¹é”™æ€§ï¼‰

## 3. æ•°æ®å¤„ç†æ¡†æ¶

### 3.1 Hadoopç”Ÿæ€ç³»ç»Ÿ

#### 3.1.1 æ•°å­¦å®šä¹‰

**å®šä¹‰ 3.1** (HDFS)
Hadoopåˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿï¼š

$$HDFS = \{NameNode, DataNodes, Blocks\}$$

å…¶ä¸­ï¼š

- $NameNode$ï¼šå…ƒæ•°æ®ç®¡ç†èŠ‚ç‚¹
- $DataNodes = \{DN_1, DN_2, \ldots, DN_n\}$ï¼šæ•°æ®èŠ‚ç‚¹
- $Blocks = \{Block_1, Block_2, \ldots, Block_m\}$ï¼šæ•°æ®å—

**å®šä¹‰ 3.2** (æ•°æ®å—åˆ†å¸ƒ)
æ•°æ®å—åˆ†å¸ƒç­–ç•¥ï¼š

$$BlockDistribution(Block_i) = \{DN_j \mid j \in \{1, 2, \ldots, n\}\}$$

**å®šä¹‰ 3.3** (å‰¯æœ¬å› å­)
å‰¯æœ¬å› å­å®šä¹‰ä¸ºï¼š

$$ReplicationFactor = \frac{TotalCopies}{OriginalBlocks}$$

### 3.2 Sparkæ¡†æ¶

#### 3.2.1 æ•°å­¦å®šä¹‰

**å®šä¹‰ 3.4** (RDD)
å¼¹æ€§åˆ†å¸ƒå¼æ•°æ®é›†ï¼š

$$RDD = \{Partition_1, Partition_2, \ldots, Partition_k\}$$

å…¶ä¸­æ¯ä¸ªåˆ†åŒº $Partition_i$ åŒ…å«æ•°æ®å­é›†ã€‚

**å®šä¹‰ 3.5** (è½¬æ¢æ“ä½œ)
RDDè½¬æ¢æ“ä½œï¼š

$$Transform: RDD \rightarrow RDD$$

**å®šä¹‰ 3.6** (è¡ŒåŠ¨æ“ä½œ)
RDDè¡ŒåŠ¨æ“ä½œï¼š

$$Action: RDD \rightarrow Result$$

### 3.3 æµå¤„ç†æ¡†æ¶

#### 3.3.1 æ•°å­¦å®šä¹‰

**å®šä¹‰ 3.7** (æ•°æ®æµ)
æ•°æ®æµå®šä¹‰ä¸ºæ—¶é—´åºåˆ—ï¼š

$$DataStream = \{(t_i, data_i) \mid i = 1, 2, \ldots\}$$

**å®šä¹‰ 3.8** (çª—å£æ“ä½œ)
çª—å£æ“ä½œï¼š

$$Window(t, w) = \{data_i \mid t - w \leq t_i \leq t\}$$

å…¶ä¸­ $w$ æ˜¯çª—å£å¤§å°ã€‚

**å®šä¹‰ 3.9** (æ»‘åŠ¨çª—å£)
æ»‘åŠ¨çª—å£ï¼š

$$SlidingWindow(t, w, s) = \{data_i \mid t - w + k \cdot s \leq t_i \leq t + k \cdot s\}$$

å…¶ä¸­ $s$ æ˜¯æ»‘åŠ¨æ­¥é•¿ã€‚

## 4. æ•°æ®åˆ†æç®—æ³•

### 4.1 æ•°æ®é¢„å¤„ç†

#### 4.1.1 æ•°å­¦å®šä¹‰

**å®šä¹‰ 4.1** (æ•°æ®æ¸…æ´—)
æ•°æ®æ¸…æ´—å‡½æ•°ï¼š

$$Clean: RawData \rightarrow CleanData$$

**å®šä¹‰ 4.2** (æ•°æ®æ ‡å‡†åŒ–)
æ•°æ®æ ‡å‡†åŒ–ï¼š

$$Standardize(x) = \frac{x - \mu}{\sigma}$$

å…¶ä¸­ $\mu$ æ˜¯å‡å€¼ï¼Œ$\sigma$ æ˜¯æ ‡å‡†å·®ã€‚

**å®šä¹‰ 4.3** (æ•°æ®å½’ä¸€åŒ–)
æ•°æ®å½’ä¸€åŒ–ï¼š

$$Normalize(x) = \frac{x - x_{min}}{x_{max} - x_{min}}$$

### 4.2 èšç±»ç®—æ³•

#### 4.2.1 æ•°å­¦å®šä¹‰

**å®šä¹‰ 4.4** (K-meansèšç±»)
K-meansç›®æ ‡å‡½æ•°ï¼š

$$J = \sum_{i=1}^{k} \sum_{x \in C_i} \|x - \mu_i\|^2$$

å…¶ä¸­ $C_i$ æ˜¯ç¬¬ $i$ ä¸ªèšç±»ï¼Œ$\mu_i$ æ˜¯èšç±»ä¸­å¿ƒã€‚

**å®šä¹‰ 4.5** (å±‚æ¬¡èšç±»)
å±‚æ¬¡èšç±»è·ç¦»ï¼š

$$d(C_i, C_j) = \min_{x \in C_i, y \in C_j} d(x, y)$$

### 4.3 å…³è”è§„åˆ™æŒ–æ˜

#### 4.3.1 æ•°å­¦å®šä¹‰

**å®šä¹‰ 4.6** (æ”¯æŒåº¦)
æ”¯æŒåº¦å®šä¹‰ä¸ºï¼š

$$Support(X) = \frac{|T_X|}{|T|}$$

å…¶ä¸­ $T_X$ æ˜¯åŒ…å«é¡¹é›† $X$ çš„äº‹åŠ¡é›†åˆã€‚

**å®šä¹‰ 4.7** (ç½®ä¿¡åº¦)
ç½®ä¿¡åº¦å®šä¹‰ä¸ºï¼š

$$Confidence(X \rightarrow Y) = \frac{Support(X \cup Y)}{Support(X)}$$

**å®šä¹‰ 4.8** (æå‡åº¦)
æå‡åº¦å®šä¹‰ä¸ºï¼š

$$Lift(X \rightarrow Y) = \frac{Confidence(X \rightarrow Y)}{Support(Y)}$$

## 5. Pythonå®ç°

### 5.1 åŸºç¡€æ¡†æ¶

```python
"""
å¤§æ•°æ®åŸºç¡€å®ç°
ä½œè€…ï¼šAIåŠ©æ‰‹
æ—¥æœŸï¼š2024å¹´
ç‰ˆæœ¬ï¼š1.0
"""

from __future__ import annotations
from abc import ABC, abstractmethod
from typing import Any, Dict, List, Optional, Tuple, Union, Iterator
import numpy as np
import pandas as pd
from dataclasses import dataclass, field
from enum import Enum
import time
import json
import hashlib
from collections import defaultdict, Counter
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
import warnings
warnings.filterwarnings('ignore')

@dataclass
class DataRecord:
    """æ•°æ®è®°å½•"""
    id: str
    timestamp: float
    data: Dict[str, Any]

    def __post_init__(self):
        if not self.id:
            self.id = hashlib.md5(f"{self.timestamp}{str(self.data)}".encode()).hexdigest()[:8]

@dataclass
class DataPartition:
    """æ•°æ®åˆ†åŒº"""
    id: str
    records: List[DataRecord]
    size: int = 0

    def __post_init__(self):
        self.size = len(self.records)

class DataNode:
    """æ•°æ®èŠ‚ç‚¹"""

    def __init__(self, node_id: str, capacity: int = 1000):
        self.node_id = node_id
        self.capacity = capacity
        self.partitions: Dict[str, DataPartition] = {}
        self.available_space = capacity

    def add_partition(self, partition: DataPartition) -> bool:
        """æ·»åŠ åˆ†åŒº"""
        if partition.size <= self.available_space:
            self.partitions[partition.id] = partition
            self.available_space -= partition.size
            return True
        return False

    def remove_partition(self, partition_id: str) -> Optional[DataPartition]:
        """ç§»é™¤åˆ†åŒº"""
        if partition_id in self.partitions:
            partition = self.partitions.pop(partition_id)
            self.available_space += partition.size
            return partition
        return None

    def get_partition(self, partition_id: str) -> Optional[DataPartition]:
        """è·å–åˆ†åŒº"""
        return self.partitions.get(partition_id)

class DistributedSystem:
    """åˆ†å¸ƒå¼ç³»ç»Ÿ"""

    def __init__(self):
        self.nodes: Dict[str, DataNode] = {}
        self.partitions: Dict[str, DataPartition] = {}

    def add_node(self, node_id: str, capacity: int = 1000) -> DataNode:
        """æ·»åŠ èŠ‚ç‚¹"""
        node = DataNode(node_id, capacity)
        self.nodes[node_id] = node
        return node

    def create_partition(self, records: List[DataRecord]) -> DataPartition:
        """åˆ›å»ºåˆ†åŒº"""
        partition_id = hashlib.md5(f"{time.time()}{len(records)}".encode()).hexdigest()[:8]
        partition = DataPartition(partition_id, records)
        self.partitions[partition_id] = partition
        return partition

    def distribute_partition(self, partition: DataPartition, replication_factor: int = 3) -> bool:
        """åˆ†å‘åˆ†åŒº"""
        available_nodes = [node for node in self.nodes.values() if node.available_space >= partition.size]

        if len(available_nodes) < replication_factor:
            return False

        # é€‰æ‹©èŠ‚ç‚¹è¿›è¡Œå¤åˆ¶
        selected_nodes = available_nodes[:replication_factor]

        for node in selected_nodes:
            node.add_partition(partition)

        return True

    def get_partition_locations(self, partition_id: str) -> List[str]:
        """è·å–åˆ†åŒºä½ç½®"""
        locations = []
        for node_id, node in self.nodes.items():
            if partition_id in node.partitions:
                locations.append(node_id)
        return locations
```

### 5.2 MapReduceå®ç°

```python
class MapReduce:
    """MapReduceæ¡†æ¶"""

    def __init__(self, distributed_system: DistributedSystem):
        self.system = distributed_system
        self.map_results: Dict[str, List[Tuple[str, Any]]] = {}
        self.reduce_results: Dict[str, Any] = {}

    def map_function(self, record: DataRecord) -> List[Tuple[str, Any]]:
        """Mapå‡½æ•°ï¼ˆå¯é‡å†™ï¼‰"""
        # é»˜è®¤å®ç°ï¼šæå–æ‰€æœ‰å­—æ®µä½œä¸ºé”®å€¼å¯¹
        results = []
        for key, value in record.data.items():
            results.append((key, value))
        return results

    def reduce_function(self, key: str, values: List[Any]) -> Any:
        """Reduceå‡½æ•°ï¼ˆå¯é‡å†™ï¼‰"""
        # é»˜è®¤å®ç°ï¼šè®¡ç®—å¹³å‡å€¼
        if isinstance(values[0], (int, float)):
            return sum(values) / len(values)
        else:
            return values

    def execute(self, partition_id: str) -> Dict[str, Any]:
        """æ‰§è¡ŒMapReduceä½œä¸š"""
        # è·å–åˆ†åŒº
        partition = self.system.partitions.get(partition_id)
        if not partition:
            raise ValueError(f"Partition {partition_id} not found")

        # Mapé˜¶æ®µ
        map_results = []
        for record in partition.records:
            map_output = self.map_function(record)
            map_results.extend(map_output)

        # Shuffleé˜¶æ®µ
        shuffled_data = defaultdict(list)
        for key, value in map_results:
            shuffled_data[key].append(value)

        # Reduceé˜¶æ®µ
        reduce_results = {}
        for key, values in shuffled_data.items():
            reduce_results[key] = self.reduce_function(key, values)

        return reduce_results

class WordCountMapReduce(MapReduce):
    """è¯é¢‘ç»Ÿè®¡MapReduce"""

    def map_function(self, record: DataRecord) -> List[Tuple[str, int]]:
        """Mapå‡½æ•°ï¼šæå–å•è¯"""
        text = str(record.data.get('text', ''))
        words = text.lower().split()
        return [(word, 1) for word in words]

    def reduce_function(self, key: str, values: List[int]) -> int:
        """Reduceå‡½æ•°ï¼šç»Ÿè®¡è¯é¢‘"""
        return sum(values)

class AverageMapReduce(MapReduce):
    """å¹³å‡å€¼è®¡ç®—MapReduce"""

    def map_function(self, record: DataRecord) -> List[Tuple[str, float]]:
        """Mapå‡½æ•°ï¼šæå–æ•°å€¼"""
        value = record.data.get('value', 0)
        category = record.data.get('category', 'default')
        return [(category, float(value))]

    def reduce_function(self, key: str, values: List[float]) -> float:
        """Reduceå‡½æ•°ï¼šè®¡ç®—å¹³å‡å€¼"""
        return sum(values) / len(values)
```

### 5.3 æµå¤„ç†å®ç°

```python
class DataStream:
    """æ•°æ®æµ"""

    def __init__(self):
        self.records: List[DataRecord] = []
        self.max_size = 10000

    def add_record(self, record: DataRecord) -> None:
        """æ·»åŠ è®°å½•"""
        self.records.append(record)

        # ä¿æŒæµå¤§å°
        if len(self.records) > self.max_size:
            self.records.pop(0)

    def get_window(self, window_size: int) -> List[DataRecord]:
        """è·å–çª—å£æ•°æ®"""
        return self.records[-window_size:] if len(self.records) >= window_size else self.records

    def get_sliding_window(self, window_size: int, step: int = 1) -> Iterator[List[DataRecord]]:
        """è·å–æ»‘åŠ¨çª—å£"""
        for i in range(0, len(self.records) - window_size + 1, step):
            yield self.records[i:i + window_size]

class StreamProcessor:
    """æµå¤„ç†å™¨"""

    def __init__(self, stream: DataStream):
        self.stream = stream
        self.processors: List[StreamProcessor] = []

    def add_processor(self, processor: 'StreamProcessor') -> None:
        """æ·»åŠ å¤„ç†å™¨"""
        self.processors.append(processor)

    def process(self, window_size: int = 100) -> Any:
        """å¤„ç†æ•°æ®æµ"""
        window = self.stream.get_window(window_size)
        return self.process_window(window)

    def process_window(self, window: List[DataRecord]) -> Any:
        """å¤„ç†çª—å£æ•°æ®ï¼ˆå¯é‡å†™ï¼‰"""
        return len(window)

class MovingAverageProcessor(StreamProcessor):
    """ç§»åŠ¨å¹³å‡å¤„ç†å™¨"""

    def __init__(self, stream: DataStream, field: str):
        super().__init__(stream)
        self.field = field

    def process_window(self, window: List[DataRecord]) -> float:
        """è®¡ç®—ç§»åŠ¨å¹³å‡"""
        values = []
        for record in window:
            value = record.data.get(self.field)
            if isinstance(value, (int, float)):
                values.append(value)

        return sum(values) / len(values) if values else 0.0

class AnomalyDetector(StreamProcessor):
    """å¼‚å¸¸æ£€æµ‹å™¨"""

    def __init__(self, stream: DataStream, field: str, threshold: float = 2.0):
        super().__init__(stream)
        self.field = field
        self.threshold = threshold
        self.mean = 0.0
        self.std = 1.0
        self.count = 0

    def process_window(self, window: List[DataRecord]) -> List[DataRecord]:
        """æ£€æµ‹å¼‚å¸¸"""
        values = []
        for record in window:
            value = record.data.get(self.field)
            if isinstance(value, (int, float)):
                values.append(value)

        if not values:
            return []

        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        self.update_statistics(values)

        # æ£€æµ‹å¼‚å¸¸
        anomalies = []
        for record in window:
            value = record.data.get(self.field)
            if isinstance(value, (int, float)):
                z_score = abs((value - self.mean) / self.std)
                if z_score > self.threshold:
                    anomalies.append(record)

        return anomalies

    def update_statistics(self, values: List[float]) -> None:
        """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
        if not values:
            return

        # åœ¨çº¿æ›´æ–°å‡å€¼å’Œæ ‡å‡†å·®
        for value in values:
            self.count += 1
            delta = value - self.mean
            self.mean += delta / self.count
            delta2 = value - self.mean
            self.std = np.sqrt((self.std ** 2 * (self.count - 1) + delta * delta2) / self.count)
```

### 5.4 æ•°æ®åˆ†æç®—æ³•å®ç°

```python
class DataAnalyzer:
    """æ•°æ®åˆ†æå™¨"""

    def __init__(self):
        self.statistics: Dict[str, Dict[str, float]] = {}

    def analyze_partition(self, partition: DataPartition) -> Dict[str, Any]:
        """åˆ†æåˆ†åŒºæ•°æ®"""
        if not partition.records:
            return {}

        # æå–æ•°å€¼å­—æ®µ
        numeric_fields = self.extract_numeric_fields(partition.records)

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        stats = {}
        for field in numeric_fields:
            values = [record.data[field] for record in partition.records if field in record.data]
            if values:
                stats[field] = {
                    'count': len(values),
                    'mean': np.mean(values),
                    'std': np.std(values),
                    'min': np.min(values),
                    'max': np.max(values),
                    'median': np.median(values)
                }

        return stats

    def extract_numeric_fields(self, records: List[DataRecord]) -> List[str]:
        """æå–æ•°å€¼å­—æ®µ"""
        numeric_fields = set()
        for record in records:
            for key, value in record.data.items():
                if isinstance(value, (int, float)):
                    numeric_fields.add(key)
        return list(numeric_fields)

class ClusteringAnalyzer:
    """èšç±»åˆ†æå™¨"""

    def __init__(self, k: int = 3):
        self.k = k
        self.centroids: Optional[np.ndarray] = None
        self.labels: Optional[np.ndarray] = None

    def kmeans_clustering(self, data: np.ndarray, max_iterations: int = 100) -> Tuple[np.ndarray, np.ndarray]:
        """K-meansèšç±»"""
        n_samples, n_features = data.shape

        # éšæœºåˆå§‹åŒ–èšç±»ä¸­å¿ƒ
        centroids = data[np.random.choice(n_samples, self.k, replace=False)]

        for iteration in range(max_iterations):
            # åˆ†é…æ ·æœ¬åˆ°æœ€è¿‘çš„èšç±»ä¸­å¿ƒ
            distances = np.sqrt(((data[:, np.newaxis, :] - centroids[np.newaxis, :, :]) ** 2).sum(axis=2))
            labels = np.argmin(distances, axis=1)

            # æ›´æ–°èšç±»ä¸­å¿ƒ
            new_centroids = np.zeros_like(centroids)
            for k in range(self.k):
                if np.sum(labels == k) > 0:
                    new_centroids[k] = np.mean(data[labels == k], axis=0)
                else:
                    new_centroids[k] = centroids[k]

            # æ£€æŸ¥æ”¶æ•›
            if np.allclose(centroids, new_centroids):
                break

            centroids = new_centroids

        self.centroids = centroids
        self.labels = labels

        return centroids, labels

    def analyze_clusters(self, data: np.ndarray, labels: np.ndarray) -> Dict[str, Any]:
        """åˆ†æèšç±»ç»“æœ"""
        analysis = {
            'n_clusters': self.k,
            'cluster_sizes': [],
            'cluster_centers': self.centroids.tolist() if self.centroids is not None else [],
            'inertia': 0.0
        }

        if self.centroids is not None:
            # è®¡ç®—èšç±»å¤§å°
            for k in range(self.k):
                cluster_size = np.sum(labels == k)
                analysis['cluster_sizes'].append(int(cluster_size))

            # è®¡ç®—æƒ¯æ€§ï¼ˆç°‡å†…å¹³æ–¹å’Œï¼‰
            inertia = 0
            for k in range(self.k):
                cluster_points = data[labels == k]
                if len(cluster_points) > 0:
                    inertia += np.sum((cluster_points - self.centroids[k]) ** 2)
            analysis['inertia'] = float(inertia)

        return analysis

class AssociationRuleMiner:
    """å…³è”è§„åˆ™æŒ–æ˜å™¨"""

    def __init__(self, min_support: float = 0.1, min_confidence: float = 0.5):
        self.min_support = min_support
        self.min_confidence = min_confidence
        self.frequent_itemsets: Dict[frozenset, int] = {}
        self.association_rules: List[Dict[str, Any]] = []

    def find_frequent_itemsets(self, transactions: List[List[str]]) -> Dict[frozenset, int]:
        """æŸ¥æ‰¾é¢‘ç¹é¡¹é›†"""
        # è®¡ç®—é¡¹çš„æ”¯æŒåº¦
        item_counts = Counter()
        for transaction in transactions:
            for item in transaction:
                item_counts[item] += 1

        # è¿‡æ»¤æ»¡è¶³æœ€å°æ”¯æŒåº¦çš„é¡¹
        n_transactions = len(transactions)
        frequent_items = {item for item, count in item_counts.items()
                         if count / n_transactions >= self.min_support}

        # ç”Ÿæˆé¢‘ç¹é¡¹é›†
        self.frequent_itemsets = {}
        for k in range(1, len(frequent_items) + 1):
            for itemset in self.generate_k_itemsets(frequent_items, k):
                support = self.calculate_support(itemset, transactions)
                if support >= self.min_support:
                    self.frequent_itemsets[itemset] = support

        return self.frequent_itemsets

    def generate_k_itemsets(self, items: set, k: int) -> Iterator[frozenset]:
        """ç”Ÿæˆké¡¹é›†"""
        if k == 1:
            for item in items:
                yield frozenset([item])
        else:
            items_list = list(items)
            for i in range(len(items_list)):
                for j in range(i + 1, len(items_list)):
                    yield frozenset([items_list[i], items_list[j]])

    def calculate_support(self, itemset: frozenset, transactions: List[List[str]]) -> float:
        """è®¡ç®—æ”¯æŒåº¦"""
        count = 0
        for transaction in transactions:
            if itemset.issubset(transaction):
                count += 1
        return count / len(transactions)

    def generate_rules(self, transactions: List[List[str]]) -> List[Dict[str, Any]]:
        """ç”Ÿæˆå…³è”è§„åˆ™"""
        if not self.frequent_itemsets:
            self.find_frequent_itemsets(transactions)

        self.association_rules = []

        for itemset, support in self.frequent_itemsets.items():
            if len(itemset) < 2:
                continue

            # ç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„è§„åˆ™
            items_list = list(itemset)
            for i in range(1, len(items_list)):
                for antecedent in self.generate_subsets(items_list, i):
                    consequent = itemset - antecedent
                    confidence = support / self.frequent_itemsets.get(antecedent, 0)

                    if confidence >= self.min_confidence:
                        rule = {
                            'antecedent': list(antecedent),
                            'consequent': list(consequent),
                            'support': support,
                            'confidence': confidence,
                            'lift': confidence / self.frequent_itemsets.get(consequent, 0)
                        }
                        self.association_rules.append(rule)

        return self.association_rules

    def generate_subsets(self, items: List[str], k: int) -> Iterator[frozenset]:
        """ç”Ÿæˆå­é›†"""
        if k == 0:
            yield frozenset()
        elif k == 1:
            for item in items:
                yield frozenset([item])
        else:
            for i in range(len(items) - k + 1):
                for subset in self.generate_subsets(items[i+1:], k-1):
                    yield frozenset([items[i]]) | subset
```

## 6. å®è·µåº”ç”¨

### 6.1 å¤§æ•°æ®å¤„ç†æ¼”ç¤º

```python
def big_data_processing_demo():
    """å¤§æ•°æ®å¤„ç†æ¼”ç¤º"""
    print("=== å¤§æ•°æ®å¤„ç†æ¼”ç¤º ===\n")

    # åˆ›å»ºåˆ†å¸ƒå¼ç³»ç»Ÿ
    system = DistributedSystem()

    # æ·»åŠ æ•°æ®èŠ‚ç‚¹
    for i in range(5):
        system.add_node(f"node-{i}", capacity=1000)

    print("1. åˆ†å¸ƒå¼ç³»ç»Ÿè®¾ç½®")
    print(f"èŠ‚ç‚¹æ•°é‡: {len(system.nodes)}")
    for node_id, node in system.nodes.items():
        print(f"  èŠ‚ç‚¹ {node_id}: å®¹é‡ {node.capacity}, å¯ç”¨ç©ºé—´ {node.available_space}")

    # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
    print("\n2. æ•°æ®ç”Ÿæˆ")
    records = []
    for i in range(1000):
        record = DataRecord(
            id=f"record-{i}",
            timestamp=time.time() + i,
            data={
                'user_id': f"user-{i % 100}",
                'product_id': f"product-{i % 50}",
                'price': np.random.uniform(10, 1000),
                'quantity': np.random.randint(1, 10),
                'category': f"cat-{i % 10}",
                'rating': np.random.uniform(1, 5)
            }
        )
        records.append(record)

    print(f"ç”Ÿæˆè®°å½•æ•°: {len(records)}")

    # åˆ›å»ºåˆ†åŒº
    print("\n3. æ•°æ®åˆ†åŒº")
    partition_size = 200
    partitions = []

    for i in range(0, len(records), partition_size):
        partition_records = records[i:i + partition_size]
        partition = system.create_partition(partition_records)
        partitions.append(partition)
        print(f"åˆ›å»ºåˆ†åŒº {partition.id}: {len(partition_records)} æ¡è®°å½•")

    # åˆ†å‘åˆ†åŒº
    print("\n4. åˆ†åŒºåˆ†å‘")
    for partition in partitions:
        success = system.distribute_partition(partition, replication_factor=3)
        if success:
            locations = system.get_partition_locations(partition.id)
            print(f"åˆ†åŒº {partition.id} åˆ†å‘åˆ°èŠ‚ç‚¹: {locations}")
        else:
            print(f"åˆ†åŒº {partition.id} åˆ†å‘å¤±è´¥")

    # MapReduceå¤„ç†
    print("\n5. MapReduceå¤„ç†")

    # è¯é¢‘ç»Ÿè®¡
    word_count_mr = WordCountMapReduce(system)
    text_records = [
        DataRecord("1", time.time(), {"text": "hello world hello python"}),
        DataRecord("2", time.time(), {"text": "python programming is fun"}),
        DataRecord("3", time.time(), {"text": "hello python world"})
    ]

    text_partition = system.create_partition(text_records)
    word_count_results = word_count_mr.execute(text_partition.id)
    print("è¯é¢‘ç»Ÿè®¡ç»“æœ:")
    for word, count in word_count_results.items():
        print(f"  {word}: {count}")

    # å¹³å‡å€¼è®¡ç®—
    avg_mr = AverageMapReduce(system)
    avg_results = avg_mr.execute(partitions[0].id)
    print("\nå¹³å‡å€¼è®¡ç®—ç»“æœ:")
    for category, avg_value in avg_results.items():
        print(f"  {category}: {avg_value:.2f}")

    return system, partitions

def stream_processing_demo():
    """æµå¤„ç†æ¼”ç¤º"""
    print("\n=== æµå¤„ç†æ¼”ç¤º ===\n")

    # åˆ›å»ºæ•°æ®æµ
    stream = DataStream()

    # ç”Ÿæˆæµæ•°æ®
    print("1. ç”Ÿæˆæµæ•°æ®")
    for i in range(100):
        record = DataRecord(
            id=f"stream-{i}",
            timestamp=time.time() + i,
            data={
                'sensor_id': f"sensor-{i % 5}",
                'temperature': np.random.normal(25, 5),
                'humidity': np.random.normal(60, 10),
                'pressure': np.random.normal(1013, 20)
            }
        )
        stream.add_record(record)

    print(f"æµæ•°æ®è®°å½•æ•°: {len(stream.records)}")

    # ç§»åŠ¨å¹³å‡å¤„ç†
    print("\n2. ç§»åŠ¨å¹³å‡å¤„ç†")
    ma_processor = MovingAverageProcessor(stream, 'temperature')

    window_sizes = [10, 20, 50]
    for window_size in window_sizes:
        avg_temp = ma_processor.process(window_size)
        print(f"çª—å£å¤§å° {window_size}: å¹³å‡æ¸©åº¦ {avg_temp:.2f}Â°C")

    # å¼‚å¸¸æ£€æµ‹
    print("\n3. å¼‚å¸¸æ£€æµ‹")
    anomaly_detector = AnomalyDetector(stream, 'temperature', threshold=2.0)

    window = stream.get_window(50)
    anomalies = anomaly_detector.process_window(window)

    print(f"æ£€æµ‹åˆ° {len(anomalies)} ä¸ªå¼‚å¸¸:")
    for anomaly in anomalies[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
        temp = anomaly.data['temperature']
        print(f"  æ—¶é—´ {anomaly.timestamp}: æ¸©åº¦ {temp:.2f}Â°C")

    return stream, ma_processor, anomaly_detector

def data_analysis_demo():
    """æ•°æ®åˆ†ææ¼”ç¤º"""
    print("\n=== æ•°æ®åˆ†ææ¼”ç¤º ===\n")

    # åˆ›å»ºåˆ†æå™¨
    analyzer = DataAnalyzer()
    clustering_analyzer = ClusteringAnalyzer(k=3)

    # ç”Ÿæˆåˆ†ææ•°æ®
    print("1. ç”Ÿæˆåˆ†ææ•°æ®")
    records = []
    for i in range(300):
        # ç”Ÿæˆ3ä¸ªèšç±»çš„æ•°æ®
        if i < 100:
            cluster = 0
            x = np.random.normal(0, 1)
            y = np.random.normal(0, 1)
        elif i < 200:
            cluster = 1
            x = np.random.normal(5, 1)
            y = np.random.normal(5, 1)
        else:
            cluster = 2
            x = np.random.normal(10, 1)
            y = np.random.normal(0, 1)

        record = DataRecord(
            id=f"analysis-{i}",
            timestamp=time.time() + i,
            data={
                'x': x,
                'y': y,
                'cluster': cluster,
                'value': np.random.uniform(0, 100)
            }
        )
        records.append(record)

    partition = DataPartition("analysis-partition", records)

    # ç»Ÿè®¡åˆ†æ
    print("\n2. ç»Ÿè®¡åˆ†æ")
    stats = analyzer.analyze_partition(partition)

    for field, field_stats in stats.items():
        print(f"\n{field} ç»Ÿè®¡:")
        for stat_name, stat_value in field_stats.items():
            print(f"  {stat_name}: {stat_value:.4f}")

    # èšç±»åˆ†æ
    print("\n3. èšç±»åˆ†æ")
    data = np.array([[record.data['x'], record.data['y']] for record in records])
    centroids, labels = clustering_analyzer.kmeans_clustering(data)

    cluster_analysis = clustering_analyzer.analyze_clusters(data, labels)
    print(f"èšç±»æ•°é‡: {cluster_analysis['n_clusters']}")
    print(f"èšç±»å¤§å°: {cluster_analysis['cluster_sizes']}")
    print(f"æƒ¯æ€§: {cluster_analysis['inertia']:.4f}")

    # å…³è”è§„åˆ™æŒ–æ˜
    print("\n4. å…³è”è§„åˆ™æŒ–æ˜")
    transactions = [
        ['milk', 'bread', 'butter'],
        ['milk', 'bread', 'eggs'],
        ['milk', 'bread', 'cheese'],
        ['bread', 'butter', 'eggs'],
        ['milk', 'cheese', 'yogurt'],
        ['bread', 'cheese', 'ham'],
        ['milk', 'bread', 'ham'],
        ['bread', 'butter', 'cheese'],
        ['milk', 'yogurt', 'fruits'],
        ['bread', 'milk', 'fruits']
    ]

    rule_miner = AssociationRuleMiner(min_support=0.3, min_confidence=0.6)
    rules = rule_miner.generate_rules(transactions)

    print(f"å‘ç° {len(rules)} æ¡å…³è”è§„åˆ™:")
    for i, rule in enumerate(rules[:5]):  # åªæ˜¾ç¤ºå‰5æ¡
        print(f"  è§„åˆ™ {i+1}: {rule['antecedent']} -> {rule['consequent']}")
        print(f"    æ”¯æŒåº¦: {rule['support']:.3f}, ç½®ä¿¡åº¦: {rule['confidence']:.3f}, æå‡åº¦: {rule['lift']:.3f}")

    return analyzer, clustering_analyzer, rule_miner

def performance_analysis():
    """æ€§èƒ½åˆ†æ"""
    print("\n=== æ€§èƒ½åˆ†æ ===\n")

    # æµ‹è¯•ä¸åŒæ•°æ®è§„æ¨¡çš„å¤„ç†æ—¶é—´
    data_sizes = [1000, 5000, 10000, 50000]
    processing_times = []

    for size in data_sizes:
        print(f"æµ‹è¯•æ•°æ®è§„æ¨¡: {size}")

        # ç”Ÿæˆæ•°æ®
        start_time = time.time()
        records = []
        for i in range(size):
            record = DataRecord(
                id=f"perf-{i}",
                timestamp=time.time() + i,
                data={
                    'value': np.random.uniform(0, 100),
                    'category': f"cat-{i % 10}",
                    'score': np.random.uniform(0, 1)
                }
            )
            records.append(record)

        # åˆ›å»ºåˆ†åŒº
        partition = DataPartition(f"perf-{size}", records)

        # ç»Ÿè®¡åˆ†æ
        analyzer = DataAnalyzer()
        stats = analyzer.analyze_partition(partition)

        end_time = time.time()
        processing_time = end_time - start_time
        processing_times.append(processing_time)

        print(f"  å¤„ç†æ—¶é—´: {processing_time:.4f}ç§’")
        print(f"  è®°å½•æ•°: {len(records)}")
        print(f"  ç»Ÿè®¡å­—æ®µæ•°: {len(stats)}")

    # å¯è§†åŒ–æ€§èƒ½ç»“æœ
    import matplotlib.pyplot as plt

    plt.figure(figsize=(10, 6))
    plt.plot(data_sizes, processing_times, 'bo-')
    plt.xlabel('æ•°æ®è§„æ¨¡')
    plt.ylabel('å¤„ç†æ—¶é—´ (ç§’)')
    plt.title('å¤§æ•°æ®å¤„ç†æ€§èƒ½åˆ†æ')
    plt.grid(True)
    plt.show()

    print(f"\næ€§èƒ½æ€»ç»“:")
    for i, size in enumerate(data_sizes):
        throughput = size / processing_times[i]
        print(f"  æ•°æ®è§„æ¨¡ {size}: ååé‡ {throughput:.0f} è®°å½•/ç§’")

if __name__ == "__main__":
    # è¿è¡Œå¤§æ•°æ®å¤„ç†æ¼”ç¤º
    system, partitions = big_data_processing_demo()

    # è¿è¡Œæµå¤„ç†æ¼”ç¤º
    stream, ma_processor, anomaly_detector = stream_processing_demo()

    # è¿è¡Œæ•°æ®åˆ†ææ¼”ç¤º
    analyzer, clustering_analyzer, rule_miner = data_analysis_demo()

    # è¿è¡Œæ€§èƒ½åˆ†æ
    performance_analysis()
```

## 7. æ€»ç»“

### 7.1 æ ¸å¿ƒè¦ç‚¹

1. **å¤§æ•°æ®ç‰¹å¾**ï¼šVolumeï¼ˆæ•°æ®é‡ï¼‰ã€Velocityï¼ˆé€Ÿåº¦ï¼‰ã€Varietyï¼ˆå¤šæ ·æ€§ï¼‰ã€Veracityï¼ˆçœŸå®æ€§ï¼‰
2. **åˆ†å¸ƒå¼è®¡ç®—**ï¼šMapReduceã€å®¹é”™æœºåˆ¶ã€CAPå®šç†
3. **å¤„ç†æ¡†æ¶**ï¼šHadoopã€Sparkã€æµå¤„ç†
4. **åˆ†æç®—æ³•**ï¼šèšç±»ã€å…³è”è§„åˆ™ã€ç»Ÿè®¡åˆ†æ
5. **åº”ç”¨åœºæ™¯**ï¼šå•†ä¸šæ™ºèƒ½ã€æ¨èç³»ç»Ÿã€é£é™©æ§åˆ¶

### 7.2 å…³é”®å…¬å¼

- **æ•°æ®é‡çº§**ï¼š$DataScale = \begin{cases} KB & \text{if } |Data| < 2^{10} \\ MB & \text{if } 2^{10} \leq |Data| < 2^{20} \\ \ldots \end{cases}$
- **MapReduce**ï¼š$MapReduce = Map \circ Shuffle \circ Reduce$
- **ç³»ç»Ÿå¯ç”¨æ€§**ï¼š$Availability = \frac{MTTF}{MTTF + MTTR}$
- **K-meansç›®æ ‡**ï¼š$J = \sum_{i=1}^{k} \sum_{x \in C_i} \|x - \mu_i\|^2$
- **æ”¯æŒåº¦**ï¼š$Support(X) = \frac{|T_X|}{|T|}$

### 7.3 åº”ç”¨åœºæ™¯

1. **å•†ä¸šæ™ºèƒ½**ï¼šé”€å”®åˆ†æã€å®¢æˆ·è¡Œä¸ºåˆ†æã€å¸‚åœºè¶‹åŠ¿
2. **æ¨èç³»ç»Ÿ**ï¼šå•†å“æ¨èã€å†…å®¹æ¨èã€ä¸ªæ€§åŒ–æœåŠ¡
3. **é£é™©æ§åˆ¶**ï¼šæ¬ºè¯ˆæ£€æµ‹ã€ä¿¡ç”¨è¯„ä¼°ã€å¼‚å¸¸ç›‘æ§
4. **ç‰©è”ç½‘**ï¼šä¼ æ„Ÿå™¨æ•°æ®åˆ†æã€è®¾å¤‡ç›‘æ§ã€é¢„æµ‹ç»´æŠ¤
5. **é‡‘èç§‘æŠ€**ï¼šäº¤æ˜“åˆ†æã€é£é™©è¯„ä¼°ã€æŠ•èµ„å†³ç­–

### 7.4 æŠ€æœ¯æŒ‘æˆ˜

1. **æ•°æ®è´¨é‡**ï¼šæ•°æ®æ¸…æ´—ã€ä¸€è‡´æ€§ã€å®Œæ•´æ€§
2. **æ€§èƒ½ä¼˜åŒ–**ï¼šå¹¶è¡Œå¤„ç†ã€å†…å­˜ç®¡ç†ã€I/Oä¼˜åŒ–
3. **å¯æ‰©å±•æ€§**ï¼šæ°´å¹³æ‰©å±•ã€è´Ÿè½½å‡è¡¡ã€èµ„æºè°ƒåº¦
4. **å®æ—¶æ€§**ï¼šæµå¤„ç†ã€ä½å»¶è¿Ÿã€å®æ—¶åˆ†æ
5. **å®‰å…¨æ€§**ï¼šæ•°æ®éšç§ã€è®¿é—®æ§åˆ¶ã€åŠ å¯†ä¼ è¾“

### 7.5 å‘å±•è¶‹åŠ¿

1. **å®æ—¶å¤„ç†**ï¼šæµè®¡ç®—ã€äº‹ä»¶é©±åŠ¨ã€å®æ—¶åˆ†æ
2. **AIé›†æˆ**ï¼šæœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€æ™ºèƒ½åˆ†æ
3. **äº‘åŸç”Ÿ**ï¼šå®¹å™¨åŒ–ã€å¾®æœåŠ¡ã€å¼¹æ€§ä¼¸ç¼©
4. **è¾¹ç¼˜è®¡ç®—**ï¼šæœ¬åœ°å¤„ç†ã€è¾¹ç¼˜åˆ†æã€åˆ†å¸ƒå¼æ™ºèƒ½
5. **æ•°æ®æ¹–**ï¼šç»Ÿä¸€å­˜å‚¨ã€å¤šæ ¼å¼æ”¯æŒã€å…ƒæ•°æ®ç®¡ç†

### 7.6 æœ€ä½³å®è·µ

1. **æ¶æ„è®¾è®¡**ï¼šåˆ†å±‚æ¶æ„ã€æ¨¡å—åŒ–è®¾è®¡ã€æ¾è€¦åˆ
2. **æ•°æ®å¤„ç†**ï¼šå¢é‡å¤„ç†ã€åˆ†åŒºç­–ç•¥ã€ç¼“å­˜ä¼˜åŒ–
3. **æ€§èƒ½è°ƒä¼˜**ï¼šå¹¶è¡Œåº¦è°ƒä¼˜ã€å†…å­˜é…ç½®ã€ç½‘ç»œä¼˜åŒ–
4. **ç›‘æ§å‘Šè­¦**ï¼šæ€§èƒ½ç›‘æ§ã€å¼‚å¸¸æ£€æµ‹ã€å®¹é‡è§„åˆ’
5. **æ•°æ®æ²»ç†**ï¼šæ•°æ®è´¨é‡ã€å…ƒæ•°æ®ç®¡ç†ã€ç”Ÿå‘½å‘¨æœŸç®¡ç†

---

**ç›¸å…³æ–‡æ¡£**ï¼š

- [04-05-02-åˆ†å¸ƒå¼è®¡ç®—](./04-05-02-åˆ†å¸ƒå¼è®¡ç®—.md)
- [04-05-03-å®æ—¶æµå¤„ç†](./04-05-03-å®æ—¶æµå¤„ç†.md)
- [06-ç»„ä»¶ç®—æ³•/06-01-åŸºç¡€ç®—æ³•/06-01-01-æ’åºç®—æ³•](../06-ç»„ä»¶ç®—æ³•/06-01-åŸºç¡€ç®—æ³•/06-01-01-æ’åºç®—æ³•.md)

**è¿”å›ä¸Šçº§**ï¼š[04-è¡Œä¸šé¢†åŸŸ](../README.md)
