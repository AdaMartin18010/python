# 日志管理

## 📋 概述

本文档介绍软件系统日志管理的理论基础、实现方法和最佳实践，为构建可靠的日志系统提供系统化的解决方案。

## 1. 理论基础

### 1.1 日志系统定义

**日志系统** 是一个记录、存储、分析和检索系统运行信息的自动化系统。

#### 1.1.1 形式化定义

设 $\mathcal{L}$ 为日志系统，则：

$$\mathcal{L} = (C, S, A, Q, R)$$

其中：

- $C$: 收集器集合 (Collectors)
- $S$: 存储系统集合 (Storage)
- $A$: 分析器集合 (Analyzers)
- $Q$: 查询系统集合 (Query)
- $R$: 保留策略集合 (Retention)

#### 1.1.2 日志条目模型

设 $E$ 为日志条目集合，每个条目 $e \in E$ 定义为：

$$e = (t, l, m, c, d)$$

其中：

- $t$: 时间戳 (Timestamp)
- $l$: 日志级别 (Level)
- $m$: 消息内容 (Message)
- $c$: 上下文信息 (Context)
- $d$: 元数据 (Metadata)

### 1.2 日志级别理论

#### 1.2.1 级别定义

日志级别表示信息的严重程度：

$$L = \{DEBUG, INFO, WARNING, ERROR, CRITICAL\}$$

#### 1.2.2 级别关系

级别之间存在偏序关系：

$$DEBUG \prec INFO \prec WARNING \prec ERROR \prec CRITICAL$$

#### 1.2.3 过滤函数

设 $f_l$ 为级别过滤函数：

$$f_l(e, l_{min}) = \begin{cases}
true & \text{if } e.l \geq l_{min} \\
false & \text{if } e.l < l_{min}
\end{cases}$$

### 1.3 日志分析理论

#### 1.3.1 模式识别

日志模式可以表示为正则表达式：

$$P = \{p_1, p_2, ..., p_n\}$$

其中每个模式 $p_i$ 是一个正则表达式。

#### 1.3.2 异常检测

基于统计模型的异常检测：

$$P(x_t | x_{t-1}, x_{t-2}, ..., x_{t-n}) < \alpha$$

其中 $\alpha$ 是显著性水平。

## 2. 核心组件实现

### 2.1 日志系统架构

```python
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from enum import Enum
from typing import Dict, List, Optional, Callable, Any, Union
import asyncio
import logging
from datetime import datetime, timedelta
import json
import re
import threading
import queue
import hashlib
from pathlib import Path
import gzip
import pickle

class LogLevel(Enum):
    """日志级别枚举"""
    DEBUG = 10
    INFO = 20
    WARNING = 30
    ERROR = 40
    CRITICAL = 50

    def __lt__(self, other):
        return self.value < other.value

    def __le__(self, other):
        return self.value <= other.value

    def __gt__(self, other):
        return self.value > other.value

    def __ge__(self, other):
        return self.value >= other.value

@dataclass
class LogEntry:
    """日志条目定义"""
    timestamp: datetime
    level: LogLevel
    message: str
    context: Dict[str, Any] = field(default_factory=dict)
    metadata: Dict[str, Any] = field(default_factory=dict)
    source: str = ""
    thread_id: int = 0
    process_id: int = 0

    def to_dict(self) -> Dict[str, Any]:
        """转换为字典"""
        return {
            "timestamp": self.timestamp.isoformat(),
            "level": self.level.name,
            "message": self.message,
            "context": self.context,
            "metadata": self.metadata,
            "source": self.source,
            "thread_id": self.thread_id,
            "process_id": self.process_id
        }

    def to_json(self) -> str:
        """转换为JSON字符串"""
        return json.dumps(self.to_dict(), default=str)

    def __str__(self) -> str:
        return f"[{self.timestamp.strftime('%Y-%m-%d %H:%M:%S')}] {self.level.name}: {self.message}"

class LogCollector(ABC):
    """日志收集器抽象基类"""

    def __init__(self, name: str):
        self.name = name
        self.is_running = False
        self.log_queue = queue.Queue()

    @abstractmethod
    async def collect(self) -> List[LogEntry]:
        """收集日志"""
        pass

    @abstractmethod
    async def start(self):
        """启动收集器"""
        pass

    @abstractmethod
    async def stop(self):
        """停止收集器"""
        pass

    def add_log(self, entry: LogEntry):
        """添加日志条目"""
        self.log_queue.put(entry)

class FileLogCollector(LogCollector):
    """文件日志收集器"""

    def __init__(self, name: str, file_path: str, pattern: str = r".*"):
        super().__init__(name)
        self.file_path = file_path
        self.pattern = re.compile(pattern)
        self.last_position = 0
        self.collection_interval = 1  # 秒

    async def collect(self) -> List[LogEntry]:
        """从文件收集日志"""
        entries = []

        try:
            if not Path(self.file_path).exists():
                return entries

            with open(self.file_path, 'r', encoding='utf-8') as f:
                f.seek(self.last_position)

                for line in f:
                    if self.pattern.match(line):
                        entry = self._parse_line(line)
                        if entry:
                            entries.append(entry)

                self.last_position = f.tell()

        except Exception as e:
            logging.error(f"Error collecting logs from {self.file_path}: {e}")

        return entries

    def _parse_line(self, line: str) -> Optional[LogEntry]:
        """解析日志行"""
        try:
            # 简单的日志解析（可以根据实际格式调整）
            parts = line.strip().split(' ', 2)
            if len(parts) >= 3:
                timestamp_str = f"{parts[0]} {parts[1]}"
                timestamp = datetime.strptime(timestamp_str, "%Y-%m-%d %H:%M:%S")

                # 提取日志级别和消息
                remaining = parts[2]
                level_match = re.search(r'\[(DEBUG|INFO|WARNING|ERROR|CRITICAL)\]', remaining)

                if level_match:
                    level_name = level_match.group(1)
                    level = LogLevel[level_name]
                    message = remaining[level_match.end():].strip()

                    return LogEntry(
                        timestamp=timestamp,
                        level=level,
                        message=message,
                        source=self.name
                    )
        except Exception as e:
            logging.error(f"Error parsing log line: {e}")

        return None

    async def start(self):
        """启动文件收集器"""
        self.is_running = True
        logging.info(f"Started file log collector: {self.name}")

    async def stop(self):
        """停止文件收集器"""
        self.is_running = False
        logging.info(f"Stopped file log collector: {self.name}")

class ApplicationLogCollector(LogCollector):
    """应用日志收集器"""

    def __init__(self, name: str, log_callback: Callable):
        super().__init__(name)
        self.log_callback = log_callback
        self.collection_interval = 5  # 秒

    async def collect(self) -> List[LogEntry]:
        """收集应用日志"""
        try:
            entries = await self.log_callback()
            return entries
        except Exception as e:
            logging.error(f"Error collecting application logs: {e}")
            return []

    async def start(self):
        """启动应用收集器"""
        self.is_running = True
        logging.info(f"Started application log collector: {self.name}")

    async def stop(self):
        """停止应用收集器"""
        self.is_running = False
        logging.info(f"Stopped application log collector: {self.name}")

class LogStorage(ABC):
    """日志存储抽象基类"""

    def __init__(self, name: str):
        self.name = name

    @abstractmethod
    async def store(self, entries: List[LogEntry]) -> bool:
        """存储日志条目"""
        pass

    @abstractmethod
    async def query(self, query: Dict[str, Any]) -> List[LogEntry]:
        """查询日志条目"""
        pass

    @abstractmethod
    async def cleanup(self, retention_days: int) -> int:
        """清理过期日志"""
        pass

class FileLogStorage(LogStorage):
    """文件日志存储"""

    def __init__(self, name: str, base_path: str):
        super().__init__(name)
        self.base_path = Path(base_path)
        self.base_path.mkdir(parents=True, exist_ok=True)
        self.current_file = None
        self.current_date = None
        self.max_file_size = 100 * 1024 * 1024  # 100MB

    def _get_log_file(self, date: datetime) -> Path:
        """获取日志文件路径"""
        date_str = date.strftime("%Y-%m-%d")
        return self.base_path / f"logs_{date_str}.jsonl"

    async def store(self, entries: List[LogEntry]) -> bool:
        """存储日志条目"""
        try:
            for entry in entries:
                date = entry.timestamp.date()

                # 检查是否需要创建新文件
                if self.current_date != date:
                    self.current_file = self._get_log_file(entry.timestamp)
                    self.current_date = date

                # 写入日志条目
                with open(self.current_file, 'a', encoding='utf-8') as f:
                    f.write(entry.to_json() + '\n')

                # 检查文件大小
                if self.current_file.stat().st_size > self.max_file_size:
                    await self._rotate_file()

            return True

        except Exception as e:
            logging.error(f"Error storing logs: {e}")
            return False

    async def _rotate_file(self):
        """轮转日志文件"""
        if self.current_file and self.current_file.exists():
            timestamp = datetime.now().strftime("%H%M%S")
            new_name = f"{self.current_file.stem}_{timestamp}{self.current_file.suffix}"
            new_path = self.current_file.parent / new_name
            self.current_file.rename(new_path)

    async def query(self, query: Dict[str, Any]) -> List[LogEntry]:
        """查询日志条目"""
        entries = []

        try:
            start_time = query.get('start_time')
            end_time = query.get('end_time')
            level = query.get('level')
            pattern = query.get('pattern')

            # 获取需要查询的文件
            if start_time and end_time:
                current_date = start_time.date()
                end_date = end_time.date()

                while current_date <= end_date:
                    log_file = self._get_log_file(datetime.combine(current_date, datetime.min.time()))

                    if log_file.exists():
                        entries.extend(await self._read_file(log_file, query))

                    current_date += timedelta(days=1)
            else:
                # 查询所有文件
                for log_file in self.base_path.glob("logs_*.jsonl"):
                    entries.extend(await self._read_file(log_file, query))

            # 排序
            entries.sort(key=lambda x: x.timestamp)

        except Exception as e:
            logging.error(f"Error querying logs: {e}")

        return entries

    async def _read_file(self, log_file: Path, query: Dict[str, Any]) -> List[LogEntry]:
        """读取日志文件"""
        entries = []

        try:
            start_time = query.get('start_time')
            end_time = query.get('end_time')
            level = query.get('level')
            pattern = query.get('pattern')

            with open(log_file, 'r', encoding='utf-8') as f:
                for line in f:
                    try:
                        data = json.loads(line.strip())
                        entry = self._dict_to_entry(data)

                        # 应用过滤条件
                        if start_time and entry.timestamp < start_time:
                            continue
                        if end_time and entry.timestamp > end_time:
                            continue
                        if level and entry.level < level:
                            continue
                        if pattern and not re.search(pattern, entry.message):
                            continue

                        entries.append(entry)

                    except json.JSONDecodeError:
                        continue

        except Exception as e:
            logging.error(f"Error reading log file {log_file}: {e}")

        return entries

    def _dict_to_entry(self, data: Dict[str, Any]) -> LogEntry:
        """字典转换为日志条目"""
        return LogEntry(
            timestamp=datetime.fromisoformat(data['timestamp']),
            level=LogLevel[data['level']],
            message=data['message'],
            context=data.get('context', {}),
            metadata=data.get('metadata', {}),
            source=data.get('source', ''),
            thread_id=data.get('thread_id', 0),
            process_id=data.get('process_id', 0)
        )

    async def cleanup(self, retention_days: int) -> int:
        """清理过期日志"""
        deleted_count = 0
        cutoff_date = datetime.now() - timedelta(days=retention_days)

        try:
            for log_file in self.base_path.glob("logs_*.jsonl"):
                # 从文件名提取日期
                date_str = log_file.stem.split['_'](1)
                file_date = datetime.strptime(date_str, "%Y-%m-%d")

                if file_date.date() < cutoff_date.date():
                    log_file.unlink()
                    deleted_count += 1

        except Exception as e:
            logging.error(f"Error cleaning up logs: {e}")

        return deleted_count

class LogAnalyzer:
    """日志分析器"""

    def __init__(self):
        self.patterns: Dict[str, re.Pattern] = {}
        self.statistics: Dict[str, Any] = {}

    def add_pattern(self, name: str, pattern: str):
        """添加分析模式"""
        self.patterns[name] = re.compile(pattern)

    def analyze_entries(self, entries: List[LogEntry]) -> Dict[str, Any]:
        """分析日志条目"""
        analysis = {
            "total_entries": len(entries),
            "level_distribution": {},
            "pattern_matches": {},
            "error_summary": {},
            "time_distribution": {},
            "source_distribution": {}
        }

        if not entries:
            return analysis

        # 级别分布
        for entry in entries:
            level_name = entry.level.name
            analysis["level_distribution"][level_name] = analysis["level_distribution"].get(level_name, 0) + 1

        # 模式匹配
        for pattern_name, pattern in self.patterns.items():
            matches = 0
            for entry in entries:
                if pattern.search(entry.message):
                    matches += 1
            analysis["pattern_matches"][pattern_name] = matches

        # 错误摘要
        error_entries = [e for e in entries if e.level >= LogLevel.ERROR]
        analysis["error_summary"] = {
            "total_errors": len(error_entries),
            "error_types": {}
        }

        for entry in error_entries:
            # 简单的错误类型提取
            error_type = entry.message.split[':'](0) if ':' in entry.message else "Unknown"
            analysis["error_summary"]["error_types"][error_type] = analysis["error_summary"]["error_types"].get(error_type, 0) + 1

        # 时间分布
        for entry in entries:
            hour = entry.timestamp.hour
            analysis["time_distribution"][hour] = analysis["time_distribution"].get(hour, 0) + 1

        # 来源分布
        for entry in entries:
            source = entry.source or "unknown"
            analysis["source_distribution"][source] = analysis["source_distribution"].get(source, 0) + 1

        return analysis

    def detect_anomalies(self, entries: List[LogEntry], window_size: int = 100) -> List[Dict[str, Any]]:
        """检测异常"""
        anomalies = []

        if len(entries) < window_size:
            return anomalies

        # 按时间窗口分析
        for i in range(window_size, len(entries)):
            window = entries[i-window_size:i]
            current = entries[i]

            # 计算窗口内的错误率
            error_count = sum(1 for e in window if e.level >= LogLevel.ERROR)
            error_rate = error_count / window_size

            # 如果当前条目是错误且错误率突然增加，可能是异常
            if (current.level >= LogLevel.ERROR and
                error_rate > 0.1):  # 10%的错误率阈值
                anomalies.append({
                    "timestamp": current.timestamp,
                    "message": current.message,
                    "error_rate": error_rate,
                    "type": "error_spike"
                })

        return anomalies

class LogQueryEngine:
    """日志查询引擎"""

    def __init__(self, storage: LogStorage):
        self.storage = storage

    async def search(self, query: str, filters: Dict[str, Any] = None) -> List[LogEntry]:
        """搜索日志"""
        # 解析查询字符串
        parsed_query = self._parse_query(query)

        if filters:
            parsed_query.update(filters)

        return await self.storage.query(parsed_query)

    def _parse_query(self, query: str) -> Dict[str, Any]:
        """解析查询字符串"""
        parsed = {}

        # 简单的查询解析
        if "level:" in query:
            level_match = re.search(r'level:(\w+)', query)
            if level_match:
                level_name = level_match.group(1).upper()
                if level_name in LogLevel.__members__:
                    parsed['level'] = LogLevel[level_name]

        if "source:" in query:
            source_match = re.search(r'source:(\w+)', query)
            if source_match:
                parsed['source'] = source_match.group(1)

        if "pattern:" in query:
            pattern_match = re.search(r'pattern:"([^"]+)"', query)
            if pattern_match:
                parsed['pattern'] = pattern_match.group(1)

        return parsed

    async def get_statistics(self, time_range: Dict[str, datetime] = None) -> Dict[str, Any]:
        """获取统计信息"""
        query = {}
        if time_range:
            query.update(time_range)

        entries = await self.storage.query(query)

        if not entries:
            return {}

        analyzer = LogAnalyzer()
        return analyzer.analyze_entries(entries)

class LogManager:
    """日志管理器"""

    def __init__(self, name: str):
        self.name = name
        self.collectors: List[LogCollector] = []
        self.storage: Optional[LogStorage] = None
        self.analyzer = LogAnalyzer()
        self.query_engine: Optional[LogQueryEngine] = None
        self.is_running = False
        self.collection_interval = 60  # 秒
        self.retention_days = 30

    def add_collector(self, collector: LogCollector):
        """添加收集器"""
        self.collectors.append(collector)

    def set_storage(self, storage: LogStorage):
        """设置存储系统"""
        self.storage = storage
        self.query_engine = LogQueryEngine(storage)

    def add_analysis_pattern(self, name: str, pattern: str):
        """添加分析模式"""
        self.analyzer.add_pattern(name, pattern)

    async def start(self):
        """启动日志管理器"""
        self.is_running = True

        # 启动所有收集器
        for collector in self.collectors:
            await collector.start()

        # 启动收集循环
        asyncio.create_task(self._collection_loop())

        # 启动清理循环
        asyncio.create_task(self._cleanup_loop())

        logging.info(f"Log manager {self.name} started")

    async def stop(self):
        """停止日志管理器"""
        self.is_running = False

        # 停止所有收集器
        for collector in self.collectors:
            await collector.stop()

        logging.info(f"Log manager {self.name} stopped")

    async def _collection_loop(self):
        """收集循环"""
        while self.is_running:
            try:
                all_entries = []

                # 收集所有收集器的日志
                for collector in self.collectors:
                    entries = await collector.collect()
                    all_entries.extend(entries)

                # 存储日志
                if all_entries and self.storage:
                    await self.storage.store(all_entries)

                # 等待下次收集
                await asyncio.sleep(self.collection_interval)

            except Exception as e:
                logging.error(f"Error in collection loop: {e}")
                await asyncio.sleep(10)

    async def _cleanup_loop(self):
        """清理循环"""
        while self.is_running:
            try:
                if self.storage:
                    deleted_count = await self.storage.cleanup(self.retention_days)
                    if deleted_count > 0:
                        logging.info(f"Cleaned up {deleted_count} old log files")

                # 每天清理一次
                await asyncio.sleep(24 * 60 * 60)

            except Exception as e:
                logging.error(f"Error in cleanup loop: {e}")
                await asyncio.sleep(60 * 60)  # 1小时后重试

    async def search_logs(self, query: str, filters: Dict[str, Any] = None) -> List[LogEntry]:
        """搜索日志"""
        if not self.query_engine:
            return []

        return await self.query_engine.search(query, filters)

    async def get_statistics(self, time_range: Dict[str, datetime] = None) -> Dict[str, Any]:
        """获取统计信息"""
        if not self.query_engine:
            return {}

        return await self.query_engine.get_statistics(time_range)

    async def detect_anomalies(self, time_range: Dict[str, datetime] = None) -> List[Dict[str, Any]]:
        """检测异常"""
        if not self.query_engine:
            return []

        query = {}
        if time_range:
            query.update(time_range)

        entries = await self.storage.query(query)
        return self.analyzer.detect_anomalies(entries)
```

### 2.2 日志格式化器

```python
class LogFormatter:
    """日志格式化器"""

    def __init__(self, format_string: str = None):
        self.format_string = format_string or "[{timestamp}] {level}: {message}"

    def format_entry(self, entry: LogEntry) -> str:
        """格式化日志条目"""
        return self.format_string.format(
            timestamp=entry.timestamp.strftime("%Y-%m-%d %H:%M:%S"),
            level=entry.level.name,
            message=entry.message,
            source=entry.source,
            thread_id=entry.thread_id,
            process_id=entry.process_id
        )

    def format_json(self, entry: LogEntry) -> str:
        """JSON格式化"""
        return entry.to_json()

    def format_structured(self, entry: LogEntry) -> str:
        """结构化格式化"""
        return f"{entry.timestamp.isoformat()} | {entry.level.name:8} | {entry.source:15} | {entry.message}"

class LogFilter:
    """日志过滤器"""

    def __init__(self, min_level: LogLevel = LogLevel.DEBUG):
        self.min_level = min_level
        self.exclude_patterns: List[re.Pattern] = []
        self.include_patterns: List[re.Pattern] = []

    def add_exclude_pattern(self, pattern: str):
        """添加排除模式"""
        self.exclude_patterns.append(re.compile(pattern))

    def add_include_pattern(self, pattern: str):
        """添加包含模式"""
        self.include_patterns.append(re.compile(pattern))

    def filter_entry(self, entry: LogEntry) -> bool:
        """过滤日志条目"""
        # 检查级别
        if entry.level < self.min_level:
            return False

        # 检查排除模式
        for pattern in self.exclude_patterns:
            if pattern.search(entry.message):
                return False

        # 检查包含模式
        if self.include_patterns:
            for pattern in self.include_patterns:
                if pattern.search(entry.message):
                    return True
            return False

        return True
```

## 3. 实际应用示例

### 3.1 Web应用日志系统

```python
async def web_app_logging_example():
    """Web应用日志系统示例"""

    # 创建日志管理器
    log_manager = LogManager("WebApp-Logging")

    # 创建文件存储
    storage = FileLogStorage("webapp", "./logs/webapp")
    log_manager.set_storage(storage)

    # 添加文件收集器
    file_collector = FileLogCollector(
        "webapp-logs",
        "./logs/webapp.log",
        r".*\[(DEBUG|INFO|WARNING|ERROR|CRITICAL)\].*"
    )
    log_manager.add_collector(file_collector)

    # 添加应用收集器
    async def collect_app_logs():
        """收集应用日志"""
        entries = []
        now = datetime.now()

        import random

        # 模拟应用日志
        log_levels = [LogLevel.INFO, LogLevel.WARNING, LogLevel.ERROR]
        messages = [
            "User login successful",
            "Database connection established",
            "API request processed",
            "Cache miss occurred",
            "Database query timeout",
            "Authentication failed"
        ]

        # 生成一些日志条目
        for _ in range(random.randint(1, 5)):
            level = random.choice(log_levels)
            message = random.choice(messages)

            entry = LogEntry(
                timestamp=now,
                level=level,
                message=message,
                context={
                    "user_id": random.randint(1, 1000),
                    "session_id": f"session_{random.randint(1000, 9999)}",
                    "ip_address": f"192.168.1.{random.randint(1, 255)}"
                },
                source="webapp"
            )
            entries.append(entry)

        return entries

    app_collector = ApplicationLogCollector("webapp", collect_app_logs)
    log_manager.add_collector(app_collector)

    # 添加分析模式
    log_manager.add_analysis_pattern("error_pattern", r"error|exception|failed|timeout")
    log_manager.add_analysis_pattern("auth_pattern", r"login|logout|authentication|authorization")
    log_manager.add_analysis_pattern("db_pattern", r"database|query|connection|transaction")

    # 启动日志管理器
    await log_manager.start()

    # 运行一段时间
    await asyncio.sleep(300)  # 5分钟

    # 搜索日志
    error_logs = await log_manager.search_logs("level:ERROR")
    print(f"Found {len(error_logs)} error logs")

    # 获取统计信息
    stats = await log_manager.get_statistics()
    print("Log statistics:")
    print(json.dumps(stats, indent=2, default=str))

    # 检测异常
    anomalies = await log_manager.detect_anomalies()
    print(f"Detected {len(anomalies)} anomalies")

    # 停止日志管理器
    await log_manager.stop()

# 运行示例
if __name__ == "__main__":
    asyncio.run(web_app_logging_example())
```

### 3.2 微服务日志系统

```python
async def microservice_logging_example():
    """微服务日志系统示例"""

    # 创建多个服务的日志管理器
    services = ["user-service", "order-service", "payment-service"]
    log_managers = {}

    for service in services:
        # 创建日志管理器
        log_manager = LogManager(f"{service}-Logging")

        # 创建存储
        storage = FileLogStorage(service, f"./logs/{service}")
        log_manager.set_storage(storage)

        # 添加收集器
        async def create_service_collector(service_name: str):
            async def collect_service_logs():
                entries = []
                now = datetime.now()

                import random

                # 服务特定的日志
                service_messages = {
                    "user-service": [
                        "User created successfully",
                        "User authentication failed",
                        "User profile updated",
                        "User deleted"
                    ],
                    "order-service": [
                        "Order created",
                        "Order status updated",
                        "Order cancelled",
                        "Payment processed"
                    ],
                    "payment-service": [
                        "Payment initiated",
                        "Payment completed",
                        "Payment failed",
                        "Refund processed"
                    ]
                }

                messages = service_messages.get(service_name, ["Service log"])

                for _ in range(random.randint(1, 3)):
                    level = random.choice([LogLevel.INFO, LogLevel.WARNING, LogLevel.ERROR])
                    message = random.choice(messages)

                    entry = LogEntry(
                        timestamp=now,
                        level=level,
                        message=message,
                        context={
                            "service": service_name,
                            "request_id": f"req_{random.randint(10000, 99999)}",
                            "user_id": random.randint(1, 1000)
                        },
                        source=service_name
                    )
                    entries.append(entry)

                return entries

            return collect_service_logs

        collector = ApplicationLogCollector(
            service,
            await create_service_collector(service)
        )
        log_manager.add_collector(collector)

        # 添加分析模式
        log_manager.add_analysis_pattern("service_error", r"error|failed|exception")
        log_manager.add_analysis_pattern("service_performance", r"timeout|slow|performance")

        log_managers[service] = log_manager

    # 启动所有日志管理器
    start_tasks = [manager.start() for manager in log_managers.values()]
    await asyncio.gather(*start_tasks)

    # 运行一段时间
    await asyncio.sleep(180)  # 3分钟

    # 分析所有服务的日志
    for service, manager in log_managers.items():
        print(f"\n{service} logs:")

        # 获取统计信息
        stats = await manager.get_statistics()
        print(f"Total entries: {stats.get('total_entries', 0)}")
        print(f"Level distribution: {stats.get('level_distribution', {})}")

        # 搜索错误日志
        error_logs = await manager.search_logs("level:ERROR")
        print(f"Error logs: {len(error_logs)}")

    # 停止所有日志管理器
    stop_tasks = [manager.stop() for manager in log_managers.values()]
    await asyncio.gather(*stop_tasks)
```

## 4. 最佳实践

### 4.1 日志设计原则

1. **结构化日志**: 使用结构化的日志格式
2. **适当级别**: 选择合适的日志级别
3. **上下文信息**: 包含足够的上下文信息
4. **性能考虑**: 避免日志影响应用性能

### 4.2 存储策略

1. **分级存储**: 不同级别的日志使用不同的存储策略
2. **压缩存储**: 对历史日志进行压缩
3. **索引优化**: 为查询创建适当的索引
4. **备份策略**: 重要日志的备份策略

### 4.3 分析策略

1. **实时分析**: 实时分析关键指标
2. **模式识别**: 识别常见的日志模式
3. **异常检测**: 自动检测异常情况
4. **趋势分析**: 分析日志趋势

## 5. 性能优化

### 5.1 异步处理

```python
class AsyncLogManager(LogManager):
    """异步日志管理器"""

    def __init__(self, name: str, max_workers: int = 4):
        super().__init__(name)
        self.max_workers = max_workers
        self.worker_pool = None

    async def start(self):
        """启动异步日志管理器"""
        await super().start()

        # 创建工作池
        self.worker_pool = asyncio.Queue(maxsize=self.max_workers)

        # 启动工作线程
        workers = [asyncio.create_task(self._worker()) for _ in range(self.max_workers)]
        self.workers = workers

    async def _worker(self):
        """工作线程"""
        while self.is_running:
            try:
                entries = await asyncio.wait_for(self.worker_pool.get(), timeout=1.0)

                if self.storage:
                    await self.storage.store(entries)

                self.worker_pool.task_done()

            except asyncio.TimeoutError:
                continue
            except Exception as e:
                logging.error(f"Worker error: {e}")

    async def _collection_loop(self):
        """异步收集循环"""
        while self.is_running:
            try:
                all_entries = []

                # 并行收集所有收集器的日志
                tasks = [collector.collect() for collector in self.collectors]
                results = await asyncio.gather(*tasks, return_exceptions=True)

                for result in results:
                    if isinstance(result, list):
                        all_entries.extend(result)

                # 异步存储
                if all_entries and self.worker_pool:
                    await self.worker_pool.put(all_entries)

                await asyncio.sleep(self.collection_interval)

            except Exception as e:
                logging.error(f"Error in collection loop: {e}")
                await asyncio.sleep(10)
```

### 5.2 缓存优化

```python
class CachedLogStorage(FileLogStorage):
    """带缓存的日志存储"""

    def __init__(self, name: str, base_path: str, cache_size: int = 1000):
        super().__init__(name, base_path)
        self.cache_size = cache_size
        self.write_cache: List[LogEntry] = []
        self.cache_lock = asyncio.Lock()

    async def store(self, entries: List[LogEntry]) -> bool:
        """带缓存的存储"""
        async with self.cache_lock:
            self.write_cache.extend(entries)

            # 检查缓存大小
            if len(self.write_cache) >= self.cache_size:
                # 批量写入
                await self._flush_cache()

        return True

    async def _flush_cache(self):
        """刷新缓存"""
        if not self.write_cache:
            return

        try:
            # 按日期分组
            entries_by_date = {}
            for entry in self.write_cache:
                date = entry.timestamp.date()
                if date not in entries_by_date:
                    entries_by_date[date] = []
                entries_by_date[date].append(entry)

            # 批量写入每个日期的文件
            for date, entries in entries_by_date.items():
                log_file = self._get_log_file(datetime.combine(date, datetime.min.time()))

                with open(log_file, 'a', encoding='utf-8') as f:
                    for entry in entries:
                        f.write(entry.to_json() + '\n')

            # 清空缓存
            self.write_cache.clear()

        except Exception as e:
            logging.error(f"Error flushing cache: {e}")

    async def stop(self):
        """停止时刷新缓存"""
        await self._flush_cache()
```

## 6. 总结

日志管理系统是现代软件工程的重要组成部分，通过系统化的日志收集、存储、分析和查询，为系统监控、问题诊断和性能优化提供了重要支持。本文档提供了完整的理论基础、实现方法和最佳实践，为构建高质量的日志管理系统提供了系统化的解决方案。

### 关键要点

1. **理论基础**: 严格的形式化定义和数学模型
2. **实现方法**: 完整的Python实现和代码示例
3. **最佳实践**: 经过验证的设计原则和策略
4. **性能优化**: 异步处理和缓存机制
5. **可扩展性**: 支持多种数据源和存储后端

### 应用价值

1. **问题诊断**: 快速定位和解决问题
2. **性能监控**: 实时监控系统性能
3. **安全审计**: 记录和审计系统行为
4. **业务分析**: 分析用户行为和业务趋势

---

**相关文档**:
- [监控告警](./07-05-03-监控告警.md)
- [CI/CD流水线](./07-05-02-CI-CD流水线.md)
- [容器化部署](./07-05-01-容器化部署.md)
