# 数据科学基础

## 📋 概述

数据科学是一门跨学科领域，结合统计学、计算机科学和领域知识，通过数据收集、处理、分析和建模来发现洞察和解决复杂问题。

## 🎯 核心概念

### 数据科学定义

**形式化定义**：数据科学是一个四元组 $DS = (D, M, A, I)$，其中：

- $D = \{d_1, d_2, ..., d_n\}$ 是数据集集合
- $M = \{m_1, m_2, ..., m_k\}$ 是模型集合
- $A = \{a_1, a_2, ..., a_l\}$ 是算法集合
- $I = \{i_1, i_2, ..., i_p\}$ 是洞察集合

### 数据科学流程

**CRISP-DM模型**：

1. **业务理解**：$BU = f(Domain, Objectives, Constraints)$
2. **数据理解**：$DU = f(DataSources, DataQuality, DataStructure)$
3. **数据准备**：$DP = f(Cleaning, Transformation, Integration)$
4. **建模**：$M = f(Algorithm, Parameters, Validation)$
5. **评估**：$E = f(Performance, BusinessValue, Deployment)$
6. **部署**：$D = f(Integration, Monitoring, Maintenance)$

## 🔧 Python实现

### 数据科学基础框架

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Any, Optional, Tuple, Union
from dataclasses import dataclass, field
from enum import Enum
import logging
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# 数据类型
class DataType(Enum):
    NUMERICAL = "numerical"
    CATEGORICAL = "categorical"
    TEXT = "text"
    DATETIME = "datetime"
    BOOLEAN = "boolean"

# 数据质量指标
class DataQualityMetric(Enum):
    COMPLETENESS = "completeness"
    ACCURACY = "accuracy"
    CONSISTENCY = "consistency"
    TIMELINESS = "timeliness"
    VALIDITY = "validity"

# 数据科学项目
@dataclass
class DataScienceProject:
    name: str
    description: str
    objectives: List[str]
    data_sources: List[str]
    created_at: datetime
    status: str = "planning"
    metadata: Dict[str, Any] = field(default_factory=dict)

# 数据集
@dataclass
class Dataset:
    name: str
    data: pd.DataFrame
    description: str
    source: str
    created_at: datetime
    schema: Dict[str, DataType] = field(default_factory=dict)
    quality_metrics: Dict[str, float] = field(default_factory=dict)

# 数据科学工作流
class DataScienceWorkflow:
    """数据科学工作流"""
    
    def __init__(self, project: DataScienceProject):
        self.project = project
        self.datasets: Dict[str, Dataset] = {}
        self.models: Dict[str, Any] = {}
        self.results: Dict[str, Any] = {}
        self.logger = logging.getLogger(f"ds_workflow.{project.name}")
        
    def add_dataset(self, dataset: Dataset) -> None:
        """添加数据集"""
        self.datasets[dataset.name] = dataset
        self.logger.info(f"Dataset added: {dataset.name}")
        
    def get_dataset(self, name: str) -> Optional[Dataset]:
        """获取数据集"""
        return self.datasets.get(name)
        
    def list_datasets(self) -> List[str]:
        """列出所有数据集"""
        return list(self.datasets.keys())
        
    def save_model(self, name: str, model: Any) -> None:
        """保存模型"""
        self.models[name] = model
        self.logger.info(f"Model saved: {name}")
        
    def get_model(self, name: str) -> Optional[Any]:
        """获取模型"""
        return self.models.get(name)
        
    def save_result(self, name: str, result: Any) -> None:
        """保存结果"""
        self.results[name] = result
        self.logger.info(f"Result saved: {name}")
        
    def get_result(self, name: str) -> Optional[Any]:
        """获取结果"""
        return self.results.get(name)
```

### 数据探索性分析

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from typing import Dict, List, Any, Optional, Tuple

class ExploratoryDataAnalysis:
    """探索性数据分析"""
    
    def __init__(self, dataset: Dataset):
        self.dataset = dataset
        self.data = dataset.data
        self.logger = logging.getLogger("eda")
        
    def basic_info(self) -> Dict[str, Any]:
        """基本信息"""
        info = {
            "shape": self.data.shape,
            "columns": list(self.data.columns),
            "dtypes": self.data.dtypes.to_dict(),
            "memory_usage": self.data.memory_usage(deep=True).sum(),
            "missing_values": self.data.isnull().sum().to_dict()
        }
        
        return info
        
    def numerical_summary(self) -> pd.DataFrame:
        """数值型数据摘要"""
        numerical_cols = self.data.select_dtypes(include=[np.number]).columns
        if len(numerical_cols) == 0:
            return pd.DataFrame()
            
        summary = self.data[numerical_cols].describe()
        
        # 添加偏度和峰度
        skewness = self.data[numerical_cols].skew()
        kurtosis = self.data[numerical_cols].kurtosis()
        
        summary.loc['skewness'] = skewness
        summary.loc['kurtosis'] = kurtosis
        
        return summary
        
    def categorical_summary(self) -> Dict[str, pd.DataFrame]:
        """分类型数据摘要"""
        categorical_cols = self.data.select_dtypes(include=['object', 'category']).columns
        summary = {}
        
        for col in categorical_cols:
            value_counts = self.data[col].value_counts()
            summary[col] = pd.DataFrame({
                'count': value_counts,
                'percentage': (value_counts / len(self.data) * 100).round(2)
            })
            
        return summary
        
    def correlation_analysis(self) -> pd.DataFrame:
        """相关性分析"""
        numerical_cols = self.data.select_dtypes(include=[np.number]).columns
        if len(numerical_cols) < 2:
            return pd.DataFrame()
            
        correlation_matrix = self.data[numerical_cols].corr()
        return correlation_matrix
        
    def missing_value_analysis(self) -> Dict[str, Any]:
        """缺失值分析"""
        missing_data = self.data.isnull().sum()
        missing_percentage = (missing_data / len(self.data) * 100).round(2)
        
        missing_summary = pd.DataFrame({
            'missing_count': missing_data,
            'missing_percentage': missing_percentage
        }).sort_values('missing_percentage', ascending=False)
        
        # 缺失值模式分析
        missing_patterns = self._analyze_missing_patterns()
        
        return {
            'summary': missing_summary,
            'patterns': missing_patterns
        }
        
    def _analyze_missing_patterns(self) -> Dict[str, Any]:
        """分析缺失值模式"""
        missing_matrix = self.data.isnull()
        
        # 计算缺失值组合
        missing_combinations = missing_matrix.sum(axis=1).value_counts().sort_index()
        
        # 找出完全缺失的行
        completely_missing = missing_matrix.all(axis=1).sum()
        
        return {
            'missing_combinations': missing_combinations,
            'completely_missing_rows': completely_missing
        }
        
    def outlier_analysis(self, method: str = 'iqr') -> Dict[str, List[int]]:
        """异常值分析"""
        numerical_cols = self.data.select_dtypes(include=[np.number]).columns
        outliers = {}
        
        for col in numerical_cols:
            if method == 'iqr':
                outliers[col] = self._detect_outliers_iqr(col)
            elif method == 'zscore':
                outliers[col] = self._detect_outliers_zscore(col)
                
        return outliers
        
    def _detect_outliers_iqr(self, column: str) -> List[int]:
        """使用IQR方法检测异常值"""
        Q1 = self.data[column].quantile(0.25)
        Q3 = self.data[column].quantile(0.75)
        IQR = Q3 - Q1
        
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        
        outliers = self.data[
            (self.data[column] < lower_bound) | 
            (self.data[column] > upper_bound)
        ].index.tolist()
        
        return outliers
        
    def _detect_outliers_zscore(self, column: str, threshold: float = 3.0) -> List[int]:
        """使用Z-score方法检测异常值"""
        z_scores = np.abs(stats.zscore(self.data[column].dropna()))
        outliers = self.data[column].dropna()[z_scores > threshold].index.tolist()
        
        return outliers
        
    def distribution_analysis(self) -> Dict[str, Dict[str, Any]]:
        """分布分析"""
        numerical_cols = self.data.select_dtypes(include=[np.number]).columns
        distributions = {}
        
        for col in numerical_cols:
            data = self.data[col].dropna()
            
            # 正态性检验
            shapiro_stat, shapiro_p = stats.shapiro(data)
            
            # 偏度和峰度
            skewness = stats.skew(data)
            kurtosis = stats.kurtosis(data)
            
            distributions[col] = {
                'shapiro_statistic': shapiro_stat,
                'shapiro_pvalue': shapiro_p,
                'is_normal': shapiro_p > 0.05,
                'skewness': skewness,
                'kurtosis': kurtosis
            }
            
        return distributions
        
    def generate_report(self) -> str:
        """生成分析报告"""
        report = []
        report.append("# 探索性数据分析报告")
        report.append(f"## 数据集: {self.dataset.name}")
        report.append(f"## 分析时间: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # 基本信息
        info = self.basic_info()
        report.append("\n## 基本信息")
        report.append(f"- 数据形状: {info['shape']}")
        report.append(f"- 列数: {len(info['columns'])}")
        report.append(f"- 内存使用: {info['memory_usage'] / 1024 / 1024:.2f} MB")
        
        # 缺失值信息
        missing_summary = self.missing_value_analysis()['summary']
        if not missing_summary.empty:
            report.append("\n## 缺失值分析")
            report.append("| 列名 | 缺失数量 | 缺失比例(%) |")
            report.append("|------|----------|-------------|")
            for col, row in missing_summary.iterrows():
                report.append(f"| {col} | {row['missing_count']} | {row['missing_percentage']} |")
                
        # 数值型数据摘要
        numerical_summary = self.numerical_summary()
        if not numerical_summary.empty:
            report.append("\n## 数值型数据摘要")
            report.append(numerical_summary.to_string())
            
        # 异常值分析
        outliers = self.outlier_analysis()
        if outliers:
            report.append("\n## 异常值分析")
            for col, outlier_indices in outliers.items():
                report.append(f"- {col}: {len(outlier_indices)} 个异常值")
                
        return "\n".join(report)
```

### 数据预处理

```python
from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.feature_selection import SelectKBest, f_classif, f_regression
import pandas as pd
import numpy as np
from typing import Dict, List, Any, Optional, Tuple

class DataPreprocessor:
    """数据预处理器"""
    
    def __init__(self):
        self.scalers: Dict[str, Any] = {}
        self.encoders: Dict[str, Any] = {}
        self.imputers: Dict[str, Any] = {}
        self.feature_selectors: Dict[str, Any] = {}
        self.logger = logging.getLogger("data_preprocessor")
        
    def detect_data_types(self, data: pd.DataFrame) -> Dict[str, DataType]:
        """检测数据类型"""
        data_types = {}
        
        for column in data.columns:
            if data[column].dtype in ['int64', 'float64']:
                data_types[column] = DataType.NUMERICAL
            elif data[column].dtype == 'bool':
                data_types[column] = DataType.BOOLEAN
            elif data[column].dtype == 'datetime64[ns]':
                data_types[column] = DataType.DATETIME
            elif data[column].dtype == 'object':
                # 检查是否为文本
                if data[column].str.len().mean() > 50:
                    data_types[column] = DataType.TEXT
                else:
                    data_types[column] = DataType.CATEGORICAL
            else:
                data_types[column] = DataType.CATEGORICAL
                
        return data_types
        
    def handle_missing_values(self, data: pd.DataFrame, strategy: str = 'auto') -> pd.DataFrame:
        """处理缺失值"""
        data_copy = data.copy()
        
        for column in data_copy.columns:
            if data_copy[column].isnull().sum() > 0:
                if strategy == 'auto':
                    # 自动选择策略
                    if data_copy[column].dtype in ['int64', 'float64']:
                        imputer = SimpleImputer(strategy='mean')
                    else:
                        imputer = SimpleImputer(strategy='most_frequent')
                elif strategy == 'mean':
                    imputer = SimpleImputer(strategy='mean')
                elif strategy == 'median':
                    imputer = SimpleImputer(strategy='median')
                elif strategy == 'most_frequent':
                    imputer = SimpleImputer(strategy='most_frequent')
                elif strategy == 'knn':
                    imputer = KNNImputer(n_neighbors=5)
                else:
                    continue
                    
                # 拟合和转换
                data_copy[column] = imputer.fit_transform(data_copy[[column]])
                self.imputers[column] = imputer
                
        return data_copy
        
    def scale_numerical_features(self, data: pd.DataFrame, method: str = 'standard') -> pd.DataFrame:
        """缩放数值特征"""
        numerical_cols = data.select_dtypes(include=[np.number]).columns
        data_copy = data.copy()
        
        for col in numerical_cols:
            if method == 'standard':
                scaler = StandardScaler()
            elif method == 'minmax':
                scaler = MinMaxScaler()
            else:
                continue
                
            # 拟合和转换
            data_copy[col] = scaler.fit_transform(data_copy[[col]])
            self.scalers[col] = scaler
            
        return data_copy
        
    def encode_categorical_features(self, data: pd.DataFrame, method: str = 'label') -> pd.DataFrame:
        """编码分类特征"""
        categorical_cols = data.select_dtypes(include=['object', 'category']).columns
        data_copy = data.copy()
        
        for col in categorical_cols:
            if method == 'label':
                encoder = LabelEncoder()
                data_copy[col] = encoder.fit_transform(data_copy[col].astype(str))
                self.encoders[col] = encoder
            elif method == 'onehot':
                encoder = OneHotEncoder(sparse=False, drop='first')
                encoded_data = encoder.fit_transform(data_copy[[col]])
                encoded_df = pd.DataFrame(
                    encoded_data,
                    columns=[f"{col}_{cat}" for cat in encoder.categories_[0][1:]]
                )
                data_copy = pd.concat([data_copy.drop(col, axis=1), encoded_df], axis=1)
                self.encoders[col] = encoder
                
        return data_copy
        
    def feature_selection(self, data: pd.DataFrame, target: str, method: str = 'kbest', k: int = 10) -> pd.DataFrame:
        """特征选择"""
        if method == 'kbest':
            # 确定评分函数
            if data[target].dtype in ['int64', 'float64']:
                score_func = f_regression
            else:
                score_func = f_classif
                
            selector = SelectKBest(score_func=score_func, k=k)
            
            # 准备特征和目标变量
            X = data.drop(target, axis=1)
            y = data[target]
            
            # 只选择数值型特征
            numerical_cols = X.select_dtypes(include=[np.number]).columns
            X_numerical = X[numerical_cols]
            
            # 拟合和转换
            X_selected = selector.fit_transform(X_numerical, y)
            
            # 获取选中的特征
            selected_features = numerical_cols[selector.get_support()]
            
            # 创建新的数据框
            selected_data = data[list(selected_features) + [target]]
            self.feature_selectors['kbest'] = selector
            
            return selected_data
            
        return data
        
    def remove_outliers(self, data: pd.DataFrame, method: str = 'iqr') -> pd.DataFrame:
        """移除异常值"""
        numerical_cols = data.select_dtypes(include=[np.number]).columns
        data_copy = data.copy()
        
        for col in numerical_cols:
            if method == 'iqr':
                Q1 = data_copy[col].quantile(0.25)
                Q3 = data_copy[col].quantile(0.75)
                IQR = Q3 - Q1
                
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                
                data_copy = data_copy[
                    (data_copy[col] >= lower_bound) & 
                    (data_copy[col] <= upper_bound)
                ]
                
        return data_copy
        
    def create_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """创建新特征"""
        data_copy = data.copy()
        
        # 数值型特征交互
        numerical_cols = data_copy.select_dtypes(include=[np.number]).columns
        if len(numerical_cols) >= 2:
            for i, col1 in enumerate(numerical_cols):
                for col2 in numerical_cols[i+1:]:
                    # 乘积特征
                    data_copy[f'{col1}_times_{col2}'] = data_copy[col1] * data_copy[col2]
                    # 比率特征
                    if data_copy[col2].min() > 0:
                        data_copy[f'{col1}_div_{col2}'] = data_copy[col1] / data_copy[col2]
                        
        # 多项式特征
        for col in numerical_cols[:3]:  # 限制数量避免维度爆炸
            data_copy[f'{col}_squared'] = data_copy[col] ** 2
            data_copy[f'{col}_cubed'] = data_copy[col] ** 3
            
        return data_copy
        
    def preprocess_pipeline(self, data: pd.DataFrame, target: str = None, 
                          steps: List[str] = None) -> pd.DataFrame:
        """预处理流水线"""
        if steps is None:
            steps = ['missing_values', 'outliers', 'scaling', 'encoding', 'feature_selection']
            
        data_copy = data.copy()
        
        for step in steps:
            if step == 'missing_values':
                data_copy = self.handle_missing_values(data_copy)
            elif step == 'outliers':
                data_copy = self.remove_outliers(data_copy)
            elif step == 'scaling':
                data_copy = self.scale_numerical_features(data_copy)
            elif step == 'encoding':
                data_copy = self.encode_categorical_features(data_copy)
            elif step == 'feature_selection' and target:
                data_copy = self.feature_selection(data_copy, target)
            elif step == 'feature_creation':
                data_copy = self.create_features(data_copy)
                
        return data_copy
```

### 统计建模

```python
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.svm import SVR, SVC
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, classification_report
import pandas as pd
import numpy as np
from typing import Dict, List, Any, Optional, Tuple

class StatisticalModeling:
    """统计建模"""
    
    def __init__(self):
        self.models: Dict[str, Any] = {}
        self.results: Dict[str, Dict[str, Any]] = {}
        self.logger = logging.getLogger("statistical_modeling")
        
    def train_linear_regression(self, X: pd.DataFrame, y: pd.Series, 
                              model_name: str = 'linear_regression') -> Dict[str, Any]:
        """训练线性回归模型"""
        # 分割数据
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        
        # 训练模型
        model = LinearRegression()
        model.fit(X_train, y_train)
        
        # 预测
        y_pred_train = model.predict(X_train)
        y_pred_test = model.predict(X_test)
        
        # 评估
        train_mse = mean_squared_error(y_train, y_pred_train)
        test_mse = mean_squared_error(y_test, y_pred_test)
        train_r2 = r2_score(y_train, y_pred_train)
        test_r2 = r2_score(y_test, y_pred_test)
        
        # 交叉验证
        cv_scores = cross_val_score(model, X, y, cv=5, scoring='r2')
        
        # 保存结果
        results = {
            'model': model,
            'train_mse': train_mse,
            'test_mse': test_mse,
            'train_r2': train_r2,
            'test_r2': test_r2,
            'cv_mean': cv_scores.mean(),
            'cv_std': cv_scores.std(),
            'coefficients': dict(zip(X.columns, model.coef_)),
            'intercept': model.intercept_
        }
        
        self.models[model_name] = model
        self.results[model_name] = results
        
        return results
        
    def train_logistic_regression(self, X: pd.DataFrame, y: pd.Series,
                                model_name: str = 'logistic_regression') -> Dict[str, Any]:
        """训练逻辑回归模型"""
        # 分割数据
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        
        # 训练模型
        model = LogisticRegression(random_state=42)
        model.fit(X_train, y_train)
        
        # 预测
        y_pred_train = model.predict(X_train)
        y_pred_test = model.predict(X_test)
        
        # 评估
        train_accuracy = accuracy_score(y_train, y_pred_train)
        test_accuracy = accuracy_score(y_test, y_pred_test)
        
        # 交叉验证
        cv_scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')
        
        # 分类报告
        classification_rep = classification_report(y_test, y_pred_test, output_dict=True)
        
        # 保存结果
        results = {
            'model': model,
            'train_accuracy': train_accuracy,
            'test_accuracy': test_accuracy,
            'cv_mean': cv_scores.mean(),
            'cv_std': cv_scores.std(),
            'classification_report': classification_rep,
            'coefficients': dict(zip(X.columns, model.coef_[0])),
            'intercept': model.intercept_[0]
        }
        
        self.models[model_name] = model
        self.results[model_name] = results
        
        return results
        
    def train_random_forest(self, X: pd.DataFrame, y: pd.Series, 
                          model_type: str = 'regression',
                          model_name: str = 'random_forest') -> Dict[str, Any]:
        """训练随机森林模型"""
        # 分割数据
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        
        # 选择模型类型
        if model_type == 'regression':
            model = RandomForestRegressor(n_estimators=100, random_state=42)
            scoring = 'r2'
        else:
            model = RandomForestClassifier(n_estimators=100, random_state=42)
            scoring = 'accuracy'
            
        # 训练模型
        model.fit(X_train, y_train)
        
        # 预测
        y_pred_train = model.predict(X_train)
        y_pred_test = model.predict(X_test)
        
        # 评估
        if model_type == 'regression':
            train_score = r2_score(y_train, y_pred_train)
            test_score = r2_score(y_test, y_pred_test)
        else:
            train_score = accuracy_score(y_train, y_pred_train)
            test_score = accuracy_score(y_test, y_pred_test)
            
        # 交叉验证
        cv_scores = cross_val_score(model, X, y, cv=5, scoring=scoring)
        
        # 特征重要性
        feature_importance = dict(zip(X.columns, model.feature_importances_))
        
        # 保存结果
        results = {
            'model': model,
            'train_score': train_score,
            'test_score': test_score,
            'cv_mean': cv_scores.mean(),
            'cv_std': cv_scores.std(),
            'feature_importance': feature_importance
        }
        
        if model_type == 'classification':
            results['classification_report'] = classification_report(y_test, y_pred_test, output_dict=True)
            
        self.models[model_name] = model
        self.results[model_name] = results
        
        return results
        
    def hyperparameter_tuning(self, X: pd.DataFrame, y: pd.Series, 
                            model_type: str = 'regression',
                            model_name: str = 'tuned_model') -> Dict[str, Any]:
        """超参数调优"""
        # 定义参数网格
        if model_type == 'regression':
            model = RandomForestRegressor(random_state=42)
            param_grid = {
                'n_estimators': [50, 100, 200],
                'max_depth': [10, 20, None],
                'min_samples_split': [2, 5, 10],
                'min_samples_leaf': [1, 2, 4]
            }
            scoring = 'r2'
        else:
            model = RandomForestClassifier(random_state=42)
            param_grid = {
                'n_estimators': [50, 100, 200],
                'max_depth': [10, 20, None],
                'min_samples_split': [2, 5, 10],
                'min_samples_leaf': [1, 2, 4]
            }
            scoring = 'accuracy'
            
        # 网格搜索
        grid_search = GridSearchCV(
            estimator=model,
            param_grid=param_grid,
            cv=5,
            scoring=scoring,
            n_jobs=-1,
            verbose=1
        )
        
        grid_search.fit(X, y)
        
        # 获取最佳模型
        best_model = grid_search.best_estimator_
        best_params = grid_search.best_params_
        best_score = grid_search.best_score_
        
        # 保存结果
        results = {
            'model': best_model,
            'best_params': best_params,
            'best_cv_score': best_score,
            'cv_results': grid_search.cv_results_
        }
        
        self.models[model_name] = best_model
        self.results[model_name] = results
        
        return results
        
    def model_comparison(self, X: pd.DataFrame, y: pd.Series) -> pd.DataFrame:
        """模型比较"""
        models = {
            'Linear Regression': LinearRegression(),
            'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),
            'SVR': SVR()
        }
        
        results = []
        
        for name, model in models.items():
            # 交叉验证
            cv_scores = cross_val_score(model, X, y, cv=5, scoring='r2')
            
            results.append({
                'Model': name,
                'CV Mean': cv_scores.mean(),
                'CV Std': cv_scores.std(),
                'CV Min': cv_scores.min(),
                'CV Max': cv_scores.max()
            })
            
        return pd.DataFrame(results)
        
    def generate_model_report(self, model_name: str) -> str:
        """生成模型报告"""
        if model_name not in self.results:
            return f"Model {model_name} not found"
            
        result = self.results[model_name]
        report = []
        
        report.append(f"# 模型报告: {model_name}")
        report.append(f"## 模型类型: {type(result['model']).__name__}")
        
        if 'train_score' in result:
            report.append(f"## 训练分数: {result['train_score']:.4f}")
            report.append(f"## 测试分数: {result['test_score']:.4f}")
            
        if 'cv_mean' in result:
            report.append(f"## 交叉验证平均分数: {result['cv_mean']:.4f}")
            report.append(f"## 交叉验证标准差: {result['cv_std']:.4f}")
            
        if 'best_params' in result:
            report.append("## 最佳参数:")
            for param, value in result['best_params'].items():
                report.append(f"- {param}: {value}")
                
        if 'feature_importance' in result:
            report.append("## 特征重要性:")
            sorted_features = sorted(result['feature_importance'].items(), 
                                   key=lambda x: x[1], reverse=True)
            for feature, importance in sorted_features[:10]:
                report.append(f"- {feature}: {importance:.4f}")
                
        return "\n".join(report)
```

## 📊 性能分析

### 模型评估指标

**回归模型**：

- **均方误差**：$MSE = \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2$
- **决定系数**：$R^2 = 1 - \frac{SS_{res}}{SS_{tot}}$

**分类模型**：

- **准确率**：$Accuracy = \frac{TP + TN}{TP + TN + FP + FN}$
- **精确率**：$Precision = \frac{TP}{TP + FP}$
- **召回率**：$Recall = \frac{TP}{TP + FN}$

### 数据质量指标

**完整性**：$Completeness = \frac{NonMissingValues}{TotalValues}$

**一致性**：$Consistency = \frac{ConsistentRecords}{TotalRecords}$

## 🛡️ 最佳实践

### 1. 数据科学流程

- **问题定义**：明确定义业务问题和目标
- **数据收集**：收集相关和高质量的数据
- **数据探索**：深入理解数据特征和分布
- **特征工程**：创建有意义的特征
- **模型选择**：选择合适的算法和模型
- **模型评估**：使用多种指标评估模型
- **模型部署**：将模型集成到生产环境

### 2. 数据质量保证

```python
class DataQualityChecker:
    """数据质量检查器"""
    
    def __init__(self):
        self.quality_metrics = {}
        
    def check_completeness(self, data: pd.DataFrame) -> float:
        """检查完整性"""
        total_values = data.size
        missing_values = data.isnull().sum().sum()
        completeness = (total_values - missing_values) / total_values
        return completeness
        
    def check_consistency(self, data: pd.DataFrame) -> float:
        """检查一致性"""
        # 简化实现，实际应该根据业务规则检查
        return 1.0
        
    def check_accuracy(self, data: pd.DataFrame, reference_data: pd.DataFrame) -> float:
        """检查准确性"""
        # 与参考数据比较
        common_columns = set(data.columns) & set(reference_data.columns)
        if not common_columns:
            return 0.0
            
        accuracy_scores = []
        for col in common_columns:
            if data[col].dtype == reference_data[col].dtype:
                # 简单的准确性检查
                accuracy = (data[col] == reference_data[col]).mean()
                accuracy_scores.append(accuracy)
                
        return np.mean(accuracy_scores) if accuracy_scores else 0.0
        
    def generate_quality_report(self, data: pd.DataFrame) -> Dict[str, float]:
        """生成质量报告"""
        report = {
            'completeness': self.check_completeness(data),
            'consistency': self.check_consistency(data)
        }
        
        return report
```

### 3. 模型监控

```python
class ModelMonitor:
    """模型监控器"""
    
    def __init__(self):
        self.performance_history = []
        
    def record_performance(self, model_name: str, metrics: Dict[str, float]) -> None:
        """记录性能指标"""
        record = {
            'timestamp': datetime.now(),
            'model_name': model_name,
            **metrics
        }
        self.performance_history.append(record)
        
    def detect_drift(self, current_metrics: Dict[str, float], 
                    window_size: int = 10) -> Dict[str, bool]:
        """检测模型漂移"""
        if len(self.performance_history) < window_size:
            return {}
            
        recent_history = self.performance_history[-window_size:]
        drift_detected = {}
        
        for metric, current_value in current_metrics.items():
            historical_values = [record[metric] for record in recent_history 
                               if metric in record]
            
            if historical_values:
                mean_value = np.mean(historical_values)
                std_value = np.std(historical_values)
                
                # 检测异常值
                z_score = abs(current_value - mean_value) / std_value
                drift_detected[metric] = z_score > 2.0
                
        return drift_detected
```

## 🔗 相关链接

- [04-行业领域/04-01-Web开发/04-01-01-Web架构基础.md](../04-01-Web开发/04-01-01-Web架构基础.md) - Web架构基础
- [04-行业领域/04-02-IoT开发/04-02-01-IoT基础.md](../04-02-IoT开发/04-02-01-IoT基础.md) - IoT开发基础
- [02-理论基础/02-01-算法理论/02-01-01-算法基础.md](../../02-理论基础/02-01-算法理论/02-01-01-算法基础.md) - 算法理论基础

---

*本文档提供了数据科学的完整理论基础和Python实现，包括数据探索、预处理、建模等核心组件。*
