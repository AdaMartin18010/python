# Python è¯æ³•åˆ†æ

**TokenåŒ–ï¼šä»æºä»£ç åˆ°è¯æ³•å•å…ƒ**

---

## ğŸ“‹ ç›®å½•

- [è¯æ³•åˆ†ææ¦‚è¿°](#è¯æ³•åˆ†ææ¦‚è¿°)
- [Tokenç±»å‹](#Tokenç±»å‹)
- [æ ‡è¯†ç¬¦ä¸å…³é”®å­—](#æ ‡è¯†ç¬¦ä¸å…³é”®å­—)
- [å­—é¢é‡](#å­—é¢é‡)
- [è¿ç®—ç¬¦ä¸åˆ†éš”ç¬¦](#è¿ç®—ç¬¦ä¸åˆ†éš”ç¬¦)
- [ç¼–ç ä¸æ³¨é‡Š](#ç¼–ç ä¸æ³¨é‡Š)

---

## è¯æ³•åˆ†ææ¦‚è¿°

### ä»€ä¹ˆæ˜¯è¯æ³•åˆ†æ

```python
"""
è¯æ³•åˆ†æ: æºä»£ç  â†’ Tokenåºåˆ—
"""

# æºä»£ç 
source = "x = 42 + y"

# è¯æ³•åˆ†æåçš„Tokenåºåˆ—:
"""
NAME        'x'
OP          '='
NUMBER      '42'
OP          '+'
NAME        'y'
NEWLINE
"""

# Pythonçš„tokenizeæ¨¡å—
import tokenize
import io

code = "x = 42 + y"
tokens = tokenize.generate_tokens(io.StringIO(code).readline)

for token in tokens:
    print(f"{tokenize.tok_name[token.type]:10} {token.string!r:10} "
          f"{token.start} {token.end}")

"""
è¾“å‡º:
NAME       'x'        (1, 0) (1, 1)
OP         '='        (1, 2) (1, 3)
NUMBER     '42'       (1, 4) (1, 6)
OP         '+'        (1, 7) (1, 8)
NAME       'y'        (1, 9) (1, 10)
NEWLINE    ''         (1, 10) (1, 10)
ENDMARKER  ''         (2, 0) (2, 0)
"""
```

### è¯æ³•åˆ†ææµç¨‹

```python
"""
è¯æ³•åˆ†æçš„ä¸‰ä¸ªé˜¶æ®µ
"""

# é˜¶æ®µ1: ç‰©ç†è¡Œ â†’ é€»è¾‘è¡Œ
physical_lines = """
x = 1 + \\
    2 + \\
    3
"""

# è½¬æ¢ä¸ºé€»è¾‘è¡Œ: "x = 1 + 2 + 3"

# é˜¶æ®µ2: é€»è¾‘è¡Œ â†’ Tokenåºåˆ—
# è¯†åˆ«æ ‡è¯†ç¬¦ã€å…³é”®å­—ã€å­—é¢é‡ã€è¿ç®—ç¬¦

# é˜¶æ®µ3: ç¼©è¿›å¤„ç†
indented_code = """
if True:
    x = 1
    y = 2
"""

# ç”ŸæˆINDENTå’ŒDEDENT token
"""
NAME    'if'
NAME    'True'
OP      ':'
NEWLINE
INDENT          # ç¼©è¿›å¼€å§‹
NAME    'x'
...
DEDENT          # ç¼©è¿›ç»“æŸ
"""
```

---

## Tokenç±»å‹

### ä¸»è¦Tokenç±»å‹

```python
"""
Pythonçš„Tokenç±»å‹
"""
import token

# æŸ¥çœ‹æ‰€æœ‰Tokenç±»å‹
print("Token types:")
for name in dir(token):
    if name.isupper():
        value = getattr(token, name)
        if isinstance(value, int):
            print(f"{value:3} {name}")

"""
ä¸»è¦ç±»å‹:
0   ENDMARKER    # æ–‡ä»¶ç»“æŸ
1   NAME         # æ ‡è¯†ç¬¦
2   NUMBER       # æ•°å­—å­—é¢é‡
3   STRING       # å­—ç¬¦ä¸²å­—é¢é‡
4   NEWLINE      # æ¢è¡Œ
5   INDENT       # ç¼©è¿›
6   DEDENT       # åç¼©è¿›
7   LPAR         # (
8   RPAR         # )
9   LSQB         # [
10  RSQB         # ]
...
54  OP           # è¿ç®—ç¬¦
62  ENCODING     # ç¼–ç å£°æ˜
63  NL           # éç»ˆæ­¢æ¢è¡Œ
"""
```

### Tokenç»“æ„

```python
"""
Tokenå¯¹è±¡çš„å±æ€§
"""
import tokenize
import io

code = "x = 42"
tokens = list(tokenize.generate_tokens(io.StringIO(code).readline))

for tok in tokens:
    print(f"""
Token:
  type:   {tokenize.tok_name[tok.type]}
  string: {tok.string!r}
  start:  {tok.start}  # (è¡Œå·, åˆ—å·)
  end:    {tok.end}
  line:   {tok.line!r}
""")

# TokenInfoå‘½åå…ƒç»„
"""
TokenInfo(
    type=1,              # Tokenç±»å‹
    string='x',          # Tokenå­—ç¬¦ä¸²
    start=(1, 0),        # èµ·å§‹ä½ç½®
    end=(1, 1),          # ç»“æŸä½ç½®
    line='x = 42'        # æ‰€åœ¨è¡Œ
)
"""
```

---

## æ ‡è¯†ç¬¦ä¸å…³é”®å­—

### æ ‡è¯†ç¬¦è§„åˆ™

```python
"""
æ ‡è¯†ç¬¦å‘½åè§„åˆ™
"""

# âœ… åˆæ³•æ ‡è¯†ç¬¦
valid_names = [
    "x",
    "my_var",
    "_private",
    "__dunder__",
    "Class",
    "function123",
    "Î±",          # Unicodeå­—ç¬¦
    "å˜é‡",       # ä¸­æ–‡
]

# âŒ éæ³•æ ‡è¯†ç¬¦
"""
123abc       # ä¸èƒ½ä»¥æ•°å­—å¼€å¤´
my-var       # ä¸èƒ½åŒ…å«è¿å­—ç¬¦
for          # ä¸èƒ½æ˜¯å…³é”®å­—
my var       # ä¸èƒ½åŒ…å«ç©ºæ ¼
"""

# æ ‡è¯†ç¬¦çš„æ­£åˆ™æ¨¡å¼
import re

identifier_pattern = r'[a-zA-Z_]\w*'
print(re.match(identifier_pattern, "my_var"))  # Match

# Unicodeæ ‡è¯†ç¬¦ (Python 3)
identifier_unicode = r'[\w\u0080-\uFFFF]+'
```

### å…³é”®å­—

```python
"""
Pythonå…³é”®å­—
"""
import keyword

# æ‰€æœ‰å…³é”®å­—
print(keyword.kwlist)
"""
['False', 'None', 'True', 'and', 'as', 'assert', 
 'async', 'await', 'break', 'class', 'continue', 
 'def', 'del', 'elif', 'else', 'except', 'finally', 
 'for', 'from', 'global', 'if', 'import', 'in', 
 'is', 'lambda', 'nonlocal', 'not', 'or', 'pass', 
 'raise', 'return', 'try', 'while', 'with', 'yield']
"""

# æ£€æŸ¥æ˜¯å¦ä¸ºå…³é”®å­—
print(keyword.iskeyword("for"))    # True
print(keyword.iskeyword("print"))  # False

# è½¯å…³é”®å­— (Python 3.10+)
print(keyword.softkwlist)
"""
['_', 'case', 'match', 'type']
"""

# è½¯å…³é”®å­—åªåœ¨ç‰¹å®šä¸Šä¸‹æ–‡ä¸­æ˜¯å…³é”®å­—
match = 10  # âœ… ä½œä¸ºå˜é‡åOK
```

### ä¿ç•™æ ‡è¯†ç¬¦

```python
"""
ç‰¹æ®Šæ ‡è¯†ç¬¦
"""

# 1. å•ä¸‹åˆ’çº¿: _ (å¸¸ç”¨äºä¸´æ—¶å˜é‡)
for _ in range(5):
    print("Hello")

# 2. å•å‰å¯¼ä¸‹åˆ’çº¿: _name (å†…éƒ¨ä½¿ç”¨)
class MyClass:
    def _internal_method(self):  # çº¦å®šä¸ºå†…éƒ¨æ–¹æ³•
        pass

# 3. å•åç½®ä¸‹åˆ’çº¿: name_ (é¿å…å…³é”®å­—å†²çª)
class_ = type("MyClass", (), {})

# 4. åŒå‰å¯¼ä¸‹åˆ’çº¿: __name (åç§°æ”¹å†™)
class MyClass:
    def __private(self):  # æ”¹å†™ä¸º _MyClass__private
        pass

# 5. åŒä¸‹åˆ’çº¿åŒ…å›´: __name__ (é­”æ³•æ–¹æ³•/å±æ€§)
class MyClass:
    def __init__(self):  # ç‰¹æ®Šæ–¹æ³•
        pass
```

---

## å­—é¢é‡

### æ•°å­—å­—é¢é‡

```python
"""
æ•°å­—å­—é¢é‡çš„å„ç§å½¢å¼
"""

# æ•´æ•°
decimal = 42          # åè¿›åˆ¶
binary = 0b101010     # äºŒè¿›åˆ¶
octal = 0o52          # å…«è¿›åˆ¶
hexadecimal = 0x2A    # åå…­è¿›åˆ¶

# å¤§æ•´æ•°ä½¿ç”¨ä¸‹åˆ’çº¿åˆ†éš”
million = 1_000_000
hex_value = 0xFF_FF_FF

# æµ®ç‚¹æ•°
float1 = 3.14
float2 = .5           # 0.5
float3 = 10.          # 10.0
scientific = 1.5e10   # ç§‘å­¦è®¡æ•°æ³•
small = 3e-5

# å¤æ•°
complex1 = 3 + 4j
complex2 = complex(3, 4)
imaginary = 2j

# æŸ¥çœ‹Token
import tokenize, io

code = "0b101010, 1_000_000, 3.14, 2j"
tokens = tokenize.generate_tokens(io.StringIO(code).readline)

for tok in tokens:
    if tok.type == tokenize.NUMBER:
        print(f"NUMBER: {tok.string!r}")

"""
NUMBER: '0b101010'
NUMBER: '1_000_000'
NUMBER: '3.14'
NUMBER: '2j'
"""
```

### å­—ç¬¦ä¸²å­—é¢é‡

```python
"""
å­—ç¬¦ä¸²å­—é¢é‡çš„å¤šç§å½¢å¼
"""

# 1. å•å¼•å·å’ŒåŒå¼•å·
s1 = 'Hello'
s2 = "World"
s3 = 'He said "Hi"'    # åŒ…å«åŒå¼•å·
s4 = "It's fine"       # åŒ…å«å•å¼•å·

# 2. ä¸‰å¼•å·å­—ç¬¦ä¸² (å¤šè¡Œ)
multiline = """
This is a
multiline string
"""

# 3. åŸå§‹å­—ç¬¦ä¸² (ä¸è½¬ä¹‰)
raw = r"C:\Users\name"  # ä¸è½¬ä¹‰\n, \tç­‰
regex = r"\d+\.\d+"

# 4. æ ¼å¼åŒ–å­—ç¬¦ä¸² (f-string)
name = "Alice"
age = 30
f_string = f"Name: {name}, Age: {age}"

# 5. å­—èŠ‚å­—ç¬¦ä¸²
bytes_str = b"Hello"
raw_bytes = rb"C:\path"

# 6. å­—ç¬¦ä¸²å‰ç¼€ç»„åˆ
# r, u, b, f å¯ä»¥ç»„åˆ
fr_string = fr"Raw f-string: {value}"
rb_string = rb"Raw bytes"

# Tokenç¤ºä¾‹
code = '''r"\\n", f"{x}", b"bytes"'''
tokens = tokenize.generate_tokens(io.StringIO(code).readline)

for tok in tokens:
    if tok.type == tokenize.STRING:
        print(f"STRING: {tok.string!r}")

"""
STRING: 'r"\\\\n"'
STRING: 'f"{x}"'
STRING: 'b"bytes"'
"""
```

### è½¬ä¹‰åºåˆ—

```python
"""
å­—ç¬¦ä¸²è½¬ä¹‰åºåˆ—
"""

# å¸¸ç”¨è½¬ä¹‰åºåˆ—
escapes = {
    r'\n': 'æ¢è¡Œ (LF)',
    r'\r': 'å›è½¦ (CR)',
    r'\t': 'åˆ¶è¡¨ç¬¦',
    r'\\': 'åæ–œæ ',
    r'\'': 'å•å¼•å·',
    r'\"': 'åŒå¼•å·',
    r'\a': 'å“é“ƒ',
    r'\b': 'é€€æ ¼',
    r'\f': 'æ¢é¡µ',
    r'\v': 'å‚ç›´åˆ¶è¡¨ç¬¦',
}

# Unicodeè½¬ä¹‰
unicode_str = "\u4e2d\u6587"  # 'ä¸­æ–‡'
unicode32 = "\U0001F600"      # 'ğŸ˜€'
named = "\N{GREEK CAPITAL LETTER DELTA}"  # 'Î”'

# å…«è¿›åˆ¶å’Œåå…­è¿›åˆ¶
octal = "\101"     # 'A' (8è¿›åˆ¶101)
hex_char = "\x41"  # 'A' (16è¿›åˆ¶41)

# åŸå§‹å­—ç¬¦ä¸²ä¸è½¬ä¹‰
raw = r"\n\t"  # å­—é¢ä¸Šçš„ '\n\t'
```

---

## è¿ç®—ç¬¦ä¸åˆ†éš”ç¬¦

### è¿ç®—ç¬¦

```python
"""
Pythonè¿ç®—ç¬¦
"""

# ç®—æœ¯è¿ç®—ç¬¦
operators_arithmetic = [
    '+',   # åŠ 
    '-',   # å‡
    '*',   # ä¹˜
    '/',   # é™¤
    '//',  # æ•´é™¤
    '%',   # å–æ¨¡
    '**',  # å¹‚
]

# ä½è¿ç®—ç¬¦
operators_bitwise = [
    '&',   # æŒ‰ä½ä¸
    '|',   # æŒ‰ä½æˆ–
    '^',   # æŒ‰ä½å¼‚æˆ–
    '~',   # æŒ‰ä½å–å
    '<<',  # å·¦ç§»
    '>>',  # å³ç§»
]

# æ¯”è¾ƒè¿ç®—ç¬¦
operators_comparison = [
    '==',  # ç­‰äº
    '!=',  # ä¸ç­‰äº
    '<',   # å°äº
    '>',   # å¤§äº
    '<=',  # å°äºç­‰äº
    '>=',  # å¤§äºç­‰äº
]

# èµ‹å€¼è¿ç®—ç¬¦
operators_assignment = [
    '=',   # èµ‹å€¼
    '+=',  # åŠ èµ‹å€¼
    '-=',  # å‡èµ‹å€¼
    '*=',  # ä¹˜èµ‹å€¼
    '/=',  # é™¤èµ‹å€¼
    '//=', # æ•´é™¤èµ‹å€¼
    '%=',  # å–æ¨¡èµ‹å€¼
    '**=', # å¹‚èµ‹å€¼
    '&=',  # æŒ‰ä½ä¸èµ‹å€¼
    '|=',  # æŒ‰ä½æˆ–èµ‹å€¼
    '^=',  # æŒ‰ä½å¼‚æˆ–èµ‹å€¼
    '>>=', # å³ç§»èµ‹å€¼
    '<<=', # å·¦ç§»èµ‹å€¼
    ':=',  # æµ·è±¡è¿ç®—ç¬¦ (Python 3.8+)
]

# é€»è¾‘è¿ç®—ç¬¦ (å…³é”®å­—)
# and, or, not

# æˆå‘˜è¿ç®—ç¬¦ (å…³é”®å­—)
# in, not in

# èº«ä»½è¿ç®—ç¬¦ (å…³é”®å­—)
# is, is not
```

### åˆ†éš”ç¬¦

```python
"""
Pythonåˆ†éš”ç¬¦
"""

# æ‹¬å·ç±»
delimiters_brackets = [
    '(',   # å·¦åœ†æ‹¬å·
    ')',   # å³åœ†æ‹¬å·
    '[',   # å·¦æ–¹æ‹¬å·
    ']',   # å³æ–¹æ‹¬å·
    '{',   # å·¦èŠ±æ‹¬å·
    '}',   # å³èŠ±æ‹¬å·
]

# æ ‡ç‚¹ç±»
delimiters_punctuation = [
    ',',   # é€—å·
    ':',   # å†’å·
    ';',   # åˆ†å·
    '.',   # ç‚¹
    '...',  # çœç•¥å· (Ellipsis)
    '@',   # è£…é¥°å™¨
    '->',  # å‡½æ•°æ³¨è§£
]

# ç‰¹æ®Š
delimiters_special = [
    '=',   # èµ‹å€¼
    '\n',  # æ¢è¡Œ
]

# Tokenç±»å‹
code = "(a, b): c[0] @ decorator"
tokens = tokenize.generate_tokens(io.StringIO(code).readline)

for tok in tokens:
    if tok.type == tokenize.OP:
        print(f"OP: {tok.string!r}")

"""
OP: '('
OP: ','
OP: ')'
OP: ':'
OP: '['
OP: ']'
OP: '@'
"""
```

---

## ç¼–ç ä¸æ³¨é‡Š

### æºæ–‡ä»¶ç¼–ç 

```python
"""
æºæ–‡ä»¶ç¼–ç å£°æ˜
"""

# é»˜è®¤ç¼–ç : UTF-8 (Python 3)

# æ˜¾å¼å£°æ˜ç¼–ç  (å¿…é¡»åœ¨ç¬¬ä¸€æˆ–ç¬¬äºŒè¡Œ)
# -*- coding: utf-8 -*-

# æˆ–
# coding: utf-8

# æˆ–
# coding=utf-8

# æ£€æµ‹ç¼–ç 
import tokenize

with open(__file__, 'rb') as f:
    encoding = tokenize.detect_encoding(f.readline)[0]
    print(f"File encoding: {encoding}")
```

### æ³¨é‡Š

```python
"""
æ³¨é‡Šçš„Tokenå¤„ç†
"""

# 1. å•è¡Œæ³¨é‡Š
x = 42  # è¿™æ˜¯æ³¨é‡Š

# 2. å¤šè¡Œæ³¨é‡Š (æ–‡æ¡£å­—ç¬¦ä¸²)
"""
è¿™æ˜¯å¤šè¡Œæ³¨é‡Š
å®é™…ä¸Šæ˜¯å­—ç¬¦ä¸²å­—é¢é‡
"""

# 3. æ³¨é‡Šä¸ä¼šç”ŸæˆToken
import tokenize, io

code = """
x = 42  # æ³¨é‡Š
y = 10
"""

tokens = list(tokenize.generate_tokens(io.StringIO(code).readline))

# é»˜è®¤æƒ…å†µä¸‹æ³¨é‡Šè¢«è·³è¿‡
print("Tokens without comments:")
for tok in tokens:
    if tok.type != tokenize.ENCODING:
        print(f"{tokenize.tok_name[tok.type]:10} {tok.string!r}")

# ä½¿ç”¨generate_tokenså¯ä»¥è·å–æ³¨é‡Š
print("\nAll tokens including comments:")
tokens = tokenize.tokenize(io.BytesIO(code.encode()).readline)
for tok in tokens:
    print(f"{tokenize.tok_name[tok.type]:10} {tok.string!r}")
```

### è¡Œè¿æ¥

```python
"""
ç‰©ç†è¡Œè¿æ¥ä¸ºé€»è¾‘è¡Œ
"""

# 1. æ˜¾å¼è¡Œè¿æ¥ (åæ–œæ )
total = 1 + 2 + \
        3 + 4 + \
        5

# 2. éšå¼è¡Œè¿æ¥ (æ‹¬å·å†…)
total = (1 + 2 +
         3 + 4 +
         5)

items = [
    'a',
    'b',
    'c',
]

# 3. å­—ç¬¦ä¸²å­—é¢é‡æ‹¼æ¥
message = "Hello " \
          "World"  # è‡ªåŠ¨æ‹¼æ¥

# Tokenç¤ºä¾‹
code = """
x = (1 +
     2 +
     3)
"""

tokens = tokenize.generate_tokens(io.StringIO(code).readline)

for tok in tokens:
    if tok.type in (tokenize.NAME, tokenize.NUMBER, tokenize.OP):
        print(f"{tokenize.tok_name[tok.type]:10} {tok.string!r:5} "
              f"at line {tok.start[0]}")

"""
è¾“å‡º:
NAME       'x'   at line 2
OP         '='   at line 2
OP         '('   at line 2
NUMBER     '1'   at line 2
OP         '+'   at line 2
NUMBER     '2'   at line 3  # æ³¨æ„è¡Œå·
OP         '+'   at line 3
NUMBER     '3'   at line 4
OP         ')'   at line 4
"""
```

---

## ğŸ“š æ ¸å¿ƒè¦ç‚¹

### è¯æ³•åˆ†æ

- âœ… **ä½œç”¨**: å°†æºä»£ç è½¬æ¢ä¸ºTokenåºåˆ—
- âœ… **æµç¨‹**: ç‰©ç†è¡Œ â†’ é€»è¾‘è¡Œ â†’ Tokenåºåˆ—
- âœ… **ç¼©è¿›**: é€šè¿‡INDENT/DEDENT Tokenè¡¨ç¤º

### Tokenç±»å‹

- âœ… **NAME**: æ ‡è¯†ç¬¦å’Œå…³é”®å­—
- âœ… **NUMBER**: æ•°å­—å­—é¢é‡
- âœ… **STRING**: å­—ç¬¦ä¸²å­—é¢é‡
- âœ… **OP**: è¿ç®—ç¬¦å’Œåˆ†éš”ç¬¦
- âœ… **INDENT/DEDENT**: ç¼©è¿›æ§åˆ¶

### æ ‡è¯†ç¬¦

- âœ… **è§„åˆ™**: å­—æ¯/ä¸‹åˆ’çº¿å¼€å¤´ï¼Œå¯å«æ•°å­—
- âœ… **Unicode**: æ”¯æŒUnicodeå­—ç¬¦
- âœ… **å…³é”®å­—**: ä¸èƒ½ä½œä¸ºæ ‡è¯†ç¬¦
- âœ… **çº¦å®š**: _private, __dunder__

### å­—é¢é‡

- âœ… **æ•°å­—**: æ•´æ•°ã€æµ®ç‚¹ã€å¤æ•°ã€å¤šè¿›åˆ¶
- âœ… **å­—ç¬¦ä¸²**: å•/åŒ/ä¸‰å¼•å·ã€åŸå§‹ã€f-string
- âœ… **è½¬ä¹‰**: \n, \t, \u, \xç­‰

### æœ€ä½³å®è·µ

- âœ… ä½¿ç”¨æœ‰æ„ä¹‰çš„æ ‡è¯†ç¬¦
- âœ… éµå¾ªPEP 8å‘½åçº¦å®š
- âœ… åˆç†ä½¿ç”¨f-string
- âœ… å¤§æ•°å­—ä½¿ç”¨ä¸‹åˆ’çº¿åˆ†éš”
- âœ… æ³¨æ„ç¼–ç å£°æ˜ï¼ˆéUTF-8æ—¶ï¼‰

---

**ç†è§£è¯æ³•åˆ†æï¼ŒæŒæ¡Pythonè¯­æ³•åŸºç¡€ï¼** ğŸ”¤âœ¨

**ç›¸å…³æ–‡æ¡£**:
- [02-grammar.md](02-grammar.md) - è¯­æ³•ç»“æ„
- [03-expressions.md](03-expressions.md) - è¡¨è¾¾å¼è¯­ä¹‰

**æœ€åæ›´æ–°**: 2025å¹´10æœˆ28æ—¥

