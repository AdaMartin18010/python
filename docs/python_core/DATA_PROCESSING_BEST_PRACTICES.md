# Python æ•°æ®å¤„ç†æœ€ä½³å®è·µ 2025

**ç°ä»£åŒ–æ•°æ®å¤„ç†æŠ€æœ¯æ ˆä¸å®æˆ˜**

---

## ğŸ“Š æ•°æ®å¤„ç†æŠ€æœ¯æ ˆ

```mermaid
mindmap
  root((æ•°æ®å¤„ç†))
    æ•°æ®è¯»å–
      CSV/Excel
      JSON/XML
      æ•°æ®åº“
      APIæ¥å£
      
    æ•°æ®æ¸…æ´—
      ç¼ºå¤±å€¼å¤„ç†
      å¼‚å¸¸å€¼æ£€æµ‹
      æ•°æ®å»é‡
      ç±»å‹è½¬æ¢
      
    æ•°æ®è½¬æ¢
      è¿‡æ»¤ç­›é€‰
      èšåˆç»Ÿè®¡
      åˆå¹¶æ‹¼æ¥
      é‡å¡‘é€è§†
      
    æ•°æ®åˆ†æ
      æè¿°ç»Ÿè®¡
      åˆ†ç»„åˆ†æ
      æ—¶é—´åºåˆ—
      å…³è”åˆ†æ
      
    æ€§èƒ½ä¼˜åŒ–
      Pandasä¼˜åŒ–
      PolarsåŠ é€Ÿ
      å¹¶è¡Œå¤„ç†
      å†…å­˜ç®¡ç†
```

---

## 1ï¸âƒ£ æ•°æ®è¯»å–

### 1.1 CSV/Excelå¤„ç†

```python
"""
CSVå’ŒExcelæ•°æ®è¯»å–
"""
import pandas as pd
import polars as pl
from pathlib import Path

# ============================================
# 1. Pandasè¯»å–CSV
# ============================================

# åŸºç¡€è¯»å–
df = pd.read_csv("data.csv")

# ä¼˜åŒ–è¯»å–
df = pd.read_csv(
    "data.csv",
    dtype={  # æŒ‡å®šç±»å‹å‡å°‘å†…å­˜
        "user_id": "int32",
        "amount": "float32",
        "status": "category"
    },
    parse_dates=["created_at"],  # è§£ææ—¥æœŸ
    usecols=["user_id", "amount", "status", "created_at"],  # åªè¯»å–éœ€è¦çš„åˆ—
    chunksize=10000  # åˆ†å—è¯»å–å¤§æ–‡ä»¶
)

# åˆ†å—å¤„ç†å¤§æ–‡ä»¶
def process_large_csv(filename: str):
    """åˆ†å—å¤„ç†å¤§CSV"""
    for chunk in pd.read_csv(filename, chunksize=100000):
        # å¤„ç†æ¯ä¸ªchunk
        result = process_chunk(chunk)
        # ä¿å­˜æˆ–ç´¯ç§¯ç»“æœ
        yield result

# ============================================
# 2. Polarsè¯»å– (æ›´å¿«!)
# ============================================

# Polarsè¯»å–CSV (æ¯”Pandaså¿«5-10x)
df = pl.read_csv(
    "data.csv",
    schema={  # æŒ‡å®šæ¨¡å¼
        "user_id": pl.Int32,
        "amount": pl.Float32,
        "status": pl.Categorical,
        "created_at": pl.Datetime
    }
)

# æ‡’åŠ è½½ (åªåœ¨éœ€è¦æ—¶è®¡ç®—)
lazy_df = pl.scan_csv("data.csv")
result = (
    lazy_df
    .filter(pl.col("amount") > 100)
    .groupby("user_id")
    .agg(pl.col("amount").sum())
    .collect()  # è§¦å‘è®¡ç®—
)

# ============================================
# 3. Excelå¤„ç†
# ============================================

# è¯»å–Excel
df = pd.read_excel(
    "data.xlsx",
    sheet_name="Sales",  # æŒ‡å®šsheet
    header=0,  # è¡¨å¤´è¡Œ
    skiprows=2,  # è·³è¿‡å‰2è¡Œ
    usecols="A:D",  # åªè¯»å–A-Dåˆ—
)

# å†™å…¥Excel (å¤šsheet)
with pd.ExcelWriter("output.xlsx", engine="openpyxl") as writer:
    df1.to_excel(writer, sheet_name="Sales", index=False)
    df2.to_excel(writer, sheet_name="Products", index=False)

# ============================================
# æ€§èƒ½å¯¹æ¯”
# ============================================

"""
è¯»å–1GB CSVæ–‡ä»¶:
- Pandas:  ~45s
- Polars:  ~8s  (5.6x faster!)

å†…å­˜å ç”¨:
- Pandas:  ~4GB
- Polars:  ~1GB (4x less!)
"""
```

### 1.2 JSON/æ•°æ®åº“

```python
"""
JSONå’Œæ•°æ®åº“æ•°æ®å¤„ç†
"""
import json
from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession

# ============================================
# 1. JSONå¤„ç†
# ============================================

# è¯»å–JSON
df = pd.read_json("data.json")

# è¯»å–JSON Lines (æ¯è¡Œä¸€ä¸ªJSON)
df = pd.read_json("data.jsonl", lines=True)

# å¤„ç†åµŒå¥—JSON
data = [
    {"id": 1, "user": {"name": "Alice", "age": 30}},
    {"id": 2, "user": {"name": "Bob", "age": 25}}
]

# å±•å¹³åµŒå¥—ç»“æ„
df = pd.json_normalize(data)
# ç»“æœ:
#    id user.name  user.age
# 0   1     Alice        30
# 1   2       Bob        25

# ============================================
# 2. æ•°æ®åº“è¯»å– (Pandas)
# ============================================

from sqlalchemy import create_engine

# åˆ›å»ºå¼•æ“
engine = create_engine("postgresql://user:pass@localhost/db")

# è¯»å–æ•´è¡¨
df = pd.read_sql_table("users", engine)

# æ‰§è¡ŒSQLæŸ¥è¯¢
df = pd.read_sql_query(
    "SELECT * FROM users WHERE created_at > '2024-01-01'",
    engine
)

# ä½¿ç”¨å‚æ•°åŒ–æŸ¥è¯¢
df = pd.read_sql_query(
    "SELECT * FROM users WHERE id = %(user_id)s",
    engine,
    params={"user_id": 123}
)

# ============================================
# 3. å¼‚æ­¥æ•°æ®åº“è¯»å–
# ============================================

async def fetch_data_async(session: AsyncSession) -> pd.DataFrame:
    """å¼‚æ­¥è¯»å–æ•°æ®"""
    stmt = select(User).where(User.active == True)
    result = await session.execute(stmt)
    users = result.scalars().all()
    
    # è½¬æ¢ä¸ºDataFrame
    return pd.DataFrame([
        {
            "id": user.id,
            "name": user.name,
            "email": user.email
        }
        for user in users
    ])

# ============================================
# 4. Polarsæ•°æ®åº“è¿æ¥
# ============================================

# Polarsè¯»å–æ•°æ®åº“ (é€šè¿‡connectorxæ›´å¿«)
import connectorx as cx

df = pl.read_database(
    "SELECT * FROM users",
    "postgresql://user:pass@localhost/db"
)
```

---

## 2ï¸âƒ£ æ•°æ®æ¸…æ´—

### 2.1 ç¼ºå¤±å€¼å¤„ç†

```python
"""
ç¼ºå¤±å€¼å¤„ç†ç­–ç•¥
"""

# ============================================
# 1. æ£€æµ‹ç¼ºå¤±å€¼
# ============================================

# æŸ¥çœ‹ç¼ºå¤±å€¼æ•°é‡
print(df.isnull().sum())

# ç¼ºå¤±å€¼å æ¯”
print(df.isnull().sum() / len(df))

# å¯è§†åŒ–ç¼ºå¤±å€¼
import missingno as msno
msno.matrix(df)  # ç¼ºå¤±å€¼çŸ©é˜µå›¾
msno.bar(df)     # ç¼ºå¤±å€¼æ¡å½¢å›¾

# ============================================
# 2. åˆ é™¤ç¼ºå¤±å€¼
# ============================================

# åˆ é™¤åŒ…å«ä»»ä½•ç¼ºå¤±å€¼çš„è¡Œ
df_clean = df.dropna()

# åˆ é™¤ç‰¹å®šåˆ—ç¼ºå¤±çš„è¡Œ
df_clean = df.dropna(subset=["email", "phone"])

# åˆ é™¤ç¼ºå¤±å€¼è¿‡å¤šçš„åˆ— (>50%)
threshold = len(df) * 0.5
df_clean = df.dropna(axis=1, thresh=threshold)

# ============================================
# 3. å¡«å……ç¼ºå¤±å€¼
# ============================================

# ç”¨å›ºå®šå€¼å¡«å……
df["age"] = df["age"].fillna(0)

# ç”¨å‡å€¼å¡«å……
df["age"] = df["age"].fillna(df["age"].mean())

# ç”¨ä¸­ä½æ•°å¡«å…… (å¯¹å¼‚å¸¸å€¼é²æ£’)
df["age"] = df["age"].fillna(df["age"].median())

# ç”¨ä¼—æ•°å¡«å…… (åˆ†ç±»å˜é‡)
df["category"] = df["category"].fillna(df["category"].mode()[0])

# å‰å‘å¡«å…… (æ—¶é—´åºåˆ—)
df["price"] = df["price"].fillna(method="ffill")

# åå‘å¡«å……
df["price"] = df["price"].fillna(method="bfill")

# æ’å€¼å¡«å…… (æ•°å€¼åºåˆ—)
df["temperature"] = df["temperature"].interpolate(method="linear")

# ============================================
# 4. åˆ†ç»„å¡«å……
# ============================================

# æŒ‰åˆ†ç»„ç”¨å‡å€¼å¡«å……
df["price"] = df.groupby("category")["price"].transform(
    lambda x: x.fillna(x.mean())
)

# ============================================
# 5. Polarsç¼ºå¤±å€¼å¤„ç†
# ============================================

# Polarsä¸­çš„ç¼ºå¤±å€¼å¤„ç†
df = (
    pl.read_csv("data.csv")
    .fill_null(strategy="forward")  # å‰å‘å¡«å……
    .fill_null(0)  # å‰©ä½™ç”¨0å¡«å……
)

# æ¡ä»¶å¡«å……
df = df.with_columns([
    pl.when(pl.col("age").is_null())
    .then(pl.col("age").mean())
    .otherwise(pl.col("age"))
    .alias("age")
])
```

### 2.2 æ•°æ®éªŒè¯ä¸æ¸…æ´—

```python
"""
æ•°æ®éªŒè¯å’Œæ¸…æ´—
"""
from typing import Dict, List
import re

# ============================================
# 1. æ•°æ®ç±»å‹éªŒè¯
# ============================================

def validate_dtypes(df: pd.DataFrame, schema: Dict[str, str]) -> pd.DataFrame:
    """éªŒè¯å¹¶è½¬æ¢æ•°æ®ç±»å‹"""
    for col, dtype in schema.items():
        if col not in df.columns:
            raise ValueError(f"Column {col} not found")
        
        try:
            df[col] = df[col].astype(dtype)
        except Exception as e:
            print(f"Failed to convert {col} to {dtype}: {e}")
    
    return df

# ä½¿ç”¨
schema = {
    "user_id": "int32",
    "amount": "float32",
    "created_at": "datetime64[ns]"
}
df = validate_dtypes(df, schema)

# ============================================
# 2. æ•°å€¼èŒƒå›´éªŒè¯
# ============================================

# æ£€æµ‹å¼‚å¸¸å€¼ (IQRæ–¹æ³•)
def detect_outliers_iqr(df: pd.DataFrame, column: str) -> pd.Series:
    """ä½¿ç”¨IQRæ£€æµ‹å¼‚å¸¸å€¼"""
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    
    return (df[column] < lower_bound) | (df[column] > upper_bound)

# ç§»é™¤å¼‚å¸¸å€¼
outliers = detect_outliers_iqr(df, "price")
df_clean = df[~outliers]

# æˆªæ–­å¼‚å¸¸å€¼
def clip_outliers(df: pd.DataFrame, column: str) -> pd.DataFrame:
    """æˆªæ–­å¼‚å¸¸å€¼åˆ°åˆç†èŒƒå›´"""
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    
    df[column] = df[column].clip(lower_bound, upper_bound)
    return df

# ============================================
# 3. å­—ç¬¦ä¸²æ¸…æ´—
# ============================================

# å»é™¤ç©ºç™½
df["name"] = df["name"].str.strip()

# è½¬æ¢å¤§å°å†™
df["email"] = df["email"].str.lower()

# æ›¿æ¢å­—ç¬¦
df["phone"] = df["phone"].str.replace(r"[^\d]", "", regex=True)

# éªŒè¯é‚®ç®±æ ¼å¼
def validate_email(email: str) -> bool:
    """éªŒè¯é‚®ç®±æ ¼å¼"""
    pattern = r"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$"
    return bool(re.match(pattern, email))

df["valid_email"] = df["email"].apply(validate_email)

# ============================================
# 4. å»é‡
# ============================================

# åˆ é™¤å®Œå…¨é‡å¤çš„è¡Œ
df_unique = df.drop_duplicates()

# åŸºäºç‰¹å®šåˆ—å»é‡
df_unique = df.drop_duplicates(subset=["user_id"])

# ä¿ç•™æœ€åä¸€æ¬¡å‡ºç°
df_unique = df.drop_duplicates(subset=["user_id"], keep="last")

# æ ‡è®°é‡å¤
df["is_duplicate"] = df.duplicated(subset=["user_id"])
```

---

## 3ï¸âƒ£ æ•°æ®è½¬æ¢

### 3.1 ç­›é€‰ä¸èšåˆ

```python
"""
æ•°æ®ç­›é€‰å’Œèšåˆ
"""

# ============================================
# 1. æ¡ä»¶ç­›é€‰
# ============================================

# å•æ¡ä»¶
df_filtered = df[df["age"] > 18]

# å¤šæ¡ä»¶ (AND)
df_filtered = df[(df["age"] > 18) & (df["amount"] > 100)]

# å¤šæ¡ä»¶ (OR)
df_filtered = df[(df["status"] == "active") | (df["status"] == "pending")]

# ä½¿ç”¨query (æ›´æ¸…æ™°)
df_filtered = df.query("age > 18 and amount > 100")

# ä½¿ç”¨isin
df_filtered = df[df["category"].isin(["A", "B", "C"])]

# ============================================
# 2. åˆ†ç»„èšåˆ
# ============================================

# å•åˆ—èšåˆ
result = df.groupby("category")["amount"].sum()

# å¤šåˆ—èšåˆ
result = df.groupby("category").agg({
    "amount": ["sum", "mean", "count"],
    "quantity": "sum"
})

# è‡ªå®šä¹‰èšåˆ
result = df.groupby("category").agg({
    "amount": lambda x: x.quantile(0.95),  # 95åˆ†ä½æ•°
    "user_id": "nunique"  # å”¯ä¸€å€¼æ•°é‡
})

# å‘½åèšåˆ (Pandas 1.0+)
result = df.groupby("category").agg(
    total_amount=("amount", "sum"),
    avg_amount=("amount", "mean"),
    user_count=("user_id", "nunique")
)

# ============================================
# 3. Polarsåˆ†ç»„èšåˆ (æ›´å¿«!)
# ============================================

result = (
    df
    .groupby("category")
    .agg([
        pl.col("amount").sum().alias("total_amount"),
        pl.col("amount").mean().alias("avg_amount"),
        pl.col("user_id").n_unique().alias("user_count")
    ])
)

# å¤šåˆ—åˆ†ç»„
result = (
    df
    .groupby(["category", "region"])
    .agg([
        pl.col("amount").sum(),
        pl.col("quantity").sum()
    ])
)

# ============================================
# 4. çª—å£å‡½æ•°
# ============================================

# è®¡ç®—æ’å
df["rank"] = df.groupby("category")["amount"].rank(
    method="dense",
    ascending=False
)

# ç´¯ç§¯å’Œ
df["cumsum"] = df.groupby("category")["amount"].cumsum()

# æ»šåŠ¨çª—å£
df["rolling_avg"] = df.groupby("user_id")["amount"].transform(
    lambda x: x.rolling(window=7, min_periods=1).mean()
)
```

### 3.2 æ•°æ®åˆå¹¶ä¸é‡å¡‘

```python
"""
æ•°æ®åˆå¹¶å’Œé‡å¡‘
"""

# ============================================
# 1. åˆå¹¶ (Join)
# ============================================

# å†…è¿æ¥
result = pd.merge(df1, df2, on="user_id", how="inner")

# å·¦è¿æ¥
result = pd.merge(df1, df2, on="user_id", how="left")

# å¤šé”®è¿æ¥
result = pd.merge(
    df1, df2,
    on=["user_id", "date"],
    how="inner"
)

# ä¸åŒåˆ—åè¿æ¥
result = pd.merge(
    df1, df2,
    left_on="user_id",
    right_on="id",
    how="inner"
)

# ============================================
# 2. æ‹¼æ¥ (Concat)
# ============================================

# å‚ç›´æ‹¼æ¥ (è¡Œ)
result = pd.concat([df1, df2], axis=0, ignore_index=True)

# æ°´å¹³æ‹¼æ¥ (åˆ—)
result = pd.concat([df1, df2], axis=1)

# ============================================
# 3. é€è§†è¡¨
# ============================================

# åˆ›å»ºé€è§†è¡¨
pivot = df.pivot_table(
    values="amount",
    index="date",
    columns="category",
    aggfunc="sum",
    fill_value=0
)

# å¤šå€¼é€è§†
pivot = df.pivot_table(
    values=["amount", "quantity"],
    index="date",
    columns="category",
    aggfunc={"amount": "sum", "quantity": "sum"}
)

# ============================================
# 4. é‡å¡‘
# ============================================

# å®½å˜é•¿ (melt)
long_df = pd.melt(
    df,
    id_vars=["id", "date"],
    value_vars=["sales", "profit"],
    var_name="metric",
    value_name="value"
)

# é•¿å˜å®½ (pivot)
wide_df = long_df.pivot(
    index="id",
    columns="metric",
    values="value"
)
```

---

## 4ï¸âƒ£ æ€§èƒ½ä¼˜åŒ–

### 4.1 Pandasä¼˜åŒ–æŠ€å·§

```python
"""
Pandasæ€§èƒ½ä¼˜åŒ–
"""

# ============================================
# 1. ä½¿ç”¨åˆé€‚çš„æ•°æ®ç±»å‹
# ============================================

# âŒ é»˜è®¤ç±»å‹ (å ç”¨å¤š)
df = pd.read_csv("data.csv")
print(df.memory_usage(deep=True))

# âœ… ä¼˜åŒ–ç±»å‹ (èŠ‚çœå†…å­˜)
dtype_map = {
    "user_id": "int32",      # int64 â†’ int32
    "amount": "float32",      # float64 â†’ float32
    "status": "category",     # object â†’ category
}
df = pd.read_csv("data.csv", dtype=dtype_map)

# å†…å­˜èŠ‚çœç¤ºä¾‹:
# Before: 400MB
# After:  120MB (70% reduction!)

# ============================================
# 2. å‘é‡åŒ–æ“ä½œ
# ============================================

# âŒ å¾ªç¯ (æ…¢)
for i in range(len(df)):
    df.loc[i, "total"] = df.loc[i, "price"] * df.loc[i, "quantity"]

# âœ… å‘é‡åŒ– (å¿« 100x+)
df["total"] = df["price"] * df["quantity"]

# âŒ apply (æ…¢)
df["category"] = df["value"].apply(lambda x: "high" if x > 100 else "low")

# âœ… å‘é‡åŒ–æ¡ä»¶ (å¿« 10x+)
df["category"] = np.where(df["value"] > 100, "high", "low")

# ============================================
# 3. ä½¿ç”¨evalå’Œquery
# ============================================

# å¤æ‚è¡¨è¾¾å¼ä¼˜åŒ–
# âŒ æ™®é€šæ–¹å¼
df["result"] = df["a"] + df["b"] * df["c"] - df["d"]

# âœ… eval (å¿« 2-3x)
df.eval("result = a + b * c - d", inplace=True)

# å¤æ‚ç­›é€‰
# âŒ æ™®é€šæ–¹å¼
filtered = df[(df["a"] > 0) & (df["b"] < 100)]

# âœ… query (æ›´å¿«æ›´æ¸…æ™°)
filtered = df.query("a > 0 and b < 100")

# ============================================
# 4. é¿å…é“¾å¼ç´¢å¼•
# ============================================

# âŒ é“¾å¼ç´¢å¼• (æ…¢ä¸”è­¦å‘Š)
df[df["age"] > 18]["name"] = "Adult"

# âœ… loc (å¿«ä¸”æ­£ç¡®)
df.loc[df["age"] > 18, "name"] = "Adult"

# ============================================
# 5. ä½¿ç”¨inplace
# ============================================

# âŒ åˆ›å»ºå‰¯æœ¬ (æ…¢,å å†…å­˜)
df = df.drop("column", axis=1)

# âœ… inplaceä¿®æ”¹ (å¿«)
df.drop("column", axis=1, inplace=True)
```

### 4.2 Polarsç°ä»£åŒ–æ–¹æ¡ˆ

```python
"""
Polars - ç°ä»£åŒ–é«˜æ€§èƒ½æ•°æ®å¤„ç†
"""
import polars as pl

# ============================================
# 1. Polars vs Pandasæ€§èƒ½å¯¹æ¯”
# ============================================

# Polarsä¼˜åŠ¿:
# - å¤šçº¿ç¨‹å¹¶è¡Œ
# - æ‡’åŠ è½½ä¼˜åŒ–
# - Arrowå†…å­˜æ ¼å¼
# - Rustå®ç°

# è¯»å–é€Ÿåº¦: 5-10x faster
df = pl.read_csv("large.csv")

# èšåˆé€Ÿåº¦: 10-100x faster
result = (
    df
    .groupby("category")
    .agg([
        pl.col("amount").sum(),
        pl.col("amount").mean()
    ])
)

# ============================================
# 2. æ‡’åŠ è½½æŸ¥è¯¢ä¼˜åŒ–
# ============================================

# æ‡’åŠ è½½ (åªè®°å½•æ“ä½œ,ä¸æ‰§è¡Œ)
lazy = (
    pl.scan_csv("data.csv")
    .filter(pl.col("amount") > 100)
    .groupby("user_id")
    .agg(pl.col("amount").sum())
)

# æŸ¥çœ‹ä¼˜åŒ–åçš„æ‰§è¡Œè®¡åˆ’
print(lazy.explain())

# è§¦å‘è®¡ç®—
result = lazy.collect()

# ============================================
# 3. è¡¨è¾¾å¼API (é“¾å¼æ“ä½œ)
# ============================================

result = (
    pl.read_csv("data.csv")
    # ç­›é€‰
    .filter(pl.col("age") > 18)
    # æ·»åŠ åˆ—
    .with_columns([
        (pl.col("amount") * 1.1).alias("amount_with_tax"),
        pl.col("name").str.to_uppercase().alias("name_upper")
    ])
    # åˆ†ç»„èšåˆ
    .groupby("category")
    .agg([
        pl.col("amount").sum().alias("total"),
        pl.col("user_id").n_unique().alias("users")
    ])
    # æ’åº
    .sort("total", descending=True)
    # å–å‰10
    .head(10)
)

# ============================================
# 4. å¹¶è¡Œå¤„ç†
# ============================================

# Polarsè‡ªåŠ¨å¹¶è¡Œ,æ— éœ€æ‰‹åŠ¨è®¾ç½®
# è‡ªåŠ¨ä½¿ç”¨æ‰€æœ‰CPUæ ¸å¿ƒ

# æ€§èƒ½ç¤ºä¾‹:
# æ•°æ®é›†: 10GB, 100Mè¡Œ
# æ“ä½œ: åˆ†ç»„èšåˆ

# Pandas (å•çº¿ç¨‹): ~120s
# Polars (å¤šçº¿ç¨‹):  ~8s  (15x faster!)
```

---

## ğŸ“Š å·¥å…·é€‰æ‹©æŒ‡å—

### Pandas vs Polarså¯¹æ¯”

| ç»´åº¦ | Pandas | Polars | æ¨è |
|------|--------|--------|------|
| **æ€§èƒ½** | åŸºå‡† | 5-100x faster | Polars |
| **å†…å­˜** | é«˜ | ä½ (50-70% less) | Polars |
| **å¹¶è¡Œ** | æœ‰é™ | åŸç”Ÿå¤šçº¿ç¨‹ | Polars |
| **ç”Ÿæ€** | æˆç†Ÿå®Œæ•´ | å¿«é€Ÿæˆé•¿ | Pandas |
| **å­¦ä¹ æ›²çº¿** | å¹³ç¼“ | ä¸­ç­‰ | Pandas |
| **APIç¨³å®šæ€§** | ç¨³å®š | å¿«é€Ÿè¿­ä»£ | Pandas |

### ä½¿ç”¨åœºæ™¯

**ä½¿ç”¨Pandas**:
- æ•°æ®é‡ < 1GB
- éœ€è¦ä¸°å¯Œçš„ç”Ÿæ€åº“
- å›¢é˜Ÿå·²ç†Ÿæ‚‰Pandas
- éœ€è¦æœ€å¤§å…¼å®¹æ€§

**ä½¿ç”¨Polars**:
- æ•°æ®é‡ > 1GB
- æ€§èƒ½è¦æ±‚é«˜
- æ–°é¡¹ç›®æˆ–å¯è¿ç§»
- éœ€è¦å¹¶è¡Œå¤„ç†

---

**æŒæ¡ç°ä»£æ•°æ®å¤„ç†,æ„å»ºé«˜æ•ˆæ•°æ®ç®¡é“ï¼** ğŸ“Šâœ¨

**æœ€åæ›´æ–°**: 2025å¹´10æœˆ28æ—¥

