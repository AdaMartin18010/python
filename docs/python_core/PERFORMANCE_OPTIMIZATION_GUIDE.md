# Python æ€§èƒ½ä¼˜åŒ–å®Œå…¨æŒ‡å— 2025

**ä»ç®—æ³•åˆ°ç³»ç»Ÿçš„å…¨æ–¹ä½æ€§èƒ½ä¼˜åŒ–**

---

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–ä½“ç³»

```mermaid
mindmap
  root((æ€§èƒ½ä¼˜åŒ–))
    ç®—æ³•å±‚
      æ—¶é—´å¤æ‚åº¦
      ç©ºé—´å¤æ‚åº¦
      æ•°æ®ç»“æ„é€‰æ‹©
    
    è¯­è¨€å±‚
      å†…ç½®ä¼˜åŒ–
      ç”Ÿæˆå™¨
      åˆ—è¡¨æ¨å¯¼
      å±€éƒ¨å˜é‡
    
    å¹¶å‘å±‚
      å¤šçº¿ç¨‹
      å¤šè¿›ç¨‹
      å¼‚æ­¥IO
      Free-Threaded
    
    æ‰©å±•å±‚
      Cython
      NumPy
      Rustæ‰©å±•
      Cæ‰©å±•
    
    ç³»ç»Ÿå±‚
      ç¼“å­˜ç­–ç•¥
      æ•°æ®åº“ä¼˜åŒ–
      ç½‘ç»œä¼˜åŒ–
      éƒ¨ç½²ä¼˜åŒ–
```

---

## 1ï¸âƒ£ ç®—æ³•å±‚ä¼˜åŒ–

### 1.1 æ—¶é—´å¤æ‚åº¦ä¼˜åŒ–

```python
"""
æ—¶é—´å¤æ‚åº¦ä¼˜åŒ–ç¤ºä¾‹
"""
from typing import List
import time
from functools import wraps

def benchmark(func):
    """æ€§èƒ½åŸºå‡†è£…é¥°å™¨"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        start = time.perf_counter()
        result = func(*args, **kwargs)
        end = time.perf_counter()
        print(f"{func.__name__}: {end - start:.6f}s")
        return result
    return wrapper

# ============================================
# æ¡ˆä¾‹1: æŸ¥æ‰¾ä¼˜åŒ–
# ============================================

# âŒ O(nÂ²) - åµŒå¥—å¾ªç¯
@benchmark
def find_duplicates_slow(nums: List[int]) -> List[int]:
    """æ…¢é€ŸæŸ¥æ‰¾é‡å¤å…ƒç´ """
    duplicates = []
    for i in range(len(nums)):
        for j in range(i + 1, len(nums)):
            if nums[i] == nums[j] and nums[i] not in duplicates:
                duplicates.append(nums[i])
    return duplicates

# âœ… O(n) - ä½¿ç”¨é›†åˆ
@benchmark
def find_duplicates_fast(nums: List[int]) -> List[int]:
    """å¿«é€ŸæŸ¥æ‰¾é‡å¤å…ƒç´ """
    seen = set()
    duplicates = set()
    for num in nums:
        if num in seen:
            duplicates.add(num)
        seen.add(num)
    return list(duplicates)

# æµ‹è¯•
data = list(range(1000)) * 2
find_duplicates_slow(data)   # ~0.15s
find_duplicates_fast(data)   # ~0.0001s  (1500x faster!)

# ============================================
# æ¡ˆä¾‹2: é¢‘ç‡ç»Ÿè®¡ä¼˜åŒ–
# ============================================

# âŒ O(nÂ²) - é‡å¤éå†
@benchmark
def count_frequency_slow(items: List[str]) -> dict[str, int]:
    """æ…¢é€Ÿé¢‘ç‡ç»Ÿè®¡"""
    freq = {}
    for item in set(items):
        freq[item] = items.count(item)  # O(n) for each item
    return freq

# âœ… O(n) - å•æ¬¡éå†
@benchmark
def count_frequency_fast(items: List[str]) -> dict[str, int]:
    """å¿«é€Ÿé¢‘ç‡ç»Ÿè®¡"""
    freq = {}
    for item in items:
        freq[item] = freq.get(item, 0) + 1
    return freq

# âœ… O(n) - ä½¿ç”¨Counter
from collections import Counter

@benchmark
def count_frequency_counter(items: List[str]) -> dict[str, int]:
    """ä½¿ç”¨Counterç»Ÿè®¡"""
    return dict(Counter(items))

# æµ‹è¯•
words = ["python"] * 1000 + ["java"] * 500 + ["rust"] * 300
count_frequency_slow(words)     # ~0.05s
count_frequency_fast(words)     # ~0.0002s
count_frequency_counter(words)  # ~0.0001s (fastest!)
```

### 1.2 æ•°æ®ç»“æ„é€‰æ‹©

```python
"""
æ­£ç¡®çš„æ•°æ®ç»“æ„é€‰æ‹©
"""
from collections import deque, defaultdict, OrderedDict
import bisect

# ============================================
# 1. åˆ—è¡¨ vs åŒç«¯é˜Ÿåˆ—
# ============================================

# âŒ åˆ—è¡¨ - O(n) å¤´éƒ¨æ’å…¥
@benchmark
def list_operations():
    items = []
    for i in range(10000):
        items.insert(0, i)  # O(n) - æ…¢!
    return items

# âœ… åŒç«¯é˜Ÿåˆ— - O(1) å¤´éƒ¨æ’å…¥
@benchmark
def deque_operations():
    items = deque()
    for i in range(10000):
        items.appendleft(i)  # O(1) - å¿«!
    return items

# ============================================
# 2. åˆ—è¡¨ vs é›†åˆæŸ¥æ‰¾
# ============================================

# âŒ åˆ—è¡¨æŸ¥æ‰¾ - O(n)
@benchmark
def list_lookup():
    items = list(range(10000))
    return sum(1 for i in range(10000) if i in items)

# âœ… é›†åˆæŸ¥æ‰¾ - O(1)
@benchmark
def set_lookup():
    items = set(range(10000))
    return sum(1 for i in range(10000) if i in items)

# ============================================
# 3. æœ‰åºæ’å…¥ä¼˜åŒ–
# ============================================

# âŒ æ’å…¥åæ’åº - O(n log n)
@benchmark
def insert_and_sort():
    items = []
    for i in range(1000, 0, -1):
        items.append(i)
        items.sort()  # æ¯æ¬¡éƒ½æ’åº!
    return items

# âœ… äºŒåˆ†æ’å…¥ - O(n log n) ä½†å¸¸æ•°æ›´å°
@benchmark
def bisect_insert():
    items = []
    for i in range(1000, 0, -1):
        bisect.insort(items, i)  # ä¿æŒæœ‰åº
    return items

# ============================================
# 4. å­—å…¸é»˜è®¤å€¼å¤„ç†
# ============================================

# âŒ æ™®é€šå­—å…¸ - éœ€è¦æ£€æŸ¥
def group_by_length_slow(words: List[str]) -> dict:
    groups = {}
    for word in words:
        length = len(word)
        if length not in groups:
            groups[length] = []
        groups[length].append(word)
    return groups

# âœ… defaultdict - è‡ªåŠ¨åˆå§‹åŒ–
def group_by_length_fast(words: List[str]) -> dict:
    groups = defaultdict(list)
    for word in words:
        groups[len(word)].append(word)
    return groups

# ============================================
# æ•°æ®ç»“æ„æ€§èƒ½å¯¹æ¯”
# ============================================

"""
æ“ä½œ            List    Deque   Set     Dict
append          O(1)    O(1)    O(1)    O(1)
appendleft      O(n)    O(1)    -       -
insert          O(n)    O(n)    -       -
pop             O(1)    O(1)    O(1)    O(1)
popleft         O(n)    O(1)    -       -
search          O(n)    O(n)    O(1)    O(1)
"""
```

---

## 2ï¸âƒ£ Pythonè¯­è¨€å±‚ä¼˜åŒ–

### 2.1 å†…ç½®å‡½æ•°å’Œæ“ä½œç¬¦

```python
"""
ä½¿ç”¨å†…ç½®å‡½æ•°ä¼˜åŒ–
"""

# ============================================
# 1. å­—ç¬¦ä¸²æ‹¼æ¥
# ============================================

# âŒ å¾ªç¯æ‹¼æ¥ - O(nÂ²)
@benchmark
def concat_slow(items: List[str]) -> str:
    result = ""
    for item in items:
        result += item  # æ¯æ¬¡åˆ›å»ºæ–°å­—ç¬¦ä¸²!
    return result

# âœ… join - O(n)
@benchmark
def concat_fast(items: List[str]) -> str:
    return "".join(items)  # ä¸€æ¬¡æ€§åˆ†é…å†…å­˜

# æµ‹è¯•
items = ["python"] * 10000
concat_slow(items)  # ~0.5s
concat_fast(items)  # ~0.001s

# ============================================
# 2. åˆ—è¡¨æ“ä½œ
# ============================================

# âŒ å¾ªç¯append
@benchmark
def build_list_slow():
    result = []
    for i in range(10000):
        result.append(i * 2)
    return result

# âœ… åˆ—è¡¨æ¨å¯¼
@benchmark
def build_list_fast():
    return [i * 2 for i in range(10000)]

# âœ… mapå‡½æ•°
@benchmark
def build_list_map():
    return list(map(lambda x: x * 2, range(10000)))

# ============================================
# 3. æ¡ä»¶è¿‡æ»¤
# ============================================

# âŒ å¾ªç¯è¿‡æ»¤
@benchmark
def filter_slow(nums: List[int]) -> List[int]:
    result = []
    for num in nums:
        if num % 2 == 0:
            result.append(num)
    return result

# âœ… åˆ—è¡¨æ¨å¯¼
@benchmark
def filter_comprehension(nums: List[int]) -> List[int]:
    return [num for num in nums if num % 2 == 0]

# âœ… filterå‡½æ•°
@benchmark
def filter_builtin(nums: List[int]) -> List[int]:
    return list(filter(lambda x: x % 2 == 0, nums))

# ============================================
# 4. æ±‚å’Œ/æœ€å€¼
# ============================================

# âŒ æ‰‹åŠ¨å¾ªç¯
@benchmark
def sum_slow(nums: List[int]) -> int:
    total = 0
    for num in nums:
        total += num
    return total

# âœ… å†…ç½®sum
@benchmark
def sum_fast(nums: List[int]) -> int:
    return sum(nums)

# âŒ æ‰‹åŠ¨æŸ¥æ‰¾æœ€å¤§å€¼
@benchmark
def max_slow(nums: List[int]) -> int:
    maximum = nums[0]
    for num in nums[1:]:
        if num > maximum:
            maximum = num
    return maximum

# âœ… å†…ç½®max
@benchmark
def max_fast(nums: List[int]) -> int:
    return max(nums)
```

### 2.2 ç”Ÿæˆå™¨å’Œè¿­ä»£å™¨

```python
"""
ç”Ÿæˆå™¨ä¼˜åŒ–å†…å­˜å’Œæ€§èƒ½
"""

# ============================================
# 1. æƒ°æ€§æ±‚å€¼
# ============================================

# âŒ è¿”å›åˆ—è¡¨ - å†…å­˜å ç”¨å¤§
def read_large_file_slow(filename: str) -> List[str]:
    """ä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰è¡Œ"""
    with open(filename) as f:
        return f.readlines()  # å ç”¨å¤§é‡å†…å­˜

# âœ… ç”Ÿæˆå™¨ - æŒ‰éœ€åŠ è½½
def read_large_file_fast(filename: str):
    """é€è¡Œç”Ÿæˆ"""
    with open(filename) as f:
        for line in f:  # åªåœ¨å†…å­˜ä¸­ä¿ç•™ä¸€è¡Œ
            yield line.strip()

# ä½¿ç”¨
for line in read_large_file_fast("large.txt"):
    process(line)  # å¤„ç†ä¸€è¡Œï¼Œé‡Šæ”¾ä¸€è¡Œ

# ============================================
# 2. ç”Ÿæˆå™¨è¡¨è¾¾å¼ vs åˆ—è¡¨æ¨å¯¼
# ============================================

# âŒ åˆ—è¡¨æ¨å¯¼ - ç«‹å³æ„å»ºæ•´ä¸ªåˆ—è¡¨
@benchmark
def process_with_list():
    data = [i ** 2 for i in range(1000000)]  # å ç”¨å†…å­˜
    return sum(data)

# âœ… ç”Ÿæˆå™¨è¡¨è¾¾å¼ - æƒ°æ€§æ±‚å€¼
@benchmark
def process_with_generator():
    data = (i ** 2 for i in range(1000000))  # ä¸å ç”¨å†…å­˜
    return sum(data)  # è¾¹ç”Ÿæˆè¾¹æ¶ˆè´¹

# ============================================
# 3. itertoolsä¼˜åŒ–
# ============================================

from itertools import islice, chain, groupby, accumulate

# åˆ†æ‰¹å¤„ç†å¤§æ•°æ®
def batch_process(iterable, batch_size=1000):
    """åˆ†æ‰¹å¤„ç†è¿­ä»£å™¨"""
    iterator = iter(iterable)
    while True:
        batch = list(islice(iterator, batch_size))
        if not batch:
            break
        yield batch

# ä½¿ç”¨
for batch in batch_process(range(1000000), batch_size=10000):
    process_batch(batch)

# é“¾æ¥å¤šä¸ªè¿­ä»£å™¨
def process_multiple_sources():
    """é«˜æ•ˆåˆå¹¶å¤šä¸ªæ•°æ®æº"""
    source1 = range(1000)
    source2 = range(1000, 2000)
    source3 = range(2000, 3000)
    
    # âŒ åˆ›å»ºä¸´æ—¶åˆ—è¡¨
    # combined = list(source1) + list(source2) + list(source3)
    
    # âœ… ç›´æ¥é“¾æ¥
    combined = chain(source1, source2, source3)
    return sum(combined)

# åˆ†ç»„æ“ä½œ
def group_data(data: List[tuple[str, int]]):
    """é«˜æ•ˆåˆ†ç»„"""
    # å‡è®¾dataå·²æ’åº
    for key, group in groupby(data, key=lambda x: x[0]):
        yield key, list(group)
```

### 2.3 å±€éƒ¨å˜é‡ä¼˜åŒ–

```python
"""
å±€éƒ¨å˜é‡å’Œå±æ€§è®¿é—®ä¼˜åŒ–
"""

# ============================================
# 1. é¿å…é‡å¤å±æ€§æŸ¥æ‰¾
# ============================================

class DataProcessor:
    def __init__(self):
        self.data = list(range(10000))
    
    # âŒ é‡å¤å±æ€§æŸ¥æ‰¾
    @benchmark
    def process_slow(self):
        result = []
        for item in self.data:
            result.append(item * 2)  # æ¯æ¬¡å¾ªç¯æŸ¥æ‰¾self.data
        return result
    
    # âœ… å±€éƒ¨å˜é‡ç¼“å­˜
    @benchmark
    def process_fast(self):
        data = self.data  # ç¼“å­˜åˆ°å±€éƒ¨å˜é‡
        result = []
        for item in data:
            result.append(item * 2)
        return result

# ============================================
# 2. é¿å…å…¨å±€æŸ¥æ‰¾
# ============================================

import math

# âŒ å…¨å±€å‡½æ•°æŸ¥æ‰¾
@benchmark
def calculate_slow(nums: List[float]) -> List[float]:
    return [math.sqrt(num) for num in nums]

# âœ… å±€éƒ¨å‡½æ•°ç¼“å­˜
@benchmark
def calculate_fast(nums: List[float]) -> List[float]:
    sqrt = math.sqrt  # ç¼“å­˜åˆ°å±€éƒ¨
    return [sqrt(num) for num in nums]

# ============================================
# 3. å¾ªç¯ä¸å˜é‡æå‡
# ============================================

# âŒ å¾ªç¯å†…é‡å¤è®¡ç®—
@benchmark
def compute_slow(data: List[int]) -> List[int]:
    result = []
    for item in data:
        result.append(item * len(data))  # æ¯æ¬¡è®¡ç®—len!
    return result

# âœ… æå‡åˆ°å¾ªç¯å¤–
@benchmark
def compute_fast(data: List[int]) -> List[int]:
    length = len(data)  # åªè®¡ç®—ä¸€æ¬¡
    result = []
    for item in data:
        result.append(item * length)
    return result
```

---

## 3ï¸âƒ£ å¹¶å‘å±‚ä¼˜åŒ–

### 3.1 å¤šè¿›ç¨‹ä¼˜åŒ– (CPUå¯†é›†)

```python
"""
å¤šè¿›ç¨‹ä¼˜åŒ–CPUå¯†é›†å‹ä»»åŠ¡
"""
from multiprocessing import Pool, cpu_count
from concurrent.futures import ProcessPoolExecutor
import time

# CPUå¯†é›†å‹ä»»åŠ¡
def cpu_intensive_task(n: int) -> int:
    """è®¡ç®—å¯†é›†ä»»åŠ¡"""
    result = 0
    for i in range(n):
        result += i ** 2
    return result

# âŒ å•è¿›ç¨‹
@benchmark
def process_single():
    tasks = [1000000] * 8
    results = [cpu_intensive_task(n) for n in tasks]
    return results

# âœ… å¤šè¿›ç¨‹
@benchmark
def process_multi():
    tasks = [1000000] * 8
    with Pool(cpu_count()) as pool:
        results = pool.map(cpu_intensive_task, tasks)
    return results

# âœ… ProcessPoolExecutor (æ›´ç°ä»£)
@benchmark
def process_executor():
    tasks = [1000000] * 8
    with ProcessPoolExecutor(max_workers=cpu_count()) as executor:
        results = list(executor.map(cpu_intensive_task, tasks))
    return results

# æµ‹è¯•ç»“æœ:
# process_single()   : ~8.0s  (å•æ ¸)
# process_multi()    : ~1.2s  (8æ ¸, 6.7x speedup)
# process_executor() : ~1.2s  (8æ ¸, 6.7x speedup)
```

### 3.2 å¼‚æ­¥IOä¼˜åŒ– (I/Oå¯†é›†)

```python
"""
å¼‚æ­¥IOä¼˜åŒ–ç½‘ç»œ/IOå¯†é›†å‹ä»»åŠ¡
"""
import asyncio
import aiohttp
from typing import List

# âŒ åŒæ­¥è¯·æ±‚
@benchmark
def fetch_urls_sync(urls: List[str]) -> List[str]:
    """åŒæ­¥è·å–URLs"""
    import requests
    results = []
    for url in urls:
        response = requests.get(url)
        results.append(response.text)
    return results

# âœ… å¼‚æ­¥è¯·æ±‚
@benchmark
async def fetch_urls_async(urls: List[str]) -> List[str]:
    """å¼‚æ­¥è·å–URLs"""
    async with aiohttp.ClientSession() as session:
        tasks = [fetch_one(session, url) for url in urls]
        results = await asyncio.gather(*tasks)
    return results

async def fetch_one(session: aiohttp.ClientSession, url: str) -> str:
    """è·å–å•ä¸ªURL"""
    async with session.get(url) as response:
        return await response.text()

# æµ‹è¯• (100ä¸ªURL)
urls = ["https://httpbin.org/delay/1"] * 100

# åŒæ­¥: ~100s (é¡ºåºæ‰§è¡Œ)
# fetch_urls_sync(urls)

# å¼‚æ­¥: ~1-2s (å¹¶å‘æ‰§è¡Œ, 50-100x faster!)
# asyncio.run(fetch_urls_async(urls))
```

### 3.3 Free-Threadedæ¨¡å¼ (Python 3.13+)

```python
"""
Python 3.13 Free-Threadedæ¨¡å¼
"""
import sys

# æ£€æŸ¥æ˜¯å¦å¯ç”¨Free-Threaded
if sys.version_info >= (3, 13) and hasattr(sys, 'is_gil_enabled'):
    if not sys.is_gil_enabled():
        print("Free-Threaded mode enabled!")
        
        # çœŸæ­£çš„å¤šçº¿ç¨‹å¹¶è¡Œ
        from threading import Thread
        import time
        
        def cpu_task(n: int) -> int:
            """CPUå¯†é›†ä»»åŠ¡"""
            result = 0
            for i in range(n):
                result += i ** 2
            return result
        
        @benchmark
        def threads_with_gil():
            """ä¼ ç»ŸGILé™åˆ¶"""
            threads = []
            for _ in range(4):
                t = Thread(target=cpu_task, args=(10000000,))
                threads.append(t)
                t.start()
            
            for t in threads:
                t.join()
        
        @benchmark
        def threads_without_gil():
            """Free-Threadedæ¨¡å¼"""
            # åœ¨Python 3.13+æ— GILæ¨¡å¼ä¸‹
            # å¤šçº¿ç¨‹å¯ä»¥çœŸæ­£å¹¶è¡Œæ‰§è¡ŒCPUä»»åŠ¡
            threads = []
            for _ in range(4):
                t = Thread(target=cpu_task, args=(10000000,))
                threads.append(t)
                t.start()
            
            for t in threads:
                t.join()
        
        # æœ‰GIL:    ~4.0s (é¡ºåºæ‰§è¡Œ)
        # æ— GIL:    ~1.0s (å¹¶è¡Œæ‰§è¡Œ, 4x speedup!)
```

---

## 4ï¸âƒ£ æ‰©å±•å±‚ä¼˜åŒ–

### 4.1 NumPyå‘é‡åŒ–

```python
"""
NumPyå‘é‡åŒ–åŠ é€Ÿ
"""
import numpy as np

# âŒ Pythonå¾ªç¯
@benchmark
def compute_python():
    data = list(range(1000000))
    result = [x ** 2 + 2 * x + 1 for x in data]
    return result

# âœ… NumPyå‘é‡åŒ–
@benchmark
def compute_numpy():
    data = np.arange(1000000)
    result = data ** 2 + 2 * data + 1
    return result

# Python: ~0.15s
# NumPy:  ~0.005s (30x faster!)

# ============================================
# çŸ©é˜µè¿ç®—
# ============================================

# âŒ PythonåµŒå¥—å¾ªç¯
@benchmark
def matrix_multiply_python():
    n = 500
    A = [[i + j for j in range(n)] for i in range(n)]
    B = [[i - j for j in range(n)] for i in range(n)]
    C = [[0] * n for _ in range(n)]
    
    for i in range(n):
        for j in range(n):
            for k in range(n):
                C[i][j] += A[i][k] * B[k][j]
    return C

# âœ… NumPyçŸ©é˜µè¿ç®—
@benchmark
def matrix_multiply_numpy():
    n = 500
    A = np.array([[i + j for j in range(n)] for i in range(n)])
    B = np.array([[i - j for j in range(n)] for i in range(n)])
    C = A @ B  # çŸ©é˜µä¹˜æ³•
    return C

# Python: ~30s
# NumPy:  ~0.05s (600x faster!)
```

### 4.2 CythonåŠ é€Ÿ

```python
"""
Cythonç¼–è¯‘åŠ é€Ÿ
"""

# Pythonç‰ˆæœ¬ (slow.py)
def fibonacci_python(n: int) -> int:
    """Pythonå®ç°"""
    if n < 2:
        return n
    return fibonacci_python(n - 1) + fibonacci_python(n - 2)

# Cythonç‰ˆæœ¬ (fast.pyx)
"""
# cython: language_level=3

cpdef long fibonacci_cython(long n):
    '''Cythonå®ç°'''
    if n < 2:
        return n
    return fibonacci_cython(n - 1) + fibonacci_cython(n - 2)

# ç¼–è¯‘: cythonize -i fast.pyx
"""

# æ€§èƒ½å¯¹æ¯”
# fibonacci_python(35): ~3.5s
# fibonacci_cython(35): ~0.15s (23x faster!)
```

---

## 5ï¸âƒ£ ç³»ç»Ÿå±‚ä¼˜åŒ–

### 5.1 ç¼“å­˜ç­–ç•¥

```python
"""
å¤šå±‚ç¼“å­˜ç­–ç•¥
"""
from functools import lru_cache, cache
import redis
from typing import Optional

# ============================================
# 1. å‡½æ•°çº§ç¼“å­˜
# ============================================

# âŒ æ— ç¼“å­˜
@benchmark
def fibonacci_no_cache(n: int) -> int:
    if n < 2:
        return n
    return fibonacci_no_cache(n - 1) + fibonacci_no_cache(n - 2)

# âœ… LRUç¼“å­˜
@lru_cache(maxsize=128)
@benchmark
def fibonacci_cached(n: int) -> int:
    if n < 2:
        return n
    return fibonacci_cached(n - 1) + fibonacci_cached(n - 2)

# âœ… æ— é™ç¼“å­˜ (Python 3.9+)
@cache
def fibonacci_unlimited(n: int) -> int:
    if n < 2:
        return n
    return fibonacci_unlimited(n - 1) + fibonacci_unlimited(n - 2)

# æµ‹è¯•
# fibonacci_no_cache(35):  ~3.5s
# fibonacci_cached(35):    ~0.00001s (350,000x faster!)

# ============================================
# 2. Redisåˆ†å¸ƒå¼ç¼“å­˜
# ============================================

class CacheService:
    """Redisç¼“å­˜æœåŠ¡"""
    
    def __init__(self, redis_client: redis.Redis):
        self.redis = redis_client
    
    def get_user(self, user_id: int) -> Optional[dict]:
        """è·å–ç”¨æˆ·(å¸¦ç¼“å­˜)"""
        # 1. å°è¯•ä»ç¼“å­˜è·å–
        cache_key = f"user:{user_id}"
        cached = self.redis.get(cache_key)
        
        if cached:
            return json.loads(cached)
        
        # 2. ç¼“å­˜æœªå‘½ä¸­,ä»æ•°æ®åº“æŸ¥è¯¢
        user = self._fetch_from_db(user_id)
        
        # 3. å†™å…¥ç¼“å­˜ (TTL 1å°æ—¶)
        if user:
            self.redis.setex(
                cache_key,
                3600,
                json.dumps(user)
            )
        
        return user
    
    def _fetch_from_db(self, user_id: int) -> Optional[dict]:
        """ä»æ•°æ®åº“è·å–"""
        # æ¨¡æ‹Ÿæ•°æ®åº“æŸ¥è¯¢
        time.sleep(0.1)
        return {"id": user_id, "name": f"User{user_id}"}

# ============================================
# 3. å¤šå±‚ç¼“å­˜
# ============================================

from cachetools import TTLCache

class MultiLayerCache:
    """å¤šå±‚ç¼“å­˜ç­–ç•¥"""
    
    def __init__(self, redis_client: redis.Redis):
        # L1: å†…å­˜ç¼“å­˜ (å¿«,å°)
        self.l1_cache = TTLCache(maxsize=100, ttl=60)
        
        # L2: Redisç¼“å­˜ (ä¸­,å¤§)
        self.l2_cache = redis_client
    
    async def get(self, key: str) -> Optional[any]:
        """å¤šå±‚æŸ¥æ‰¾"""
        # 1. L1ç¼“å­˜
        if key in self.l1_cache:
            return self.l1_cache[key]
        
        # 2. L2ç¼“å­˜
        value = self.l2_cache.get(key)
        if value:
            self.l1_cache[key] = value  # å›å¡«L1
            return value
        
        # 3. æ•°æ®æº
        value = await self._fetch_from_source(key)
        if value:
            self.l1_cache[key] = value  # å†™å…¥L1
            self.l2_cache.setex(key, 3600, value)  # å†™å…¥L2
        
        return value
```

### 5.2 æ•°æ®åº“ä¼˜åŒ–

```python
"""
æ•°æ®åº“æŸ¥è¯¢ä¼˜åŒ–
"""
from sqlalchemy import select, func
from sqlalchemy.orm import selectinload, joinedload

# ============================================
# 1. N+1æŸ¥è¯¢é—®é¢˜
# ============================================

# âŒ N+1æŸ¥è¯¢
async def get_users_with_posts_slow(session):
    """N+1æŸ¥è¯¢"""
    users = await session.execute(select(User))
    users = users.scalars().all()
    
    # æ¯ä¸ªç”¨æˆ·è§¦å‘ä¸€æ¬¡æŸ¥è¯¢!
    for user in users:
        posts = user.posts  # SELECT * FROM posts WHERE user_id = ?
        print(f"{user.name}: {len(posts)} posts")

# âœ… é¢„åŠ è½½
async def get_users_with_posts_fast(session):
    """ä½¿ç”¨joinedloadé¢„åŠ è½½"""
    stmt = select(User).options(joinedload(User.posts))
    users = await session.execute(stmt)
    users = users.unique().scalars().all()
    
    # åªæœ‰ä¸€æ¬¡æŸ¥è¯¢!
    for user in users:
        posts = user.posts  # å·²åŠ è½½,ä¸è§¦å‘æŸ¥è¯¢
        print(f"{user.name}: {len(posts)} posts")

# ============================================
# 2. æ‰¹é‡æ“ä½œ
# ============================================

# âŒ é€æ¡æ’å…¥
async def insert_slow(session, users: List[dict]):
    for user_data in users:
        user = User(**user_data)
        session.add(user)
        await session.commit()  # æ¯æ¬¡éƒ½æäº¤!

# âœ… æ‰¹é‡æ’å…¥
async def insert_fast(session, users: List[dict]):
    user_objects = [User(**data) for data in users]
    session.add_all(user_objects)
    await session.commit()  # ä¸€æ¬¡æäº¤!

# ============================================
# 3. ç´¢å¼•ä¼˜åŒ–
# ============================================

# åˆ›å»ºç´¢å¼•
"""
CREATE INDEX idx_users_email ON users(email);
CREATE INDEX idx_posts_user_id ON posts(user_id);
CREATE INDEX idx_posts_created_at ON posts(created_at DESC);

# å¤åˆç´¢å¼•
CREATE INDEX idx_posts_user_created ON posts(user_id, created_at DESC);
"""

# ============================================
# 4. åˆ†é¡µä¼˜åŒ–
# ============================================

# âŒ OFFSETåˆ†é¡µ (å¤§åç§»é‡æ…¢)
async def paginate_offset(session, page: int, size: int):
    offset = (page - 1) * size
    stmt = select(Post).offset(offset).limit(size)
    return await session.execute(stmt)

# âœ… æ¸¸æ ‡åˆ†é¡µ (Cursor-based)
async def paginate_cursor(session, last_id: int, size: int):
    stmt = (
        select(Post)
        .where(Post.id > last_id)
        .order_by(Post.id)
        .limit(size)
    )
    return await session.execute(stmt)
```

---

## 6ï¸âƒ£ æ€§èƒ½ç›‘æ§ä¸åˆ†æ

### 6.1 æ€§èƒ½åˆ†æå·¥å…·

```python
"""
æ€§èƒ½åˆ†æå·¥å…·
"""
import cProfile
import pstats
from line_profiler import LineProfiler
import memory_profiler

# ============================================
# 1. cProfile - å‡½æ•°çº§åˆ†æ
# ============================================

def analyze_with_cprofile():
    """ä½¿ç”¨cProfileåˆ†æ"""
    profiler = cProfile.Profile()
    profiler.enable()
    
    # è¿è¡Œä»£ç 
    slow_function()
    
    profiler.disable()
    
    # æ‰“å°ç»Ÿè®¡
    stats = pstats.Stats(profiler)
    stats.sort_stats('cumulative')
    stats.print_stats(20)  # å‰20ä¸ªæœ€æ…¢çš„å‡½æ•°

# å‘½ä»¤è¡Œä½¿ç”¨
# python -m cProfile -s cumulative script.py

# ============================================
# 2. line_profiler - è¡Œçº§åˆ†æ
# ============================================

@profile  # éœ€è¦ kernprof è¿è¡Œ
def slow_function():
    """é€è¡Œåˆ†æ"""
    data = []
    for i in range(10000):
        data.append(i ** 2)  # å“ªä¸€è¡Œæœ€æ…¢?
    return sum(data)

# è¿è¡Œ: kernprof -l -v script.py

# ============================================
# 3. memory_profiler - å†…å­˜åˆ†æ
# ============================================

@memory_profiler.profile
def memory_intensive():
    """å†…å­˜ä½¿ç”¨åˆ†æ"""
    big_list = [i for i in range(1000000)]
    big_dict = {i: i ** 2 for i in range(1000000)}
    return len(big_list) + len(big_dict)

# è¿è¡Œ: python -m memory_profiler script.py

# ============================================
# 4. py-spy - å®æ—¶åˆ†æ
# ============================================

# æ— éœ€ä¿®æ”¹ä»£ç ,ç›´æ¥é‡‡æ ·è¿è¡Œä¸­çš„ç¨‹åº
# py-spy top --pid <PID>
# py-spy record --pid <PID> --output profile.svg
```

---

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–æ¸…å•

### ä¼˜å…ˆçº§çŸ©é˜µ

| ä¼˜åŒ–æ‰‹æ®µ | éš¾åº¦ | æ”¶ç›Š | ä¼˜å…ˆçº§ | é€‚ç”¨åœºæ™¯ |
|---------|------|------|-------|---------|
| **ç®—æ³•ä¼˜åŒ–** | ä½-ä¸­ | æé«˜ | â­â­â­â­â­ | æ‰€æœ‰åœºæ™¯ |
| **æ•°æ®ç»“æ„** | ä½ | é«˜ | â­â­â­â­â­ | æ‰€æœ‰åœºæ™¯ |
| **å†…ç½®å‡½æ•°** | ä½ | ä¸­-é«˜ | â­â­â­â­ | æ‰€æœ‰åœºæ™¯ |
| **ç”Ÿæˆå™¨** | ä½ | ä¸­-é«˜ | â­â­â­â­ | å¤§æ•°æ® |
| **ç¼“å­˜** | ä½-ä¸­ | æé«˜ | â­â­â­â­â­ | é‡å¤è®¡ç®— |
| **å¼‚æ­¥IO** | ä¸­ | æé«˜ | â­â­â­â­â­ | I/Oå¯†é›† |
| **å¤šè¿›ç¨‹** | ä¸­ | é«˜ | â­â­â­â­ | CPUå¯†é›† |
| **NumPy** | ä¸­ | æé«˜ | â­â­â­â­â­ | æ•°å€¼è®¡ç®— |
| **Cython** | é«˜ | æé«˜ | â­â­â­ | æ€§èƒ½ç“¶é¢ˆ |
| **Cæ‰©å±•** | æé«˜ | æé«˜ | â­â­ | æç«¯åœºæ™¯ |

### ä¼˜åŒ–æµç¨‹

```
1. æµ‹é‡ (Measure)
   â”œâ”€â”€ æ‰¾å‡ºæ€§èƒ½ç“¶é¢ˆ
   â”œâ”€â”€ ä½¿ç”¨profilingå·¥å…·
   â””â”€â”€ å»ºç«‹æ€§èƒ½åŸºå‡†

2. åˆ†æ (Analyze)
   â”œâ”€â”€ æ—¶é—´å¤æ‚åº¦
   â”œâ”€â”€ ç©ºé—´å¤æ‚åº¦
   â””â”€â”€ ç³»ç»Ÿèµ„æºä½¿ç”¨

3. ä¼˜åŒ– (Optimize)
   â”œâ”€â”€ ç®—æ³•å±‚: é™ä½å¤æ‚åº¦
   â”œâ”€â”€ è¯­è¨€å±‚: ä½¿ç”¨Pythonç‰¹æ€§
   â”œâ”€â”€ å¹¶å‘å±‚: å¼‚æ­¥/å¤šè¿›ç¨‹
   â””â”€â”€ æ‰©å±•å±‚: NumPy/Cython

4. éªŒè¯ (Verify)
   â”œâ”€â”€ é‡æ–°æµ‹é‡æ€§èƒ½
   â”œâ”€â”€ ç¡®ä¿åŠŸèƒ½æ­£ç¡®
   â””â”€â”€ è¯„ä¼°ä¼˜åŒ–æ”¶ç›Š
```

---

**æ€§èƒ½ä¼˜åŒ–æ˜¯ä¸€ä¸ªæŒç»­è¿‡ç¨‹ï¼Œå…ˆæµ‹é‡ï¼Œå†ä¼˜åŒ–ï¼** âš¡âœ¨

**æœ€åæ›´æ–°**: 2025å¹´10æœˆ28æ—¥

